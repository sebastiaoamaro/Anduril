2023-07-30 13:45:57,218 - INFO  [main:AbstractConfig@361] - MirrorMakerConfig values: 
	clusters = [primary, backup]
	config.providers = []
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:45:57,470 - INFO  [main:Reflections@239] - Reflections took 217 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:45:57,479 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:45:57,487 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:45:57,490 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:45:57,502 - WARN  [main:AppInfoParser@46] - Error while loading kafka-version.properties: null
2023-07-30 13:45:57,515 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:45:57,516 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:45:57,516 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:45:57,516 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:45:57,516 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:45:57,516 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:45:57,521 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:45:57,521 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:45:57,521 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:45:57,521 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:45:57,522 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:45:57,641 - INFO  [main:Log4jControllerRegistration$@31] - Registered kafka:type=kafka.Log4jController MBean
2023-07-30 13:45:57,712 - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
2023-07-30 13:45:57,712 - INFO  [main:Environment@109] - Server environment:host.name=razor15
2023-07-30 13:45:57,712 - INFO  [main:Environment@109] - Server environment:java.version=1.8.0_275
2023-07-30 13:45:57,713 - INFO  [main:Environment@109] - Server environment:java.vendor=Private Build
2023-07-30 13:45:57,713 - INFO  [main:Environment@109] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2023-07-30 13:45:57,713 - INFO  [main:Environment@109] - Server environment:java.class.path=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048/junit-platform-console-standalone-1.7.0.jar
2023-07-30 13:45:57,713 - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2023-07-30 13:45:57,713 - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2023-07-30 13:45:57,713 - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2023-07-30 13:45:57,713 - INFO  [main:Environment@109] - Server environment:os.name=Linux
2023-07-30 13:45:57,713 - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2023-07-30 13:45:57,714 - INFO  [main:Environment@109] - Server environment:os.version=4.15.0-128-generic
2023-07-30 13:45:57,714 - INFO  [main:Environment@109] - Server environment:user.name=tonypan
2023-07-30 13:45:57,714 - INFO  [main:Environment@109] - Server environment:user.home=/home/tonypan
2023-07-30 13:45:57,714 - INFO  [main:Environment@109] - Server environment:user.dir=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048
2023-07-30 13:45:57,714 - INFO  [main:Environment@109] - Server environment:os.memory.free=370MB
2023-07-30 13:45:57,714 - INFO  [main:Environment@109] - Server environment:os.memory.max=7051MB
2023-07-30 13:45:57,714 - INFO  [main:Environment@109] - Server environment:os.memory.total=475MB
2023-07-30 13:45:57,719 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2023-07-30 13:45:57,741 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2023-07-30 13:45:57,743 - INFO  [main:ZooKeeperServer@938] - minSessionTimeout set to 1600
2023-07-30 13:45:57,744 - INFO  [main:ZooKeeperServer@947] - maxSessionTimeout set to 16000
2023-07-30 13:45:57,744 - INFO  [main:ZooKeeperServer@166] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-2152266765152958871/version-2 snapdir /tmp/kafka-6549988588981482602/version-2
2023-07-30 13:45:57,756 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2023-07-30 13:45:57,763 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2023-07-30 13:45:57,772 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-6549988588981482602/version-2/snapshot.0
2023-07-30 13:45:57,776 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-6549988588981482602/version-2/snapshot.0
2023-07-30 13:45:58,177 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6154326205091447453/junit85516104450203242
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:41995
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:45:58,198 - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2023-07-30 13:45:58,270 - INFO  [main:Logging@66] - starting
2023-07-30 13:45:58,271 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:41995
2023-07-30 13:45:58,298 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:41995.
2023-07-30 13:45:58,305 - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
2023-07-30 13:45:58,305 - INFO  [main:Environment@109] - Client environment:host.name=razor15
2023-07-30 13:45:58,306 - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_275
2023-07-30 13:45:58,306 - INFO  [main:Environment@109] - Client environment:java.vendor=Private Build
2023-07-30 13:45:58,306 - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2023-07-30 13:45:58,306 - INFO  [main:Environment@109] - Client environment:java.class.path=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048/junit-platform-console-standalone-1.7.0.jar
2023-07-30 13:45:58,306 - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2023-07-30 13:45:58,306 - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2023-07-30 13:45:58,307 - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2023-07-30 13:45:58,307 - INFO  [main:Environment@109] - Client environment:os.name=Linux
2023-07-30 13:45:58,307 - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2023-07-30 13:45:58,307 - INFO  [main:Environment@109] - Client environment:os.version=4.15.0-128-generic
2023-07-30 13:45:58,307 - INFO  [main:Environment@109] - Client environment:user.name=tonypan
2023-07-30 13:45:58,307 - INFO  [main:Environment@109] - Client environment:user.home=/home/tonypan
2023-07-30 13:45:58,307 - INFO  [main:Environment@109] - Client environment:user.dir=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048
2023-07-30 13:45:58,307 - INFO  [main:Environment@109] - Client environment:os.memory.free=217MB
2023-07-30 13:45:58,307 - INFO  [main:Environment@109] - Client environment:os.memory.max=7051MB
2023-07-30 13:45:58,308 - INFO  [main:Environment@109] - Client environment:os.memory.total=332MB
2023-07-30 13:45:58,312 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:41995 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3b96c42e
2023-07-30 13:45:58,316 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2023-07-30 13:45:58,324 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2023-07-30 13:45:58,326 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2023-07-30 13:45:58,332 - INFO  [main-SendThread(127.0.0.1:41995):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:41995. Will not attempt to authenticate using SASL (unknown error)
2023-07-30 13:45:58,334 - INFO  [main-SendThread(127.0.0.1:41995):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:56152, server: localhost/127.0.0.1:41995
2023-07-30 13:45:58,344 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2023-07-30 13:45:58,357 - INFO  [main-SendThread(127.0.0.1:41995):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:41995, sessionid = 0x1087f2e35810000, negotiated timeout = 16000
2023-07-30 13:45:58,361 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2023-07-30 13:45:58,456 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2023-07-30 13:45:58,468 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2023-07-30 13:45:58,469 - INFO  [feature-zk-node-event-process-thread:FinalizedFeatureCache$@42] - Cleared cache
2023-07-30 13:45:58,729 - INFO  [main:Logging@66] - Cluster ID = 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:45:58,734 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit6154326205091447453/junit85516104450203242/meta.properties
2023-07-30 13:45:58,796 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6154326205091447453/junit85516104450203242
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:41995
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:45:58,807 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6154326205091447453/junit85516104450203242
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:41995
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:45:58,835 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2023-07-30 13:45:58,836 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2023-07-30 13:45:58,837 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2023-07-30 13:45:58,838 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2023-07-30 13:45:58,872 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit6154326205091447453/junit85516104450203242)
2023-07-30 13:45:58,874 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit6154326205091447453/junit85516104450203242 since no clean shutdown file was found
2023-07-30 13:45:58,880 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2023-07-30 13:45:58,895 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2023-07-30 13:45:58,899 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2023-07-30 13:45:59,469 - INFO  [main:Logging@66] - Created ConnectionAcceptRate sensor, quotaLimit=2147483647
2023-07-30 13:45:59,472 - INFO  [main:Logging@66] - Created ConnectionAcceptRate-PLAINTEXT sensor, quotaLimit=2147483647
2023-07-30 13:45:59,476 - INFO  [main:Logging@66] - Updated PLAINTEXT max connection creation rate to 2147483647
2023-07-30 13:45:59,478 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:40623.
2023-07-30 13:45:59,514 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:45:59,546 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2023-07-30 13:45:59,546 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2023-07-30 13:45:59,547 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2023-07-30 13:45:59,547 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2023-07-30 13:45:59,560 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2023-07-30 13:45:59,560 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Starting
2023-07-30 13:45:59,598 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2023-07-30 13:45:59,622 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 24,24,1690739159616,1690739159616,1,0,0,74449230294482944,204,0,24

2023-07-30 13:45:59,623 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:40623, czxid (broker epoch): 24
2023-07-30 13:45:59,696 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2023-07-30 13:45:59,701 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2023-07-30 13:45:59,702 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2023-07-30 13:45:59,704 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2023-07-30 13:45:59,729 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2023-07-30 13:45:59,730 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2023-07-30 13:45:59,744 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2023-07-30 13:45:59,761 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2023-07-30 13:45:59,762 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2023-07-30 13:45:59,762 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2023-07-30 13:45:59,788 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2023-07-30 13:45:59,809 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2023-07-30 13:45:59,817 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Starting socket server acceptors and processors
2023-07-30 13:45:59,821 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:45:59,821 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started socket server acceptors and processors
2023-07-30 13:45:59,823 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:45:59,823 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:45:59,823 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739157540
2023-07-30 13:45:59,824 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2023-07-30 13:45:59,833 - INFO  [main:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:45:59,850 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:45:59,850 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:45:59,850 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739159850
2023-07-30 13:45:59,851 - INFO  [main:EmbeddedConnectCluster@235] - Starting Connect cluster 'primary-connect-cluster' with 3 workers
2023-07-30 13:45:59,854 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:45:59,861 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Recorded new controller, from now on will use broker 0
2023-07-30 13:45:59,871 - INFO  [kafka-producer-network-thread | producer-1:Metadata@279] - [Producer clientId=producer-1] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:45:59,983 - INFO  [main:Reflections@239] - Reflections took 127 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:45:59,985 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:45:59,988 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:45:59,990 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:45:59,994 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:45:59,994 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:45:59,994 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:45:59,994 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:45:59,994 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:45:59,994 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:45:59,995 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:45:59,995 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:45:59,995 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:45:59,996 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:45:59,996 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:00,031 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.backup.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = backup-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.backup.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.backup.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:46:00,032 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:46:00,033 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:00,035 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:00,057 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,057 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,058 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,059 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,059 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,059 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160059
2023-07-30 13:46:00,072 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:00,072 - INFO  [kafka-admin-client-thread | adminclient-1:AppInfoParser@83] - App info kafka.admin.client for adminclient-1 unregistered
2023-07-30 13:46:00,076 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:00,076 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:00,076 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:00,114 - INFO  [main:Log@169] - Logging initialized @3652ms to org.eclipse.jetty.util.log.Slf4jLog
2023-07-30 13:46:00,201 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:46:00,201 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:46:00,225 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:46:00,260 - INFO  [main:AbstractConnector@331] - Started http_localhost0@3113a37{HTTP/1.1, (http/1.1)}{localhost:40703}
2023-07-30 13:46:00,261 - INFO  [main:Server@400] - Started @3799ms
2023-07-30 13:46:00,307 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:40703/
2023-07-30 13:46:00,308 - INFO  [main:RestServer@219] - REST server listening at http://localhost:40703/, advertising URL http://localhost:40703/
2023-07-30 13:46:00,308 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:40703/
2023-07-30 13:46:00,308 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:40703/
2023-07-30 13:46:00,308 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:40703/
2023-07-30 13:46:00,310 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:00,310 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:00,312 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,312 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,312 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,312 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,312 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,312 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,312 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,312 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,313 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,313 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,313 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,313 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,313 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,313 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,313 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160313
2023-07-30 13:46:00,322 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:00,322 - INFO  [kafka-admin-client-thread | adminclient-2:AppInfoParser@83] - App info kafka.admin.client for adminclient-2 unregistered
2023-07-30 13:46:00,323 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:00,324 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:00,324 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:00,329 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:46:00,338 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:00,338 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:00,340 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,340 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,340 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,340 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,340 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,341 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,341 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,341 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,341 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,341 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,341 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,341 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,342 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,342 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,342 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160341
2023-07-30 13:46:00,348 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:00,349 - INFO  [kafka-admin-client-thread | adminclient-3:AppInfoParser@83] - App info kafka.admin.client for adminclient-3 unregistered
2023-07-30 13:46:00,350 - INFO  [kafka-admin-client-thread | adminclient-3:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:00,350 - INFO  [kafka-admin-client-thread | adminclient-3:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:00,350 - INFO  [kafka-admin-client-thread | adminclient-3:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:00,353 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,353 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,353 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160353
2023-07-30 13:46:00,370 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:00,371 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:00,371 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:00,372 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:00,373 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,373 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,373 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,373 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,373 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,373 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,374 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,374 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,374 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,374 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,374 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,374 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,374 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,374 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,374 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160374
2023-07-30 13:46:00,381 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:00,382 - INFO  [kafka-admin-client-thread | adminclient-4:AppInfoParser@83] - App info kafka.admin.client for adminclient-4 unregistered
2023-07-30 13:46:00,383 - INFO  [kafka-admin-client-thread | adminclient-4:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:00,383 - INFO  [kafka-admin-client-thread | adminclient-4:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:00,383 - INFO  [kafka-admin-client-thread | adminclient-4:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:00,389 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:00,389 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:00,391 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,391 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,391 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,391 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,391 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,391 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,391 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,391 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,392 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,392 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,392 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,392 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,392 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,392 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,392 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160392
2023-07-30 13:46:00,398 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:00,398 - INFO  [kafka-admin-client-thread | adminclient-5:AppInfoParser@83] - App info kafka.admin.client for adminclient-5 unregistered
2023-07-30 13:46:00,400 - INFO  [kafka-admin-client-thread | adminclient-5:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:00,400 - INFO  [kafka-admin-client-thread | adminclient-5:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:00,400 - INFO  [kafka-admin-client-thread | adminclient-5:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:00,403 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:00,403 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:00,404 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,405 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,405 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,405 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,405 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,405 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,405 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,405 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,405 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,405 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,406 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,406 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,406 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,406 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,406 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160406
2023-07-30 13:46:00,412 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:00,412 - INFO  [kafka-admin-client-thread | adminclient-6:AppInfoParser@83] - App info kafka.admin.client for adminclient-6 unregistered
2023-07-30 13:46:00,413 - INFO  [kafka-admin-client-thread | adminclient-6:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:00,413 - INFO  [kafka-admin-client-thread | adminclient-6:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:00,414 - INFO  [kafka-admin-client-thread | adminclient-6:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:00,427 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:00,427 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,429 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,430 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,430 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,430 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,430 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,430 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160430
2023-07-30 13:46:00,436 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:00,436 - INFO  [kafka-admin-client-thread | adminclient-7:AppInfoParser@83] - App info kafka.admin.client for adminclient-7 unregistered
2023-07-30 13:46:00,438 - INFO  [kafka-admin-client-thread | adminclient-7:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:00,438 - INFO  [kafka-admin-client-thread | adminclient-7:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:00,438 - INFO  [kafka-admin-client-thread | adminclient-7:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:00,455 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,455 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,455 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160455
2023-07-30 13:46:00,458 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 604ms
2023-07-30 13:46:00,458 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:46:00,459 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:46:00,459 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@286] - [Worker clientId=connect-1, groupId=backup-mm2] Herder starting
2023-07-30 13:46:00,459 - INFO  [DistributedHerder-connect-1-1:Worker@195] - Worker starting
2023-07-30 13:46:00,459 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:46:00,459 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.backup.internal
2023-07-30 13:46:00,459 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:00,460 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,460 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,461 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,461 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,461 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160461
2023-07-30 13:46:00,495 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic mm2-offsets.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0))
2023-07-30 13:46:00,519 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:46:00,603 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-21, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-13)
2023-07-30 13:46:00,636 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:46:00,636 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:46:00,638 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 600000ms
2023-07-30 13:46:00,666 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-16, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,675 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-16 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,676 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-16 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-16
2023-07-30 13:46:00,676 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-16 broker=0] Log loaded for partition mm2-offsets.backup.internal-16 with initial high watermark 0
2023-07-30 13:46:00,690 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-12, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,691 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-12 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,691 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-12 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-12
2023-07-30 13:46:00,691 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-12 broker=0] Log loaded for partition mm2-offsets.backup.internal-12 with initial high watermark 0
2023-07-30 13:46:00,694 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-23, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,695 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-23 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,695 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-23 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-23
2023-07-30 13:46:00,695 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-23 broker=0] Log loaded for partition mm2-offsets.backup.internal-23 with initial high watermark 0
2023-07-30 13:46:00,703 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-8, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,704 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-8 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,704 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-8 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-8
2023-07-30 13:46:00,704 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-8 broker=0] Log loaded for partition mm2-offsets.backup.internal-8 with initial high watermark 0
2023-07-30 13:46:00,711 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-19, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,712 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-19 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,712 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-19 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-19
2023-07-30 13:46:00,712 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-19 broker=0] Log loaded for partition mm2-offsets.backup.internal-19 with initial high watermark 0
2023-07-30 13:46:00,719 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-4, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,720 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-4 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,720 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-4
2023-07-30 13:46:00,720 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-4 broker=0] Log loaded for partition mm2-offsets.backup.internal-4 with initial high watermark 0
2023-07-30 13:46:00,727 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-13, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,728 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-13 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,728 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-13 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-13
2023-07-30 13:46:00,728 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-13 broker=0] Log loaded for partition mm2-offsets.backup.internal-13 with initial high watermark 0
2023-07-30 13:46:00,737 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-9, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,738 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-9 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,738 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-9 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-9
2023-07-30 13:46:00,738 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-9 broker=0] Log loaded for partition mm2-offsets.backup.internal-9 with initial high watermark 0
2023-07-30 13:46:00,744 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-24, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,745 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-24 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,745 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-24 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-24
2023-07-30 13:46:00,745 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-24 broker=0] Log loaded for partition mm2-offsets.backup.internal-24 with initial high watermark 0
2023-07-30 13:46:00,752 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-5, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,753 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-5 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,754 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-5 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-5
2023-07-30 13:46:00,754 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-5 broker=0] Log loaded for partition mm2-offsets.backup.internal-5 with initial high watermark 0
2023-07-30 13:46:00,761 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-20, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,762 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-20 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,762 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-20 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-20
2023-07-30 13:46:00,762 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-20 broker=0] Log loaded for partition mm2-offsets.backup.internal-20 with initial high watermark 0
2023-07-30 13:46:00,769 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-1, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,770 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-1 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,770 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-1
2023-07-30 13:46:00,770 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-1 broker=0] Log loaded for partition mm2-offsets.backup.internal-1 with initial high watermark 0
2023-07-30 13:46:00,777 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-14, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,778 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-14 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,778 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-14 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-14
2023-07-30 13:46:00,778 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-14 broker=0] Log loaded for partition mm2-offsets.backup.internal-14 with initial high watermark 0
2023-07-30 13:46:00,785 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-10, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,786 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-10 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,786 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-10 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-10
2023-07-30 13:46:00,786 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-10 broker=0] Log loaded for partition mm2-offsets.backup.internal-10 with initial high watermark 0
2023-07-30 13:46:00,794 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-21, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,795 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-21 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,795 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-21 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-21
2023-07-30 13:46:00,795 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-21 broker=0] Log loaded for partition mm2-offsets.backup.internal-21 with initial high watermark 0
2023-07-30 13:46:00,802 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-6, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,803 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-6 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,803 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-6 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-6
2023-07-30 13:46:00,803 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-6 broker=0] Log loaded for partition mm2-offsets.backup.internal-6 with initial high watermark 0
2023-07-30 13:46:00,810 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-17, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,811 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-17 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,811 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-17 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-17
2023-07-30 13:46:00,811 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-17 broker=0] Log loaded for partition mm2-offsets.backup.internal-17 with initial high watermark 0
2023-07-30 13:46:00,819 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-2, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,820 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-2 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,820 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-2
2023-07-30 13:46:00,820 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-2 broker=0] Log loaded for partition mm2-offsets.backup.internal-2 with initial high watermark 0
2023-07-30 13:46:00,827 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-15, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,828 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-15 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,828 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-15 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-15
2023-07-30 13:46:00,828 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-15 broker=0] Log loaded for partition mm2-offsets.backup.internal-15 with initial high watermark 0
2023-07-30 13:46:00,835 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-0, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,837 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-0 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,837 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-0
2023-07-30 13:46:00,837 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-0 broker=0] Log loaded for partition mm2-offsets.backup.internal-0 with initial high watermark 0
2023-07-30 13:46:00,844 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-11, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,844 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-11 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,845 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-11 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-11
2023-07-30 13:46:00,845 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-11 broker=0] Log loaded for partition mm2-offsets.backup.internal-11 with initial high watermark 0
2023-07-30 13:46:00,852 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-7, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,853 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-7 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,853 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-7 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-7
2023-07-30 13:46:00,853 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-7 broker=0] Log loaded for partition mm2-offsets.backup.internal-7 with initial high watermark 0
2023-07-30 13:46:00,875 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-22, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,876 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-22 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,876 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-22 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-22
2023-07-30 13:46:00,876 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-22 broker=0] Log loaded for partition mm2-offsets.backup.internal-22 with initial high watermark 0
2023-07-30 13:46:00,879 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-3, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,880 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-3 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,881 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-3
2023-07-30 13:46:00,881 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-3 broker=0] Log loaded for partition mm2-offsets.backup.internal-3 with initial high watermark 0
2023-07-30 13:46:00,887 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-18, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:00,888 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-18 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offsets.backup.internal-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:00,889 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-18 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-18
2023-07-30 13:46:00,889 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-18 broker=0] Log loaded for partition mm2-offsets.backup.internal-18 with initial high watermark 0
2023-07-30 13:46:00,915 - INFO  [DistributedHerder-connect-1-1:TopicAdmin@284] - Created topic (name=mm2-offsets.backup.internal, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:40623
2023-07-30 13:46:00,916 - INFO  [kafka-admin-client-thread | adminclient-8:AppInfoParser@83] - App info kafka.admin.client for adminclient-8 unregistered
2023-07-30 13:46:00,916 - INFO  [kafka-admin-client-thread | adminclient-8:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:00,916 - INFO  [kafka-admin-client-thread | adminclient-8:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:00,916 - INFO  [kafka-admin-client-thread | adminclient-8:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:00,917 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:00,919 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,919 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,920 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,921 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,921 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,921 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160920
2023-07-30 13:46:00,924 - INFO  [kafka-producer-network-thread | producer-2:Metadata@279] - [Producer clientId=producer-2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:00,929 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:00,951 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,952 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:00,952 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:00,952 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:00,952 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739160952
2023-07-30 13:46:00,957 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:00,974 - INFO  [DistributedHerder-connect-1-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Subscribed to partition(s): mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-21
2023-07-30 13:46:00,977 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-0
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-5
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-10
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-20
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-15
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-9
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-11
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-16
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-4
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-17
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-3
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-24
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-23
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-13
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-18
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-22
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-2
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-8
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-12
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-19
2023-07-30 13:46:00,978 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-14
2023-07-30 13:46:00,979 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-1
2023-07-30 13:46:00,979 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-6
2023-07-30 13:46:00,979 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-7
2023-07-30 13:46:00,979 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-21
2023-07-30 13:46:01,014 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,015 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,015 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,016 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,017 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,017 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,017 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,017 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,017 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,017 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,017 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,017 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,017 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,018 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,018 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,019 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:01,019 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:01,019 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:46:01,022 - INFO  [DistributedHerder-connect-1-1:Worker@202] - Worker started
2023-07-30 13:46:01,022 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.backup.internal
2023-07-30 13:46:01,022 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,023 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,024 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,024 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,024 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161024
2023-07-30 13:46:01,039 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic mm2-status.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2023-07-30 13:46:01,058 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-status.backup.internal-0, mm2-status.backup.internal-3, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2)
2023-07-30 13:46:01,061 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-4, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,062 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-4 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-status.backup.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,062 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-4
2023-07-30 13:46:01,062 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-4 broker=0] Log loaded for partition mm2-status.backup.internal-4 with initial high watermark 0
2023-07-30 13:46:01,065 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-3, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,066 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-3 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-status.backup.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,066 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-3
2023-07-30 13:46:01,066 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-3 broker=0] Log loaded for partition mm2-status.backup.internal-3 with initial high watermark 0
2023-07-30 13:46:01,074 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-2, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,075 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-2 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-status.backup.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,075 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-2
2023-07-30 13:46:01,075 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-2 broker=0] Log loaded for partition mm2-status.backup.internal-2 with initial high watermark 0
2023-07-30 13:46:01,082 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-1, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,083 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-1 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-status.backup.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,083 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-1
2023-07-30 13:46:01,083 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-1 broker=0] Log loaded for partition mm2-status.backup.internal-1 with initial high watermark 0
2023-07-30 13:46:01,090 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-0, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,091 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-0 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-status.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,092 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-0
2023-07-30 13:46:01,092 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-0 broker=0] Log loaded for partition mm2-status.backup.internal-0 with initial high watermark 0
2023-07-30 13:46:01,102 - INFO  [DistributedHerder-connect-1-1:TopicAdmin@284] - Created topic (name=mm2-status.backup.internal, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:40623
2023-07-30 13:46:01,103 - INFO  [kafka-admin-client-thread | adminclient-9:AppInfoParser@83] - App info kafka.admin.client for adminclient-9 unregistered
2023-07-30 13:46:01,104 - INFO  [kafka-admin-client-thread | adminclient-9:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,104 - INFO  [kafka-admin-client-thread | adminclient-9:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,104 - INFO  [kafka-admin-client-thread | adminclient-9:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,104 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:01,106 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,107 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,108 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,109 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161107
2023-07-30 13:46:01,109 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:01,111 - INFO  [kafka-producer-network-thread | producer-3:Metadata@279] - [Producer clientId=producer-3] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,112 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,113 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,113 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,113 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161113
2023-07-30 13:46:01,116 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,123 - INFO  [DistributedHerder-connect-1-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Subscribed to partition(s): mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2, mm2-status.backup.internal-3
2023-07-30 13:46:01,123 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-0
2023-07-30 13:46:01,123 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-4
2023-07-30 13:46:01,123 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-1
2023-07-30 13:46:01,123 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-2
2023-07-30 13:46:01,123 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-3
2023-07-30 13:46:01,129 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,129 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,130 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,130 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,130 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,130 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:01,130 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:01,133 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:46:01,133 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.backup.internal
2023-07-30 13:46:01,133 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,135 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,135 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,135 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161135
2023-07-30 13:46:01,147 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic mm2-configs.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:46:01,159 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:46:01,161 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-configs.backup.internal-0, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,162 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-configs.backup.internal-0 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-configs.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,162 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-configs.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-configs.backup.internal-0
2023-07-30 13:46:01,162 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-configs.backup.internal-0 broker=0] Log loaded for partition mm2-configs.backup.internal-0 with initial high watermark 0
2023-07-30 13:46:01,167 - INFO  [DistributedHerder-connect-1-1:TopicAdmin@284] - Created topic (name=mm2-configs.backup.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:40623
2023-07-30 13:46:01,169 - INFO  [kafka-admin-client-thread | adminclient-10:AppInfoParser@83] - App info kafka.admin.client for adminclient-10 unregistered
2023-07-30 13:46:01,170 - INFO  [kafka-admin-client-thread | adminclient-10:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,170 - INFO  [kafka-admin-client-thread | adminclient-10:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,170 - INFO  [kafka-admin-client-thread | adminclient-10:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,171 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:01,173 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,174 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,175 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,175 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,175 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,175 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161175
2023-07-30 13:46:01,175 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:01,177 - INFO  [kafka-producer-network-thread | producer-4:Metadata@279] - [Producer clientId=producer-4] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,178 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,179 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,179 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,179 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,179 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,179 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161179
2023-07-30 13:46:01,181 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,185 - INFO  [DistributedHerder-connect-1-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Subscribed to partition(s): mm2-configs.backup.internal-0
2023-07-30 13:46:01,185 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-configs.backup.internal-0
2023-07-30 13:46:01,190 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Resetting offset for partition mm2-configs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,191 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:01,191 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:01,191 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:46:01,191 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@290] - [Worker clientId=connect-1, groupId=backup-mm2] Herder started
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:46:01,198 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Worker clientId=connect-1, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,205 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0))
2023-07-30 13:46:01,209 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2023-07-30 13:46:01,265 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40)
2023-07-30 13:46:01,270 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,271 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,272 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2023-07-30 13:46:01,272 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2023-07-30 13:46:01,276 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-18, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,276 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-18 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,277 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18
2023-07-30 13:46:01,277 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0
2023-07-30 13:46:01,284 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-41, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,285 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-41 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,285 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41
2023-07-30 13:46:01,285 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0
2023-07-30 13:46:01,292 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-10, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,293 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-10 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,293 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10
2023-07-30 13:46:01,293 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0
2023-07-30 13:46:01,435 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-33, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,436 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-33 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,436 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33
2023-07-30 13:46:01,436 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0
2023-07-30 13:46:01,441 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-48, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,442 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-48 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,442 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48
2023-07-30 13:46:01,442 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0
2023-07-30 13:46:01,449 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-19, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,450 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-19 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,450 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19
2023-07-30 13:46:01,450 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0
2023-07-30 13:46:01,457 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-34, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,458 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-34 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,458 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34
2023-07-30 13:46:01,458 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0
2023-07-30 13:46:01,465 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,466 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,466 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2023-07-30 13:46:01,466 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2023-07-30 13:46:01,473 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-11, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,474 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-11 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,474 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11
2023-07-30 13:46:01,474 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0
2023-07-30 13:46:01,482 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-26, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,482 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-26 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,482 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26
2023-07-30 13:46:01,483 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0
2023-07-30 13:46:01,490 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-49, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,491 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-49 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,491 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49
2023-07-30 13:46:01,491 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0
2023-07-30 13:46:01,498 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-39, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,499 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-39 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,499 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39
2023-07-30 13:46:01,499 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0
2023-07-30 13:46:01,506 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-9, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,507 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-9 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,507 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9
2023-07-30 13:46:01,507 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0
2023-07-30 13:46:01,515 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-24, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,515 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-24 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,515 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24
2023-07-30 13:46:01,515 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0
2023-07-30 13:46:01,524 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-31, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,525 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-31 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,525 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31
2023-07-30 13:46:01,525 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:46:01,532 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-46, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,533 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-46 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,533 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46
2023-07-30 13:46:01,533 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0
2023-07-30 13:46:01,534 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@2424686b{/,null,AVAILABLE}
2023-07-30 13:46:01,534 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:46:01,535 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:46:01,535 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:40703/'}
2023-07-30 13:46:01,535 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:46:01,539 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,540 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,540 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2023-07-30 13:46:01,540 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2023-07-30 13:46:01,548 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-16, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,548 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-16 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,548 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16
2023-07-30 13:46:01,549 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0
2023-07-30 13:46:01,556 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,557 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,557 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2023-07-30 13:46:01,557 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2023-07-30 13:46:01,565 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-25, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,566 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-25 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,566 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25
2023-07-30 13:46:01,566 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0
2023-07-30 13:46:01,573 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-40, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,574 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-40 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,574 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40
2023-07-30 13:46:01,574 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0
2023-07-30 13:46:01,582 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-47, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,583 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-47 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,583 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47
2023-07-30 13:46:01,583 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0
2023-07-30 13:46:01,590 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-17, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,590 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-17 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,591 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17
2023-07-30 13:46:01,591 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0
2023-07-30 13:46:01,598 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-32, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,599 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-32 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,599 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32
2023-07-30 13:46:01,599 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0
2023-07-30 13:46:01,606 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-37, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,607 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-37 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,607 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37
2023-07-30 13:46:01,608 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0
2023-07-30 13:46:01,614 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-7, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,615 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-7 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,615 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7
2023-07-30 13:46:01,615 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0
2023-07-30 13:46:01,623 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-22, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,623 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-22 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,624 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22
2023-07-30 13:46:01,624 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0
2023-07-30 13:46:01,631 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-29, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,632 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-29 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,632 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29
2023-07-30 13:46:01,632 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0
2023-07-30 13:46:01,634 - INFO  [main:Reflections@239] - Reflections took 98 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:46:01,637 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:01,639 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:01,641 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-44, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,642 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:01,642 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-44 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,644 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44
2023-07-30 13:46:01,644 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0
2023-07-30 13:46:01,646 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:46:01,647 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:01,647 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:01,647 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:01,647 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:01,648 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:01,648 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-14, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,648 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:01,648 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:01,649 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:01,649 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-14 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,649 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:01,649 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14
2023-07-30 13:46:01,649 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:01,649 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0
2023-07-30 13:46:01,650 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.backup.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = backup-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.backup.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.backup.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:46:01,650 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:46:01,651 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:01,651 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,652 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,652 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,652 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,652 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,652 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,652 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,653 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,653 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,653 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,653 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,653 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,653 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,653 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,653 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,654 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161653
2023-07-30 13:46:01,657 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-23, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,659 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-23 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,659 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23
2023-07-30 13:46:01,659 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0
2023-07-30 13:46:01,661 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,662 - INFO  [kafka-admin-client-thread | adminclient-11:AppInfoParser@83] - App info kafka.admin.client for adminclient-11 unregistered
2023-07-30 13:46:01,663 - INFO  [kafka-admin-client-thread | adminclient-11:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,664 - INFO  [kafka-admin-client-thread | adminclient-11:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,664 - INFO  [kafka-admin-client-thread | adminclient-11:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,664 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:46:01,664 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:46:01,665 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:46:01,665 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-38, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,666 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-38 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,666 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38
2023-07-30 13:46:01,666 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0
2023-07-30 13:46:01,711 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-8, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,712 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-8 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,712 - INFO  [main:AbstractConnector@331] - Started http_localhost0@181e72d3{HTTP/1.1, (http/1.1)}{localhost:38925}
2023-07-30 13:46:01,712 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8
2023-07-30 13:46:01,713 - INFO  [main:Server@400] - Started @5251ms
2023-07-30 13:46:01,713 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0
2023-07-30 13:46:01,714 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:38925/
2023-07-30 13:46:01,714 - INFO  [main:RestServer@219] - REST server listening at http://localhost:38925/, advertising URL http://localhost:38925/
2023-07-30 13:46:01,714 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:38925/
2023-07-30 13:46:01,714 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:38925/
2023-07-30 13:46:01,715 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:38925/
2023-07-30 13:46:01,715 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:01,715 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,716 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,716 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,716 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,716 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,717 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,717 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,717 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,717 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,717 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,717 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,717 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,717 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,717 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-45, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,717 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,717 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,718 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161717
2023-07-30 13:46:01,718 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-45 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,718 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45
2023-07-30 13:46:01,719 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0
2023-07-30 13:46:01,723 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,724 - INFO  [kafka-admin-client-thread | adminclient-12:AppInfoParser@83] - App info kafka.admin.client for adminclient-12 unregistered
2023-07-30 13:46:01,724 - INFO  [kafka-admin-client-thread | adminclient-12:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,725 - INFO  [kafka-admin-client-thread | adminclient-12:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,725 - INFO  [kafka-admin-client-thread | adminclient-12:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,725 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:46:01,725 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:01,725 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-15, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,725 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,726 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-15 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,726 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15
2023-07-30 13:46:01,726 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,727 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,728 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,728 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,728 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,728 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161728
2023-07-30 13:46:01,733 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-30, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,734 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,734 - INFO  [kafka-admin-client-thread | adminclient-13:AppInfoParser@83] - App info kafka.admin.client for adminclient-13 unregistered
2023-07-30 13:46:01,735 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-30 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,735 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30
2023-07-30 13:46:01,735 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0
2023-07-30 13:46:01,735 - INFO  [kafka-admin-client-thread | adminclient-13:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,735 - INFO  [kafka-admin-client-thread | adminclient-13:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,735 - INFO  [kafka-admin-client-thread | adminclient-13:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,736 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,736 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,736 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161736
2023-07-30 13:46:01,737 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:01,737 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:01,737 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:01,737 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,738 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,739 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,740 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,740 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,740 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,740 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161740
2023-07-30 13:46:01,742 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,743 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,744 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2023-07-30 13:46:01,744 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2023-07-30 13:46:01,746 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,746 - INFO  [kafka-admin-client-thread | adminclient-14:AppInfoParser@83] - App info kafka.admin.client for adminclient-14 unregistered
2023-07-30 13:46:01,747 - INFO  [kafka-admin-client-thread | adminclient-14:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,747 - INFO  [kafka-admin-client-thread | adminclient-14:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,748 - INFO  [kafka-admin-client-thread | adminclient-14:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,748 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:01,748 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,749 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,749 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,749 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,749 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,750 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,750 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,750 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,750 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,750 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-35, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,750 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,750 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,750 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,750 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,751 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,751 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,751 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161751
2023-07-30 13:46:01,751 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-35 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,751 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35
2023-07-30 13:46:01,751 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0
2023-07-30 13:46:01,757 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,757 - INFO  [kafka-admin-client-thread | adminclient-15:AppInfoParser@83] - App info kafka.admin.client for adminclient-15 unregistered
2023-07-30 13:46:01,758 - INFO  [kafka-admin-client-thread | adminclient-15:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,758 - INFO  [kafka-admin-client-thread | adminclient-15:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,758 - INFO  [kafka-admin-client-thread | adminclient-15:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,759 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:01,759 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-5, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,759 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,760 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-5 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,760 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5
2023-07-30 13:46:01,760 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0
2023-07-30 13:46:01,760 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,760 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,760 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,760 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,760 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,761 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,761 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,761 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,761 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,761 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,761 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,761 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,761 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,761 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,761 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161761
2023-07-30 13:46:01,766 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-20, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,767 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,767 - INFO  [kafka-admin-client-thread | adminclient-16:AppInfoParser@83] - App info kafka.admin.client for adminclient-16 unregistered
2023-07-30 13:46:01,768 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-20 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,768 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20
2023-07-30 13:46:01,768 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0
2023-07-30 13:46:01,768 - INFO  [kafka-admin-client-thread | adminclient-16:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,768 - INFO  [kafka-admin-client-thread | adminclient-16:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,768 - INFO  [kafka-admin-client-thread | adminclient-16:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,768 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:01,769 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,770 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,771 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,771 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,771 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,771 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,771 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161771
2023-07-30 13:46:01,775 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-27, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,775 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-27 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,776 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,776 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27
2023-07-30 13:46:01,776 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0
2023-07-30 13:46:01,776 - INFO  [kafka-admin-client-thread | adminclient-17:AppInfoParser@83] - App info kafka.admin.client for adminclient-17 unregistered
2023-07-30 13:46:01,777 - INFO  [kafka-admin-client-thread | adminclient-17:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,777 - INFO  [kafka-admin-client-thread | adminclient-17:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,777 - INFO  [kafka-admin-client-thread | adminclient-17:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,778 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,778 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,778 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161778
2023-07-30 13:46:01,779 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 243ms
2023-07-30 13:46:01,779 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:46:01,779 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:46:01,779 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@286] - [Worker clientId=connect-2, groupId=backup-mm2] Herder starting
2023-07-30 13:46:01,779 - INFO  [DistributedHerder-connect-2-1:Worker@195] - Worker starting
2023-07-30 13:46:01,779 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:46:01,779 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.backup.internal
2023-07-30 13:46:01,779 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:46:01,779 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,780 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,781 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,781 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,781 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161781
2023-07-30 13:46:01,782 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:46:01,782 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:46:01,782 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 660000ms
2023-07-30 13:46:01,784 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-42, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,784 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-42 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,785 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42
2023-07-30 13:46:01,785 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0
2023-07-30 13:46:01,791 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-12, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,792 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-12 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,793 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12
2023-07-30 13:46:01,793 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0
2023-07-30 13:46:01,800 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-21, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,801 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-21 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,801 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21
2023-07-30 13:46:01,802 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0
2023-07-30 13:46:01,804 - INFO  [kafka-admin-client-thread | adminclient-18:AppInfoParser@83] - App info kafka.admin.client for adminclient-18 unregistered
2023-07-30 13:46:01,805 - INFO  [kafka-admin-client-thread | adminclient-18:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,805 - INFO  [kafka-admin-client-thread | adminclient-18:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,805 - INFO  [kafka-admin-client-thread | adminclient-18:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,805 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:01,807 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,808 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,808 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,809 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,809 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,809 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,809 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,809 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,809 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,809 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,809 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,810 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,810 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,810 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,810 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,810 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,810 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161810
2023-07-30 13:46:01,810 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-36, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,811 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:01,811 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-36 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,812 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36
2023-07-30 13:46:01,812 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0
2023-07-30 13:46:01,812 - INFO  [kafka-producer-network-thread | producer-5:Metadata@279] - [Producer clientId=producer-5] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,813 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,814 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,814 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161813
2023-07-30 13:46:01,817 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-6, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,817 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,817 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-6 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,817 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6
2023-07-30 13:46:01,818 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0
2023-07-30 13:46:01,822 - INFO  [DistributedHerder-connect-2-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Subscribed to partition(s): mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-21
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-0
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-5
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-10
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-20
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-15
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-9
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-11
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-16
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-4
2023-07-30 13:46:01,823 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-17
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-3
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-24
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-23
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-13
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-18
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-22
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-2
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-8
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-12
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-19
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-14
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-1
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-6
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-7
2023-07-30 13:46:01,824 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-21
2023-07-30 13:46:01,825 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-43, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,826 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-43 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,828 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43
2023-07-30 13:46:01,828 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0
2023-07-30 13:46:01,833 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-13, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,833 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,834 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,834 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,834 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,834 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,834 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,834 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-13 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,834 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,834 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,835 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,835 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:01,836 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:46:01,837 - INFO  [DistributedHerder-connect-2-1:Worker@202] - Worker started
2023-07-30 13:46:01,837 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.backup.internal
2023-07-30 13:46:01,838 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,840 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,840 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,841 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,841 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,841 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161841
2023-07-30 13:46:01,844 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-28, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:01,845 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-28 in /tmp/junit6154326205091447453/junit85516104450203242/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:01,845 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28
2023-07-30 13:46:01,845 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0
2023-07-30 13:46:01,851 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2023-07-30 13:46:01,852 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18
2023-07-30 13:46:01,852 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41
2023-07-30 13:46:01,852 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9
2023-07-30 13:46:01,853 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22
2023-07-30 13:46:01,854 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27
2023-07-30 13:46:01,855 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42
2023-07-30 13:46:01,857 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,857 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12
2023-07-30 13:46:01,858 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21
2023-07-30 13:46:01,858 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36
2023-07-30 13:46:01,858 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,858 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6
2023-07-30 13:46:01,858 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,858 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43
2023-07-30 13:46:01,858 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13
2023-07-30 13:46:01,858 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,858 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28
2023-07-30 13:46:01,859 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,859 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,859 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,859 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,859 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,860 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,860 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,860 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,861 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,861 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,861 - INFO  [kafka-admin-client-thread | adminclient-19:AppInfoParser@83] - App info kafka.admin.client for adminclient-19 unregistered
2023-07-30 13:46:01,861 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,861 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,861 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,862 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,862 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,862 - INFO  [kafka-admin-client-thread | adminclient-19:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,862 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,862 - INFO  [kafka-admin-client-thread | adminclient-19:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,862 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,862 - INFO  [kafka-admin-client-thread | adminclient-19:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,862 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,863 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,863 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,863 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:01,863 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,863 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,863 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,864 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 10 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,864 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,864 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,864 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,864 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,864 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,865 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,865 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,865 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,865 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,865 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,865 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,865 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,865 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,865 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 10 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,867 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,867 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,867 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,867 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,867 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,867 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,867 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,867 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,867 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,867 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161867
2023-07-30 13:46:01,867 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,868 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 13 milliseconds, of which 13 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,868 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 13 milliseconds, of which 13 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,868 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:01,868 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 13 milliseconds, of which 13 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,869 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 14 milliseconds, of which 14 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,869 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 14 milliseconds, of which 14 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,869 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 14 milliseconds, of which 14 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,869 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 13 milliseconds, of which 13 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,869 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 11 milliseconds, of which 11 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,869 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 11 milliseconds, of which 11 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,870 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,870 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,870 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,870 - INFO  [kafka-producer-network-thread | producer-6:Metadata@279] - [Producer clientId=producer-6] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,870 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,870 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 11 milliseconds, of which 11 milliseconds was spent in the scheduler.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,871 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,872 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,872 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,872 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,872 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,872 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,872 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161872
2023-07-30 13:46:01,875 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,880 - INFO  [DistributedHerder-connect-2-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Subscribed to partition(s): mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2, mm2-status.backup.internal-3
2023-07-30 13:46:01,880 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-0
2023-07-30 13:46:01,880 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-4
2023-07-30 13:46:01,880 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-1
2023-07-30 13:46:01,880 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-2
2023-07-30 13:46:01,881 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-3
2023-07-30 13:46:01,886 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,887 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,887 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,887 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,887 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,887 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:01,887 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:01,888 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:46:01,889 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.backup.internal
2023-07-30 13:46:01,889 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:01,892 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,893 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,893 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,893 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161893
Jul 30, 2023 1:46:01 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:46:01,899 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@1fac1d5c{/,null,AVAILABLE}
2023-07-30 13:46:01,900 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:46:01,900 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:46:01,900 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:38925/'}
2023-07-30 13:46:01,901 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:46:01,906 - INFO  [kafka-admin-client-thread | adminclient-20:AppInfoParser@83] - App info kafka.admin.client for adminclient-20 unregistered
2023-07-30 13:46:01,907 - INFO  [kafka-admin-client-thread | adminclient-20:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:01,908 - INFO  [kafka-admin-client-thread | adminclient-20:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:01,908 - INFO  [kafka-admin-client-thread | adminclient-20:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:01,908 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:01,910 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,910 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,910 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,910 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,911 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,911 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,911 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161911
2023-07-30 13:46:01,912 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:01,913 - INFO  [kafka-producer-network-thread | producer-7:Metadata@279] - [Producer clientId=producer-7] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:01,914 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:01,915 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:01,915 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739161914
2023-07-30 13:46:01,917 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,921 - INFO  [DistributedHerder-connect-2-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Subscribed to partition(s): mm2-configs.backup.internal-0
2023-07-30 13:46:01,921 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-configs.backup.internal-0
2023-07-30 13:46:01,926 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Resetting offset for partition mm2-configs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:01,926 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:01,926 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:01,927 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:46:01,927 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@290] - [Worker clientId=connect-2, groupId=backup-mm2] Herder started
2023-07-30 13:46:01,933 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Worker clientId=connect-2, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:01,934 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-2, groupId=backup-mm2] Discovered group coordinator localhost:40623 (id: 2147483647 rack: null)
2023-07-30 13:46:01,937 - INFO  [DistributedHerder-connect-2-1:WorkerCoordinator@225] - [Worker clientId=connect-2, groupId=backup-mm2] Rebalance started
2023-07-30 13:46:01,938 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@534] - [Worker clientId=connect-2, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:46:01,948 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-1, groupId=backup-mm2] Discovered group coordinator localhost:40623 (id: 2147483647 rack: null)
2023-07-30 13:46:01,951 - INFO  [DistributedHerder-connect-1-1:WorkerCoordinator@225] - [Worker clientId=connect-1, groupId=backup-mm2] Rebalance started
2023-07-30 13:46:01,951 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@534] - [Worker clientId=connect-1, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:46:01,966 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@468] - [Worker clientId=connect-2, groupId=backup-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:46:01,966 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@468] - [Worker clientId=connect-1, groupId=backup-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:46:01,966 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@534] - [Worker clientId=connect-2, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:46:01,966 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@534] - [Worker clientId=connect-1, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:46:01,973 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group backup-mm2 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3 with group instance id None)
2023-07-30 13:46:01,981 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group backup-mm2 generation 1 (__consumer_offsets-5)
2023-07-30 13:46:01,984 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=1, memberId='connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3', protocol='sessioned'}
2023-07-30 13:46:01,984 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=1, memberId='connect-1-a8a55554-5291-434c-9af9-322de7d60f07', protocol='sessioned'}
2023-07-30 13:46:01,999 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group backup-mm2 for generation 1
2023-07-30 13:46:02,007 - INFO  [main:Reflections@239] - Reflections took 104 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:46:02,010 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:02,012 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:02,014 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:02,018 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:46:02,018 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:02,018 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:02,018 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,018 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,019 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,019 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:02,020 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:02,020 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,020 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,020 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,021 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.backup.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = backup-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.backup.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.backup.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:46:02,021 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:46:02,021 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,021 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,022 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,022 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,022 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,023 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,023 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,023 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162023
2023-07-30 13:46:02,032 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,032 - INFO  [kafka-admin-client-thread | adminclient-21:AppInfoParser@83] - App info kafka.admin.client for adminclient-21 unregistered
2023-07-30 13:46:02,033 - INFO  [kafka-admin-client-thread | adminclient-21:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,034 - INFO  [kafka-admin-client-thread | adminclient-21:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,034 - INFO  [kafka-admin-client-thread | adminclient-21:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,034 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:46:02,034 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:46:02,035 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:46:02,044 - INFO  [main:AbstractConnector@331] - Started http_localhost0@28da7d11{HTTP/1.1, (http/1.1)}{localhost:44117}
2023-07-30 13:46:02,045 - INFO  [main:Server@400] - Started @5583ms
2023-07-30 13:46:02,045 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44117/
2023-07-30 13:46:02,045 - INFO  [main:RestServer@219] - REST server listening at http://localhost:44117/, advertising URL http://localhost:44117/
2023-07-30 13:46:02,045 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44117/
2023-07-30 13:46:02,045 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:44117/
2023-07-30 13:46:02,046 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44117/
2023-07-30 13:46:02,046 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,046 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,047 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,047 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,047 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,048 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,048 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,049 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162048
2023-07-30 13:46:02,055 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,056 - INFO  [kafka-admin-client-thread | adminclient-22:AppInfoParser@83] - App info kafka.admin.client for adminclient-22 unregistered
2023-07-30 13:46:02,057 - INFO  [kafka-admin-client-thread | adminclient-22:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,058 - INFO  [kafka-admin-client-thread | adminclient-22:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,058 - INFO  [kafka-admin-client-thread | adminclient-22:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,059 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=1, memberId='connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3', protocol='sessioned'}
2023-07-30 13:46:02,059 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=1, memberId='connect-1-a8a55554-5291-434c-9af9-322de7d60f07', protocol='sessioned'}
2023-07-30 13:46:02,059 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:46:02,059 - INFO  [DistributedHerder-connect-1-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-1, groupId=backup-mm2] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3', leaderUrl='http://localhost:38925/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:02,059 - INFO  [DistributedHerder-connect-2-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-2, groupId=backup-mm2] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3', leaderUrl='http://localhost:38925/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:02,059 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,060 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1215] - [Worker clientId=connect-1, groupId=backup-mm2] Starting connectors and tasks using config offset -1
2023-07-30 13:46:02,060 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1215] - [Worker clientId=connect-2, groupId=backup-mm2] Starting connectors and tasks using config offset -1
2023-07-30 13:46:02,060 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1243] - [Worker clientId=connect-1, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:46:02,060 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,060 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1243] - [Worker clientId=connect-2, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:46:02,061 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,062 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,062 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,062 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,062 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,062 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,062 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,062 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,063 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,063 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,063 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,063 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,063 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,063 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,063 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162063
2023-07-30 13:46:02,069 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,070 - INFO  [kafka-admin-client-thread | adminclient-23:AppInfoParser@83] - App info kafka.admin.client for adminclient-23 unregistered
2023-07-30 13:46:02,071 - INFO  [kafka-admin-client-thread | adminclient-23:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,071 - INFO  [kafka-admin-client-thread | adminclient-23:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,071 - INFO  [kafka-admin-client-thread | adminclient-23:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,071 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,072 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,072 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162071
2023-07-30 13:46:02,072 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:02,072 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:02,073 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,073 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,074 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,074 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,074 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,074 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,074 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,074 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,074 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,075 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,075 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,075 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,075 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,075 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,075 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,075 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,075 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162075
2023-07-30 13:46:02,083 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,084 - INFO  [kafka-admin-client-thread | adminclient-24:AppInfoParser@83] - App info kafka.admin.client for adminclient-24 unregistered
2023-07-30 13:46:02,085 - INFO  [kafka-admin-client-thread | adminclient-24:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,085 - INFO  [kafka-admin-client-thread | adminclient-24:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,085 - INFO  [kafka-admin-client-thread | adminclient-24:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,085 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,085 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,086 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,087 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,087 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,087 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,087 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,087 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,087 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,087 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,087 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,088 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,088 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,088 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,088 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,089 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,089 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162088
2023-07-30 13:46:02,096 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,097 - INFO  [kafka-admin-client-thread | adminclient-25:AppInfoParser@83] - App info kafka.admin.client for adminclient-25 unregistered
2023-07-30 13:46:02,098 - INFO  [kafka-admin-client-thread | adminclient-25:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,098 - INFO  [kafka-admin-client-thread | adminclient-25:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,099 - INFO  [kafka-admin-client-thread | adminclient-25:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,099 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,099 - INFO  [KafkaBasedLog Work Thread - mm2-configs.backup.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-2, groupId=backup-mm2] Session key updated
2023-07-30 13:46:02,099 - INFO  [KafkaBasedLog Work Thread - mm2-configs.backup.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-1, groupId=backup-mm2] Session key updated
2023-07-30 13:46:02,099 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,101 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,101 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,101 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,101 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,102 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,102 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,102 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,102 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,102 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,102 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,102 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,102 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,102 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,103 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,103 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162102
2023-07-30 13:46:02,109 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,109 - INFO  [kafka-admin-client-thread | adminclient-26:AppInfoParser@83] - App info kafka.admin.client for adminclient-26 unregistered
2023-07-30 13:46:02,110 - INFO  [kafka-admin-client-thread | adminclient-26:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,110 - INFO  [kafka-admin-client-thread | adminclient-26:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,110 - INFO  [kafka-admin-client-thread | adminclient-26:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,111 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,111 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,112 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,112 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,112 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,112 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,112 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,113 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,113 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,113 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,113 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,113 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,113 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,113 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,113 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,113 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,113 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162113
2023-07-30 13:46:02,119 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,119 - INFO  [kafka-admin-client-thread | adminclient-27:AppInfoParser@83] - App info kafka.admin.client for adminclient-27 unregistered
2023-07-30 13:46:02,120 - INFO  [kafka-admin-client-thread | adminclient-27:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,120 - INFO  [kafka-admin-client-thread | adminclient-27:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,120 - INFO  [kafka-admin-client-thread | adminclient-27:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,122 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,122 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,122 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162122
2023-07-30 13:46:02,122 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 222ms
2023-07-30 13:46:02,122 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:46:02,123 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:46:02,123 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@286] - [Worker clientId=connect-3, groupId=backup-mm2] Herder starting
2023-07-30 13:46:02,123 - INFO  [DistributedHerder-connect-3-1:Worker@195] - Worker starting
2023-07-30 13:46:02,123 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:46:02,123 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.backup.internal
2023-07-30 13:46:02,123 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:46:02,123 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,124 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,125 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,125 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,125 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162124
2023-07-30 13:46:02,126 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:46:02,127 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:46:02,127 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 600000ms
2023-07-30 13:46:02,138 - INFO  [kafka-admin-client-thread | adminclient-28:AppInfoParser@83] - App info kafka.admin.client for adminclient-28 unregistered
2023-07-30 13:46:02,139 - INFO  [kafka-admin-client-thread | adminclient-28:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,139 - INFO  [kafka-admin-client-thread | adminclient-28:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,139 - INFO  [kafka-admin-client-thread | adminclient-28:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,140 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:02,143 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,143 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,143 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,143 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,143 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,143 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,144 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,144 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,144 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,144 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,144 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,144 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,144 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,144 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,144 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,145 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,145 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162144
2023-07-30 13:46:02,145 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:02,146 - INFO  [kafka-producer-network-thread | producer-8:Metadata@279] - [Producer clientId=producer-8] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,148 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,148 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,148 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162148
2023-07-30 13:46:02,151 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,155 - INFO  [DistributedHerder-connect-3-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Subscribed to partition(s): mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-21
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-0
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-5
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-10
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-20
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-15
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-9
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-11
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-16
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-4
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-17
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-3
2023-07-30 13:46:02,156 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-24
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-23
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-13
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-18
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-22
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-2
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-8
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-12
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-19
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-14
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-1
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-6
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-7
2023-07-30 13:46:02,157 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-21
2023-07-30 13:46:02,164 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,165 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,165 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,165 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,165 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,165 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,165 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,165 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,166 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,166 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,166 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,166 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,166 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,166 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,166 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,167 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,167 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,167 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,167 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,167 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,167 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,167 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,167 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,168 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,168 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,168 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:02,168 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:02,169 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:46:02,169 - INFO  [DistributedHerder-connect-3-1:Worker@202] - Worker started
2023-07-30 13:46:02,170 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.backup.internal
2023-07-30 13:46:02,170 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,173 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,173 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,173 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,173 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,173 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,173 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,173 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,174 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,174 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,174 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,174 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,174 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,174 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,174 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,174 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,174 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,175 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162174
Jul 30, 2023 1:46:02 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:46:02,186 - INFO  [kafka-admin-client-thread | adminclient-29:AppInfoParser@83] - App info kafka.admin.client for adminclient-29 unregistered
Jul 30, 2023 1:46:02 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:46:02 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
2023-07-30 13:46:02,186 - INFO  [kafka-admin-client-thread | adminclient-29:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,187 - INFO  [kafka-admin-client-thread | adminclient-29:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,187 - INFO  [kafka-admin-client-thread | adminclient-29:Metrics@678] - Metrics reporters closed
Jul 30, 2023 1:46:02 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:46:02,187 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:02,188 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,188 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,189 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,189 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,189 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,189 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,190 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,190 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,190 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,190 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,190 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,190 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,190 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,190 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,190 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,190 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,190 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162190
2023-07-30 13:46:02,191 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:02,192 - INFO  [kafka-producer-network-thread | producer-9:Metadata@279] - [Producer clientId=producer-9] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,192 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,193 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,193 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,193 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162193
2023-07-30 13:46:02,196 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,200 - INFO  [DistributedHerder-connect-3-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Subscribed to partition(s): mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2, mm2-status.backup.internal-3
2023-07-30 13:46:02,200 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-0
2023-07-30 13:46:02,200 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-4
2023-07-30 13:46:02,200 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-1
2023-07-30 13:46:02,200 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-2
2023-07-30 13:46:02,200 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-3
2023-07-30 13:46:02,205 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,206 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,206 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,206 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,206 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,206 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:02,206 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:02,207 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:46:02,208 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.backup.internal
2023-07-30 13:46:02,209 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,211 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,211 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,211 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,211 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,211 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,211 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,211 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,211 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,212 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,212 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,212 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,212 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,212 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,212 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,212 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,212 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,212 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162212
2023-07-30 13:46:02,224 - INFO  [kafka-admin-client-thread | adminclient-30:AppInfoParser@83] - App info kafka.admin.client for adminclient-30 unregistered
2023-07-30 13:46:02,225 - INFO  [kafka-admin-client-thread | adminclient-30:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,225 - INFO  [kafka-admin-client-thread | adminclient-30:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,225 - INFO  [kafka-admin-client-thread | adminclient-30:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,226 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:02,229 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,230 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,230 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,230 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,230 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,232 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,232 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,232 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162232
2023-07-30 13:46:02,233 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:02,234 - INFO  [kafka-producer-network-thread | producer-10:Metadata@279] - [Producer clientId=producer-10] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,234 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,235 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,235 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,235 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162235
2023-07-30 13:46:02,237 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,243 - INFO  [DistributedHerder-connect-3-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Subscribed to partition(s): mm2-configs.backup.internal-0
2023-07-30 13:46:02,243 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-configs.backup.internal-0
Jul 30, 2023 1:46:02 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:46:02,244 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@6f94fb9d{/,null,AVAILABLE}
2023-07-30 13:46:02,244 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:46:02,244 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:46:02,244 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:44117/'}
2023-07-30 13:46:02,248 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Resetting offset for partition mm2-configs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:02,253 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:02,254 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:02,255 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:46:02,256 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@290] - [Worker clientId=connect-3, groupId=backup-mm2] Herder started
2023-07-30 13:46:02,260 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Worker clientId=connect-3, groupId=backup-mm2] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:02,260 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-3, groupId=backup-mm2] Discovered group coordinator localhost:40623 (id: 2147483647 rack: null)
2023-07-30 13:46:02,261 - INFO  [DistributedHerder-connect-3-1:WorkerCoordinator@225] - [Worker clientId=connect-3, groupId=backup-mm2] Rebalance started
2023-07-30 13:46:02,262 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@534] - [Worker clientId=connect-3, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:46:02,264 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@468] - [Worker clientId=connect-3, groupId=backup-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:46:02,264 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@534] - [Worker clientId=connect-3, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:46:02,266 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group backup-mm2 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member connect-3-617ca672-0829-41dc-b8e2-9d66aba06a1c with group instance id None)
2023-07-30 13:46:02,561 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:40703/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"8pcqclJ2Qg-7GNfdkeMb6A"}
2023-07-30 13:46:02,576 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:38925/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"8pcqclJ2Qg-7GNfdkeMb6A"}
2023-07-30 13:46:02,586 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44117/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"8pcqclJ2Qg-7GNfdkeMb6A"}
2023-07-30 13:46:02,590 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2023-07-30 13:46:02,590 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2023-07-30 13:46:02,590 - INFO  [main:ZooKeeperServer@938] - minSessionTimeout set to 1600
2023-07-30 13:46:02,590 - INFO  [main:ZooKeeperServer@947] - maxSessionTimeout set to 16000
2023-07-30 13:46:02,590 - INFO  [main:ZooKeeperServer@166] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-2690557449841660752/version-2 snapdir /tmp/kafka-3112704166230188206/version-2
2023-07-30 13:46:02,591 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2023-07-30 13:46:02,591 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2023-07-30 13:46:02,592 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-3112704166230188206/version-2/snapshot.0
2023-07-30 13:46:02,593 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-3112704166230188206/version-2/snapshot.0
2023-07-30 13:46:02,595 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5301514646100141969/junit2114663958254445386
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33443
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:46:02,596 - INFO  [main:Logging@66] - starting
2023-07-30 13:46:02,597 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:33443
2023-07-30 13:46:02,597 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:33443.
2023-07-30 13:46:02,597 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:33443 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3ae126d1
2023-07-30 13:46:02,598 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2023-07-30 13:46:02,598 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2023-07-30 13:46:02,599 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2023-07-30 13:46:02,600 - INFO  [main-SendThread(127.0.0.1:33443):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:33443. Will not attempt to authenticate using SASL (unknown error)
2023-07-30 13:46:02,600 - INFO  [main-SendThread(127.0.0.1:33443):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:52942, server: localhost/127.0.0.1:33443
2023-07-30 13:46:02,602 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2023-07-30 13:46:02,603 - INFO  [main-SendThread(127.0.0.1:33443):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:33443, sessionid = 0x1087f2e484f0000, negotiated timeout = 16000
2023-07-30 13:46:02,603 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2023-07-30 13:46:02,634 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2023-07-30 13:46:02,636 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2023-07-30 13:46:02,636 - INFO  [feature-zk-node-event-process-thread:FinalizedFeatureCache$@42] - Cleared cache
2023-07-30 13:46:02,639 - INFO  [main:Logging@66] - Cluster ID = QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:02,639 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit5301514646100141969/junit2114663958254445386/meta.properties
2023-07-30 13:46:02,642 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5301514646100141969/junit2114663958254445386
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33443
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:46:02,644 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5301514646100141969/junit2114663958254445386
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33443
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:46:02,671 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2023-07-30 13:46:02,671 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2023-07-30 13:46:02,675 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2023-07-30 13:46:02,675 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2023-07-30 13:46:02,677 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit5301514646100141969/junit2114663958254445386)
2023-07-30 13:46:02,677 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit5301514646100141969/junit2114663958254445386 since no clean shutdown file was found
2023-07-30 13:46:02,678 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2023-07-30 13:46:02,678 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2023-07-30 13:46:02,679 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2023-07-30 13:46:02,736 - INFO  [main:Logging@66] - Created ConnectionAcceptRate sensor, quotaLimit=2147483647
2023-07-30 13:46:02,737 - INFO  [main:Logging@66] - Created ConnectionAcceptRate-PLAINTEXT sensor, quotaLimit=2147483647
2023-07-30 13:46:02,737 - INFO  [main:Logging@66] - Updated PLAINTEXT max connection creation rate to 2147483647
2023-07-30 13:46:02,738 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:43245.
2023-07-30 13:46:02,741 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:46:02,743 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2023-07-30 13:46:02,744 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2023-07-30 13:46:02,745 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2023-07-30 13:46:02,745 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2023-07-30 13:46:02,747 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2023-07-30 13:46:02,748 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Starting
2023-07-30 13:46:02,749 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2023-07-30 13:46:02,751 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 24,24,1690739162750,1690739162750,1,0,0,74449230609973248,204,0,24

2023-07-30 13:46:02,751 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:43245, czxid (broker epoch): 24
2023-07-30 13:46:02,756 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2023-07-30 13:46:02,757 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2023-07-30 13:46:02,758 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2023-07-30 13:46:02,759 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2023-07-30 13:46:02,760 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2023-07-30 13:46:02,762 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2023-07-30 13:46:02,764 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2023-07-30 13:46:02,765 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2023-07-30 13:46:02,766 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2023-07-30 13:46:02,767 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2023-07-30 13:46:02,768 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2023-07-30 13:46:02,775 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2023-07-30 13:46:02,778 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Starting socket server acceptors and processors
2023-07-30 13:46:02,782 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:46:02,782 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started socket server acceptors and processors
2023-07-30 13:46:02,783 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,783 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,783 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739157545
2023-07-30 13:46:02,784 - WARN  [main:AppInfoParser@68] - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.server:type=app-info,id=0
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:389)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:160)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:151)
	at kafka.utils.TestUtils.createServer(TestUtils.scala)
	at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.start(EmbeddedKafkaCluster.java:156)
	at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.start(EmbeddedKafkaCluster.java:134)
	at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.before(EmbeddedKafkaCluster.java:111)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:136)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2023-07-30 13:46:02,785 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2023-07-30 13:46:02,785 - INFO  [main:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:02,790 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,791 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,791 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162790
2023-07-30 13:46:02,791 - INFO  [main:EmbeddedConnectCluster@235] - Starting Connect cluster 'backup-connect-cluster' with 3 workers
2023-07-30 13:46:02,792 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:46:02,795 - INFO  [kafka-producer-network-thread | producer-11:Metadata@279] - [Producer clientId=producer-11] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:02,848 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Recorded new controller, from now on will use broker 0
2023-07-30 13:46:02,882 - INFO  [main:Reflections@239] - Reflections took 88 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:46:02,883 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:02,885 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:02,887 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:02,890 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:46:02,890 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:02,890 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:02,890 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,890 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,890 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,891 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:02,891 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:02,891 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,891 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,891 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:02,892 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.primary.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = primary-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.primary.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.primary.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:46:02,892 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:46:02,893 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,893 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,894 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,894 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,894 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,894 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,894 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,894 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,894 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,894 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,895 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,895 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,895 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,895 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,895 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,895 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,895 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162895
2023-07-30 13:46:02,901 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:02,901 - INFO  [kafka-admin-client-thread | adminclient-31:AppInfoParser@83] - App info kafka.admin.client for adminclient-31 unregistered
2023-07-30 13:46:02,902 - INFO  [kafka-admin-client-thread | adminclient-31:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,902 - INFO  [kafka-admin-client-thread | adminclient-31:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,902 - INFO  [kafka-admin-client-thread | adminclient-31:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,902 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:46:02,903 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:46:02,903 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:46:02,911 - INFO  [main:AbstractConnector@331] - Started http_localhost0@630b6190{HTTP/1.1, (http/1.1)}{localhost:37121}
2023-07-30 13:46:02,911 - INFO  [main:Server@400] - Started @6449ms
2023-07-30 13:46:02,912 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37121/
2023-07-30 13:46:02,912 - INFO  [main:RestServer@219] - REST server listening at http://localhost:37121/, advertising URL http://localhost:37121/
2023-07-30 13:46:02,912 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37121/
2023-07-30 13:46:02,912 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:37121/
2023-07-30 13:46:02,912 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37121/
2023-07-30 13:46:02,912 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,912 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,913 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,914 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,915 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,915 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,915 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162914
2023-07-30 13:46:02,919 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:02,919 - INFO  [kafka-admin-client-thread | adminclient-32:AppInfoParser@83] - App info kafka.admin.client for adminclient-32 unregistered
2023-07-30 13:46:02,920 - INFO  [kafka-admin-client-thread | adminclient-32:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,920 - INFO  [kafka-admin-client-thread | adminclient-32:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,920 - INFO  [kafka-admin-client-thread | adminclient-32:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,920 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:46:02,921 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,921 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,922 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,923 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,923 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,923 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,923 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,923 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162923
2023-07-30 13:46:02,927 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:02,928 - INFO  [kafka-admin-client-thread | adminclient-33:AppInfoParser@83] - App info kafka.admin.client for adminclient-33 unregistered
2023-07-30 13:46:02,928 - INFO  [kafka-admin-client-thread | adminclient-33:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,929 - INFO  [kafka-admin-client-thread | adminclient-33:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,929 - INFO  [kafka-admin-client-thread | adminclient-33:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,929 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,929 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,929 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162929
2023-07-30 13:46:02,930 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:02,930 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:02,930 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,930 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,931 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,931 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,931 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,931 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,932 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,932 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,932 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,932 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,932 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,932 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,932 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,932 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,932 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,932 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,932 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162932
2023-07-30 13:46:02,939 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:02,939 - INFO  [kafka-admin-client-thread | adminclient-34:AppInfoParser@83] - App info kafka.admin.client for adminclient-34 unregistered
2023-07-30 13:46:02,940 - INFO  [kafka-admin-client-thread | adminclient-34:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,940 - INFO  [kafka-admin-client-thread | adminclient-34:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,940 - INFO  [kafka-admin-client-thread | adminclient-34:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,940 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,940 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,941 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,941 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,941 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,941 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,942 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,942 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,942 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,942 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,942 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,942 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,942 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,942 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,942 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,942 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,942 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162942
2023-07-30 13:46:02,947 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:02,948 - INFO  [kafka-admin-client-thread | adminclient-35:AppInfoParser@83] - App info kafka.admin.client for adminclient-35 unregistered
2023-07-30 13:46:02,949 - INFO  [kafka-admin-client-thread | adminclient-35:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,949 - INFO  [kafka-admin-client-thread | adminclient-35:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,949 - INFO  [kafka-admin-client-thread | adminclient-35:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,949 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,950 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,951 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,952 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,952 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,952 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,952 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,952 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162952
2023-07-30 13:46:02,957 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:02,957 - INFO  [kafka-admin-client-thread | adminclient-36:AppInfoParser@83] - App info kafka.admin.client for adminclient-36 unregistered
2023-07-30 13:46:02,958 - INFO  [kafka-admin-client-thread | adminclient-36:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,958 - INFO  [kafka-admin-client-thread | adminclient-36:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,958 - INFO  [kafka-admin-client-thread | adminclient-36:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,958 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:02,959 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,960 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,961 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,961 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,961 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,961 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162961
2023-07-30 13:46:02,965 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:02,966 - INFO  [kafka-admin-client-thread | adminclient-37:AppInfoParser@83] - App info kafka.admin.client for adminclient-37 unregistered
2023-07-30 13:46:02,966 - INFO  [kafka-admin-client-thread | adminclient-37:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:02,966 - INFO  [kafka-admin-client-thread | adminclient-37:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:02,966 - INFO  [kafka-admin-client-thread | adminclient-37:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:02,967 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,967 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,968 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162967
2023-07-30 13:46:02,968 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 177ms
2023-07-30 13:46:02,968 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:46:02,968 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:46:02,968 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@286] - [Worker clientId=connect-4, groupId=primary-mm2] Herder starting
2023-07-30 13:46:02,968 - INFO  [DistributedHerder-connect-4-1:Worker@195] - Worker starting
2023-07-30 13:46:02,968 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:46:02,968 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.primary.internal
2023-07-30 13:46:02,968 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:46:02,969 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:02,969 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:02,970 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:02,970 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:02,970 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739162970
2023-07-30 13:46:02,971 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:46:02,972 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:46:02,972 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 660000ms
2023-07-30 13:46:02,979 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic mm2-offsets.primary.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0))
2023-07-30 13:46:03,015 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-21, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-7)
2023-07-30 13:46:03,018 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-17, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
2023-07-30 13:46:03,020 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-17 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:46:03,020 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-17 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-17
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
2023-07-30 13:46:03,020 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-17 broker=0] Log loaded for partition mm2-offsets.primary.internal-17 with initial high watermark 0
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:46:03,024 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-2, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,024 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-2 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,025 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-2
2023-07-30 13:46:03,025 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-2 broker=0] Log loaded for partition mm2-offsets.primary.internal-2 with initial high watermark 0
2023-07-30 13:46:03,032 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-21, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,033 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-21 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,033 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-21 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-21
2023-07-30 13:46:03,033 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-21 broker=0] Log loaded for partition mm2-offsets.primary.internal-21 with initial high watermark 0
2023-07-30 13:46:03,040 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-6, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,041 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-6 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,041 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-6 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-6
2023-07-30 13:46:03,041 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-6 broker=0] Log loaded for partition mm2-offsets.primary.internal-6 with initial high watermark 0
2023-07-30 13:46:03,048 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-10, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,049 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-10 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,049 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-10 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-10
2023-07-30 13:46:03,049 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-10 broker=0] Log loaded for partition mm2-offsets.primary.internal-10 with initial high watermark 0
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:46:03,056 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@4667c4c1{/,null,AVAILABLE}
2023-07-30 13:46:03,057 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:46:03,057 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:46:03,057 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:37121/'}
2023-07-30 13:46:03,057 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-14, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,057 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:46:03,058 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-14 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,058 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-14 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-14
2023-07-30 13:46:03,058 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-14 broker=0] Log loaded for partition mm2-offsets.primary.internal-14 with initial high watermark 0
2023-07-30 13:46:03,065 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-16, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,066 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-16 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,066 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-16 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-16
2023-07-30 13:46:03,066 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-16 broker=0] Log loaded for partition mm2-offsets.primary.internal-16 with initial high watermark 0
2023-07-30 13:46:03,073 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-1, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,074 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-1 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,074 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-1
2023-07-30 13:46:03,074 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-1 broker=0] Log loaded for partition mm2-offsets.primary.internal-1 with initial high watermark 0
2023-07-30 13:46:03,082 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-20, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,082 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-20 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,083 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-20 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-20
2023-07-30 13:46:03,083 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-20 broker=0] Log loaded for partition mm2-offsets.primary.internal-20 with initial high watermark 0
2023-07-30 13:46:03,090 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-5, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,091 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-5 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,091 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-5 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-5
2023-07-30 13:46:03,091 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-5 broker=0] Log loaded for partition mm2-offsets.primary.internal-5 with initial high watermark 0
2023-07-30 13:46:03,098 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-24, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,099 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-24 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,099 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-24 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-24
2023-07-30 13:46:03,099 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-24 broker=0] Log loaded for partition mm2-offsets.primary.internal-24 with initial high watermark 0
2023-07-30 13:46:03,107 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-9, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,107 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-9 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,107 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-9 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-9
2023-07-30 13:46:03,107 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-9 broker=0] Log loaded for partition mm2-offsets.primary.internal-9 with initial high watermark 0
2023-07-30 13:46:03,115 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-13, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,116 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-13 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,116 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-13 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-13
2023-07-30 13:46:03,116 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-13 broker=0] Log loaded for partition mm2-offsets.primary.internal-13 with initial high watermark 0
2023-07-30 13:46:03,123 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-15, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,124 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-15 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,124 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-15 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-15
2023-07-30 13:46:03,124 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-15 broker=0] Log loaded for partition mm2-offsets.primary.internal-15 with initial high watermark 0
2023-07-30 13:46:03,132 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,133 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-0 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,133 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-0
2023-07-30 13:46:03,133 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-0 broker=0] Log loaded for partition mm2-offsets.primary.internal-0 with initial high watermark 0
2023-07-30 13:46:03,140 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-19, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,141 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-19 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,141 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-19 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-19
2023-07-30 13:46:03,141 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-19 broker=0] Log loaded for partition mm2-offsets.primary.internal-19 with initial high watermark 0
2023-07-30 13:46:03,142 - INFO  [main:Reflections@239] - Reflections took 84 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:46:03,144 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:03,146 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:03,147 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:03,148 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-4, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,149 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-4 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,149 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-4
2023-07-30 13:46:03,149 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-4 broker=0] Log loaded for partition mm2-offsets.primary.internal-4 with initial high watermark 0
2023-07-30 13:46:03,150 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:46:03,150 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:03,150 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:03,150 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,151 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,151 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,151 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:03,151 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:03,151 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,151 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,152 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,152 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.primary.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = primary-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.primary.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.primary.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:46:03,152 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:46:03,152 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,153 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,153 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,153 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,154 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,154 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,154 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,154 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,154 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,154 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,154 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,154 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,155 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,155 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,155 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,155 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,155 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163155
2023-07-30 13:46:03,157 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-23, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,157 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-23 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,158 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-23 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-23
2023-07-30 13:46:03,158 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-23 broker=0] Log loaded for partition mm2-offsets.primary.internal-23 with initial high watermark 0
2023-07-30 13:46:03,159 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,159 - INFO  [kafka-admin-client-thread | adminclient-39:AppInfoParser@83] - App info kafka.admin.client for adminclient-39 unregistered
2023-07-30 13:46:03,160 - INFO  [kafka-admin-client-thread | adminclient-39:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,160 - INFO  [kafka-admin-client-thread | adminclient-39:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,160 - INFO  [kafka-admin-client-thread | adminclient-39:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,160 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:46:03,160 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:46:03,160 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:46:03,169 - INFO  [main:AbstractConnector@331] - Started http_localhost0@25d0cb3a{HTTP/1.1, (http/1.1)}{localhost:44923}
2023-07-30 13:46:03,169 - INFO  [main:Server@400] - Started @6707ms
2023-07-30 13:46:03,169 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-8, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,169 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44923/
2023-07-30 13:46:03,170 - INFO  [main:RestServer@219] - REST server listening at http://localhost:44923/, advertising URL http://localhost:44923/
2023-07-30 13:46:03,170 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44923/
2023-07-30 13:46:03,170 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:44923/
2023-07-30 13:46:03,170 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44923/
2023-07-30 13:46:03,170 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,170 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-8 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,170 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-8 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-8
2023-07-30 13:46:03,170 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,170 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-8 broker=0] Log loaded for partition mm2-offsets.primary.internal-8 with initial high watermark 0
2023-07-30 13:46:03,171 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,171 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,171 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,171 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,171 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,172 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,172 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,172 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,172 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,172 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,172 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,172 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,172 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,172 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,172 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163172
2023-07-30 13:46:03,173 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-12, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,174 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-12 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,174 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-12 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-12
2023-07-30 13:46:03,174 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-12 broker=0] Log loaded for partition mm2-offsets.primary.internal-12 with initial high watermark 0
2023-07-30 13:46:03,176 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,177 - INFO  [kafka-admin-client-thread | adminclient-40:AppInfoParser@83] - App info kafka.admin.client for adminclient-40 unregistered
2023-07-30 13:46:03,177 - INFO  [kafka-admin-client-thread | adminclient-40:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,177 - INFO  [kafka-admin-client-thread | adminclient-40:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,177 - INFO  [kafka-admin-client-thread | adminclient-40:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,178 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:46:03,178 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,178 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,179 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,179 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,179 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,179 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,179 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,180 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,180 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,180 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,180 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,180 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,180 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,180 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,180 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,180 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,180 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163180
2023-07-30 13:46:03,182 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-18, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,183 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-18 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,183 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-18 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-18
2023-07-30 13:46:03,183 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-18 broker=0] Log loaded for partition mm2-offsets.primary.internal-18 with initial high watermark 0
2023-07-30 13:46:03,184 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,185 - INFO  [kafka-admin-client-thread | adminclient-41:AppInfoParser@83] - App info kafka.admin.client for adminclient-41 unregistered
2023-07-30 13:46:03,185 - INFO  [kafka-admin-client-thread | adminclient-41:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,185 - INFO  [kafka-admin-client-thread | adminclient-41:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,185 - INFO  [kafka-admin-client-thread | adminclient-41:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,186 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,186 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,186 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163186
2023-07-30 13:46:03,186 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:03,187 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:03,187 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,187 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,188 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,189 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,189 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,189 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,189 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,189 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163189
2023-07-30 13:46:03,190 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-3, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,193 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-3 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,193 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-3
2023-07-30 13:46:03,193 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-3 broker=0] Log loaded for partition mm2-offsets.primary.internal-3 with initial high watermark 0
2023-07-30 13:46:03,196 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,196 - INFO  [kafka-admin-client-thread | adminclient-42:AppInfoParser@83] - App info kafka.admin.client for adminclient-42 unregistered
2023-07-30 13:46:03,197 - INFO  [kafka-admin-client-thread | adminclient-42:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,197 - INFO  [kafka-admin-client-thread | adminclient-42:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,197 - INFO  [kafka-admin-client-thread | adminclient-42:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,197 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,198 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,198 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-22, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,199 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-22 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,199 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,199 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-22 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-22
2023-07-30 13:46:03,199 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,199 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,199 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,199 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-22 broker=0] Log loaded for partition mm2-offsets.primary.internal-22 with initial high watermark 0
2023-07-30 13:46:03,199 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,199 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,200 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,200 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,200 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,200 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,200 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,200 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,200 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,200 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,200 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163200
2023-07-30 13:46:03,204 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,204 - INFO  [kafka-admin-client-thread | adminclient-43:AppInfoParser@83] - App info kafka.admin.client for adminclient-43 unregistered
2023-07-30 13:46:03,205 - INFO  [kafka-admin-client-thread | adminclient-43:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,205 - INFO  [kafka-admin-client-thread | adminclient-43:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,205 - INFO  [kafka-admin-client-thread | adminclient-43:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,205 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,206 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,206 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-7, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,207 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,207 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,207 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,207 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-7 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,207 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,207 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,207 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,207 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,207 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,208 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,208 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,208 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,208 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,207 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-7 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-7
2023-07-30 13:46:03,208 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,208 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,208 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163208
2023-07-30 13:46:03,208 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-7 broker=0] Log loaded for partition mm2-offsets.primary.internal-7 with initial high watermark 0
2023-07-30 13:46:03,212 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,212 - INFO  [kafka-admin-client-thread | adminclient-44:AppInfoParser@83] - App info kafka.admin.client for adminclient-44 unregistered
2023-07-30 13:46:03,213 - INFO  [kafka-admin-client-thread | adminclient-44:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,213 - INFO  [kafka-admin-client-thread | adminclient-44:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,213 - INFO  [kafka-admin-client-thread | adminclient-44:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,213 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,214 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,214 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,214 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,214 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,215 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-11, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,215 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,216 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,216 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163215
2023-07-30 13:46:03,217 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-11 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,217 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-11 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-11
2023-07-30 13:46:03,217 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-11 broker=0] Log loaded for partition mm2-offsets.primary.internal-11 with initial high watermark 0
2023-07-30 13:46:03,220 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,220 - INFO  [kafka-admin-client-thread | adminclient-45:AppInfoParser@83] - App info kafka.admin.client for adminclient-45 unregistered
2023-07-30 13:46:03,221 - INFO  [kafka-admin-client-thread | adminclient-45:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,221 - INFO  [kafka-admin-client-thread | adminclient-45:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,221 - INFO  [kafka-admin-client-thread | adminclient-45:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,222 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,222 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,222 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163222
2023-07-30 13:46:03,222 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 165ms
2023-07-30 13:46:03,222 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:46:03,222 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:46:03,222 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@286] - [Worker clientId=connect-5, groupId=primary-mm2] Herder starting
2023-07-30 13:46:03,223 - INFO  [DistributedHerder-connect-5-1:Worker@195] - Worker starting
2023-07-30 13:46:03,223 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:46:03,223 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.primary.internal
2023-07-30 13:46:03,223 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:46:03,223 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,225 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,226 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,226 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,226 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163226
2023-07-30 13:46:03,226 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:46:03,227 - INFO  [DistributedHerder-connect-4-1:TopicAdmin@284] - Created topic (name=mm2-offsets.primary.internal, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:43245
2023-07-30 13:46:03,227 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:46:03,227 - INFO  [kafka-admin-client-thread | adminclient-38:AppInfoParser@83] - App info kafka.admin.client for adminclient-38 unregistered
2023-07-30 13:46:03,227 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 600000ms
2023-07-30 13:46:03,228 - INFO  [kafka-admin-client-thread | adminclient-38:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,228 - INFO  [kafka-admin-client-thread | adminclient-38:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,228 - INFO  [kafka-admin-client-thread | adminclient-38:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,228 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:03,230 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,231 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,231 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,231 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163231
2023-07-30 13:46:03,232 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:03,232 - INFO  [kafka-producer-network-thread | producer-12:Metadata@279] - [Producer clientId=producer-12] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,233 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,234 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,234 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163233
2023-07-30 13:46:03,234 - INFO  [kafka-admin-client-thread | adminclient-46:AppInfoParser@83] - App info kafka.admin.client for adminclient-46 unregistered
2023-07-30 13:46:03,235 - INFO  [kafka-admin-client-thread | adminclient-46:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,235 - INFO  [kafka-admin-client-thread | adminclient-46:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,235 - INFO  [kafka-admin-client-thread | adminclient-46:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,235 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:03,236 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,237 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,239 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,239 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,239 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163239
2023-07-30 13:46:03,240 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:03,240 - INFO  [DistributedHerder-connect-4-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Subscribed to partition(s): mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-7, mm2-offsets.primary.internal-21
2023-07-30 13:46:03,240 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-0
2023-07-30 13:46:03,240 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-5
2023-07-30 13:46:03,240 - INFO  [kafka-producer-network-thread | producer-13:Metadata@279] - [Producer clientId=producer-13] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,240 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-10
2023-07-30 13:46:03,240 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-20
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-15
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-9
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-11
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-16
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-4
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-17
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-3
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-24
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-23
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-13
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-18
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-22
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-2
2023-07-30 13:46:03,241 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-8
2023-07-30 13:46:03,242 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-12
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,242 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-19
2023-07-30 13:46:03,242 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-14
2023-07-30 13:46:03,242 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-1
2023-07-30 13:46:03,242 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,242 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-6
2023-07-30 13:46:03,242 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,243 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-7
2023-07-30 13:46:03,243 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163242
2023-07-30 13:46:03,243 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-21
2023-07-30 13:46:03,245 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-5-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Subscribed to partition(s): mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-7, mm2-offsets.primary.internal-21
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-0
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-5
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-10
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-20
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-15
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-9
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,248 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-11
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-16
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-4
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-17
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-3
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-24
2023-07-30 13:46:03,249 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-23
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-13
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-18
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-22
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-2
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-8
2023-07-30 13:46:03,250 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-12
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-19
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-14
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-1
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-6
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-7
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-21
2023-07-30 13:46:03,251 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,252 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,252 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,252 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,252 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,252 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:03,252 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:03,252 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:46:03,254 - INFO  [DistributedHerder-connect-4-1:Worker@202] - Worker started
2023-07-30 13:46:03,254 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.primary.internal
2023-07-30 13:46:03,254 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,257 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,257 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,257 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,258 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,258 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,258 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163258
2023-07-30 13:46:03,263 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,263 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,263 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,263 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,263 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,263 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,263 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,263 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,264 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,265 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,265 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,265 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,265 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,265 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:03,265 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:03,265 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:46:03,266 - INFO  [DistributedHerder-connect-5-1:Worker@202] - Worker started
2023-07-30 13:46:03,267 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.primary.internal
2023-07-30 13:46:03,267 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,270 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,270 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,270 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,270 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,270 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,270 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,271 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,271 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,271 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,271 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,271 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,271 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,271 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,271 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,271 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,271 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,271 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163271
2023-07-30 13:46:03,272 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Creating topic mm2-status.primary.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2023-07-30 13:46:03,277 - INFO  [DistributedHerder-connect-5-1:TopicAdmin@400] - Unable to use admin client to verify the cleanup policy of 'mm2-status.primary.internal' topic is 'compact', either because the broker is an older version or because the Kafka principal used for Connect internal topics does not have the required permission to describe topic configurations.
2023-07-30 13:46:03,277 - INFO  [kafka-admin-client-thread | adminclient-48:AppInfoParser@83] - App info kafka.admin.client for adminclient-48 unregistered
2023-07-30 13:46:03,278 - INFO  [kafka-admin-client-thread | adminclient-48:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,278 - INFO  [kafka-admin-client-thread | adminclient-48:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,278 - INFO  [kafka-admin-client-thread | adminclient-48:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,278 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:03,280 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,281 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,281 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,281 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163281
2023-07-30 13:46:03,282 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:03,282 - INFO  [kafka-producer-network-thread | producer-14:Metadata@279] - [Producer clientId=producer-14] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,283 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,284 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,284 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,284 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,284 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,284 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163284
2023-07-30 13:46:03,285 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-status.primary.internal-4, mm2-status.primary.internal-1, mm2-status.primary.internal-3, mm2-status.primary.internal-0, mm2-status.primary.internal-2)
2023-07-30 13:46:03,285 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:46:03,287 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-status.primary.internal-1, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,288 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-status.primary.internal-1 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,289 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-1
2023-07-30 13:46:03,289 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-1 broker=0] Log loaded for partition mm2-status.primary.internal-1 with initial high watermark 0
2023-07-30 13:46:03,291 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-status.primary.internal-2, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,292 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-status.primary.internal-2 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,292 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-2
2023-07-30 13:46:03,292 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-2 broker=0] Log loaded for partition mm2-status.primary.internal-2 with initial high watermark 0
2023-07-30 13:46:03,300 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-status.primary.internal-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,301 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-status.primary.internal-0 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,301 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-0
2023-07-30 13:46:03,302 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-0 broker=0] Log loaded for partition mm2-status.primary.internal-0 with initial high watermark 0
2023-07-30 13:46:03,309 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-status.primary.internal-3, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,310 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-status.primary.internal-3 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,310 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-3
2023-07-30 13:46:03,310 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-3 broker=0] Log loaded for partition mm2-status.primary.internal-3 with initial high watermark 0
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:46:03,315 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@2fd64b11{/,null,AVAILABLE}
2023-07-30 13:46:03,315 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:46:03,315 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:46:03,316 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:44923/'}
2023-07-30 13:46:03,316 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:46:03,317 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-status.primary.internal-4, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,317 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-status.primary.internal-4 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,318 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-4
2023-07-30 13:46:03,318 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-status.primary.internal-4 broker=0] Log loaded for partition mm2-status.primary.internal-4 with initial high watermark 0
2023-07-30 13:46:03,326 - INFO  [DistributedHerder-connect-4-1:TopicAdmin@284] - Created topic (name=mm2-status.primary.internal, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:43245
2023-07-30 13:46:03,328 - INFO  [kafka-admin-client-thread | adminclient-47:AppInfoParser@83] - App info kafka.admin.client for adminclient-47 unregistered
2023-07-30 13:46:03,329 - INFO  [kafka-admin-client-thread | adminclient-47:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,329 - INFO  [kafka-admin-client-thread | adminclient-47:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,329 - INFO  [kafka-admin-client-thread | adminclient-47:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,329 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:03,330 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,331 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,331 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,331 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163331
2023-07-30 13:46:03,332 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:03,332 - INFO  [kafka-producer-network-thread | producer-15:Metadata@279] - [Producer clientId=producer-15] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,333 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,333 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,333 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,333 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,333 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,333 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,333 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,334 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,334 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,334 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,334 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,334 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,334 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,334 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,334 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,334 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163334
2023-07-30 13:46:03,335 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,338 - INFO  [DistributedHerder-connect-4-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Subscribed to partition(s): mm2-status.primary.internal-0, mm2-status.primary.internal-1, mm2-status.primary.internal-4, mm2-status.primary.internal-2, mm2-status.primary.internal-3
2023-07-30 13:46:03,338 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-0
2023-07-30 13:46:03,338 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-1
2023-07-30 13:46:03,338 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-4
2023-07-30 13:46:03,338 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-2
2023-07-30 13:46:03,338 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-3
2023-07-30 13:46:03,341 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,341 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,341 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,341 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,341 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,342 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:03,342 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:03,342 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:46:03,343 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.primary.internal
2023-07-30 13:46:03,343 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,346 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,346 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,346 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163346
2023-07-30 13:46:03,351 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic mm2-configs.primary.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:46:03,358 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.primary.internal-0)
2023-07-30 13:46:03,360 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-configs.primary.internal-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,360 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-configs.primary.internal-0 in /tmp/junit5301514646100141969/junit2114663958254445386/mm2-configs.primary.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,361 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-configs.primary.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-configs.primary.internal-0
2023-07-30 13:46:03,361 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-configs.primary.internal-0 broker=0] Log loaded for partition mm2-configs.primary.internal-0 with initial high watermark 0
2023-07-30 13:46:03,365 - INFO  [DistributedHerder-connect-4-1:TopicAdmin@284] - Created topic (name=mm2-configs.primary.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:43245
2023-07-30 13:46:03,365 - INFO  [kafka-admin-client-thread | adminclient-49:AppInfoParser@83] - App info kafka.admin.client for adminclient-49 unregistered
2023-07-30 13:46:03,365 - INFO  [kafka-admin-client-thread | adminclient-49:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,365 - INFO  [kafka-admin-client-thread | adminclient-49:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,366 - INFO  [kafka-admin-client-thread | adminclient-49:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,366 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:03,367 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,368 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,369 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,369 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,369 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,369 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,369 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,369 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163369
2023-07-30 13:46:03,369 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:03,369 - INFO  [kafka-producer-network-thread | producer-16:Metadata@279] - [Producer clientId=producer-16] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,371 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,372 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,372 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163371
2023-07-30 13:46:03,373 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,376 - INFO  [DistributedHerder-connect-4-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Subscribed to partition(s): mm2-configs.primary.internal-0
2023-07-30 13:46:03,376 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-configs.primary.internal-0
2023-07-30 13:46:03,379 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Resetting offset for partition mm2-configs.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,379 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:03,379 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:03,379 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:46:03,379 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@290] - [Worker clientId=connect-4, groupId=primary-mm2] Herder started
2023-07-30 13:46:03,385 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Worker clientId=connect-4, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,388 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0))
2023-07-30 13:46:03,390 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2023-07-30 13:46:03,416 - INFO  [main:Reflections@239] - Reflections took 99 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:46:03,417 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:03,419 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:03,420 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:46:03,424 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40)
2023-07-30 13:46:03,425 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:46:03,425 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:03,425 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:03,425 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,425 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,426 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,427 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:03,429 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:03,429 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,429 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,429 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,429 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:03,430 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,430 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.primary.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = primary-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.primary.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.primary.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:46:03,430 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2023-07-30 13:46:03,430 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:46:03,430 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2023-07-30 13:46:03,431 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,431 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,432 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,432 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,432 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,432 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,432 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,432 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,432 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,433 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,433 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,433 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,433 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,433 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,433 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,433 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,433 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163433
2023-07-30 13:46:03,433 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-18, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,434 - INFO  [DistributedHerder-connect-5-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Subscribed to partition(s): mm2-status.primary.internal-0, mm2-status.primary.internal-1, mm2-status.primary.internal-4, mm2-status.primary.internal-2, mm2-status.primary.internal-3
2023-07-30 13:46:03,434 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-0
2023-07-30 13:46:03,434 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-1
2023-07-30 13:46:03,434 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-4
2023-07-30 13:46:03,434 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-2
2023-07-30 13:46:03,434 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-3
2023-07-30 13:46:03,434 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-18 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,434 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18
2023-07-30 13:46:03,434 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0
2023-07-30 13:46:03,436 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,437 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,437 - INFO  [kafka-admin-client-thread | adminclient-50:AppInfoParser@83] - App info kafka.admin.client for adminclient-50 unregistered
2023-07-30 13:46:03,437 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,437 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,437 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,437 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,437 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:03,437 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:03,439 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:46:03,439 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.primary.internal
2023-07-30 13:46:03,439 - INFO  [kafka-admin-client-thread | adminclient-50:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,439 - INFO  [kafka-admin-client-thread | adminclient-50:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,440 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,440 - INFO  [kafka-admin-client-thread | adminclient-50:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,442 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:46:03,442 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:46:03,442 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,443 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,444 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,444 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,444 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163444
2023-07-30 13:46:03,444 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-41, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,445 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-41 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,445 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41
2023-07-30 13:46:03,445 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0
2023-07-30 13:46:03,450 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-10, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,450 - INFO  [kafka-admin-client-thread | adminclient-51:AppInfoParser@83] - App info kafka.admin.client for adminclient-51 unregistered
2023-07-30 13:46:03,450 - INFO  [main:AbstractConnector@331] - Started http_localhost0@1984212d{HTTP/1.1, (http/1.1)}{localhost:37429}
2023-07-30 13:46:03,451 - INFO  [main:Server@400] - Started @6989ms
2023-07-30 13:46:03,451 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-10 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,451 - INFO  [kafka-admin-client-thread | adminclient-51:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,451 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10
2023-07-30 13:46:03,451 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37429/
2023-07-30 13:46:03,451 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0
2023-07-30 13:46:03,451 - INFO  [kafka-admin-client-thread | adminclient-51:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,451 - INFO  [main:RestServer@219] - REST server listening at http://localhost:37429/, advertising URL http://localhost:37429/
2023-07-30 13:46:03,451 - INFO  [kafka-admin-client-thread | adminclient-51:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,451 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37429/
2023-07-30 13:46:03,452 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:37429/
2023-07-30 13:46:03,452 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37429/
2023-07-30 13:46:03,452 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:03,452 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,452 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,453 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,454 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,454 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,454 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,456 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,456 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,456 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,456 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,455 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,456 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,456 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,456 - INFO  [kafka-producer-network-thread | producer-17:Metadata@279] - [Producer clientId=producer-17] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,457 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,457 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,457 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163457
2023-07-30 13:46:03,456 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,457 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,457 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,457 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163457
2023-07-30 13:46:03,458 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:03,458 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-33, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,459 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-33 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,459 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33
2023-07-30 13:46:03,459 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0
2023-07-30 13:46:03,459 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,460 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,460 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,460 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,460 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163460
2023-07-30 13:46:03,460 - INFO  [kafka-admin-client-thread | adminclient-52:AppInfoParser@83] - App info kafka.admin.client for adminclient-52 unregistered
2023-07-30 13:46:03,461 - INFO  [kafka-admin-client-thread | adminclient-52:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,461 - INFO  [kafka-admin-client-thread | adminclient-52:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,461 - INFO  [kafka-admin-client-thread | adminclient-52:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,462 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:46:03,462 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,462 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,462 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,463 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,463 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,463 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,463 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,464 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,464 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,464 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,464 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,464 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,464 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,464 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,464 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,464 - INFO  [DistributedHerder-connect-5-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Subscribed to partition(s): mm2-configs.primary.internal-0
2023-07-30 13:46:03,465 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,465 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,465 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163465
2023-07-30 13:46:03,465 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-configs.primary.internal-0
2023-07-30 13:46:03,467 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-48, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,468 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-48 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,468 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48
2023-07-30 13:46:03,468 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0
2023-07-30 13:46:03,468 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,468 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Resetting offset for partition mm2-configs.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,468 - INFO  [kafka-admin-client-thread | adminclient-53:AppInfoParser@83] - App info kafka.admin.client for adminclient-53 unregistered
2023-07-30 13:46:03,469 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:03,469 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:03,469 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:46:03,470 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@290] - [Worker clientId=connect-5, groupId=primary-mm2] Herder started
2023-07-30 13:46:03,471 - INFO  [kafka-admin-client-thread | adminclient-53:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,473 - INFO  [kafka-admin-client-thread | adminclient-53:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,473 - INFO  [kafka-admin-client-thread | adminclient-53:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,473 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,473 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,473 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163473
2023-07-30 13:46:03,474 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:03,474 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:46:03,474 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,474 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Worker clientId=connect-5, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,475 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,475 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-19, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,475 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,476 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,475 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-19 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,477 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,477 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,477 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19
2023-07-30 13:46:03,477 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163477
2023-07-30 13:46:03,477 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0
2023-07-30 13:46:03,480 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,480 - INFO  [kafka-admin-client-thread | adminclient-54:AppInfoParser@83] - App info kafka.admin.client for adminclient-54 unregistered
2023-07-30 13:46:03,481 - INFO  [kafka-admin-client-thread | adminclient-54:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,481 - INFO  [kafka-admin-client-thread | adminclient-54:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,481 - INFO  [kafka-admin-client-thread | adminclient-54:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,481 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,482 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,483 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,483 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,483 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,483 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,483 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,483 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,483 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,483 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,484 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,484 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,484 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,484 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,483 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-34, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,484 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,484 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,484 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163484
2023-07-30 13:46:03,485 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-34 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,485 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34
2023-07-30 13:46:03,485 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0
2023-07-30 13:46:03,487 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,488 - INFO  [kafka-admin-client-thread | adminclient-55:AppInfoParser@83] - App info kafka.admin.client for adminclient-55 unregistered
2023-07-30 13:46:03,488 - INFO  [kafka-admin-client-thread | adminclient-55:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,488 - INFO  [kafka-admin-client-thread | adminclient-55:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,488 - INFO  [kafka-admin-client-thread | adminclient-55:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,489 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,489 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,490 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,490 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,490 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,490 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,490 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,491 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,491 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,491 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,491 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,491 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,491 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,491 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,491 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,491 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,491 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163491
2023-07-30 13:46:03,492 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,492 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,492 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2023-07-30 13:46:03,493 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2023-07-30 13:46:03,497 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,497 - INFO  [kafka-admin-client-thread | adminclient-56:AppInfoParser@83] - App info kafka.admin.client for adminclient-56 unregistered
2023-07-30 13:46:03,498 - INFO  [kafka-admin-client-thread | adminclient-56:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,498 - INFO  [kafka-admin-client-thread | adminclient-56:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,498 - INFO  [kafka-admin-client-thread | adminclient-56:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,498 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:46:03,499 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,499 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,501 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,500 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-11, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,501 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,501 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,501 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163501
2023-07-30 13:46:03,501 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-11 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,501 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11
2023-07-30 13:46:03,501 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0
2023-07-30 13:46:03,504 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,504 - INFO  [kafka-admin-client-thread | adminclient-57:AppInfoParser@83] - App info kafka.admin.client for adminclient-57 unregistered
2023-07-30 13:46:03,505 - INFO  [kafka-admin-client-thread | adminclient-57:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,505 - INFO  [kafka-admin-client-thread | adminclient-57:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,505 - INFO  [kafka-admin-client-thread | adminclient-57:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,506 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,506 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,506 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163506
2023-07-30 13:46:03,507 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 191ms
2023-07-30 13:46:03,507 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:46:03,507 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:46:03,507 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@286] - [Worker clientId=connect-6, groupId=primary-mm2] Herder starting
2023-07-30 13:46:03,507 - INFO  [DistributedHerder-connect-6-1:Worker@195] - Worker starting
2023-07-30 13:46:03,507 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:46:03,507 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.primary.internal
2023-07-30 13:46:03,507 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:46:03,507 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,508 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,508 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,508 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,508 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,508 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-26, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,508 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,509 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,509 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,509 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163509
2023-07-30 13:46:03,509 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-26 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,509 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:46:03,509 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26
2023-07-30 13:46:03,509 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:46:03,510 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0
2023-07-30 13:46:03,510 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 660000ms
2023-07-30 13:46:03,515 - INFO  [kafka-admin-client-thread | adminclient-58:AppInfoParser@83] - App info kafka.admin.client for adminclient-58 unregistered
2023-07-30 13:46:03,516 - INFO  [kafka-admin-client-thread | adminclient-58:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,517 - INFO  [kafka-admin-client-thread | adminclient-58:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,517 - INFO  [kafka-admin-client-thread | adminclient-58:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,517 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-49, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,517 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-49 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,517 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:03,517 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49
2023-07-30 13:46:03,518 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0
2023-07-30 13:46:03,519 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,520 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,521 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,521 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,521 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163521
2023-07-30 13:46:03,521 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:03,522 - INFO  [kafka-producer-network-thread | producer-18:Metadata@279] - [Producer clientId=producer-18] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,523 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,524 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,524 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163523
2023-07-30 13:46:03,525 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-39, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,525 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,526 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-39 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,526 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39
2023-07-30 13:46:03,526 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0
2023-07-30 13:46:03,528 - INFO  [DistributedHerder-connect-6-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Subscribed to partition(s): mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-7, mm2-offsets.primary.internal-21
2023-07-30 13:46:03,528 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-0
2023-07-30 13:46:03,528 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-5
2023-07-30 13:46:03,528 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-10
2023-07-30 13:46:03,528 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-20
2023-07-30 13:46:03,528 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-15
2023-07-30 13:46:03,528 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-9
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-11
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-16
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-4
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-17
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-3
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-24
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-23
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-13
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-18
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-22
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-2
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-8
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-12
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-19
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-14
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-1
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-6
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-7
2023-07-30 13:46:03,529 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-21
2023-07-30 13:46:03,533 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-9, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,534 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,534 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-9 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,534 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,534 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9
2023-07-30 13:46:03,535 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,535 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0
2023-07-30 13:46:03,535 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,535 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,535 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,535 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,535 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,535 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,536 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,536 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,536 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,536 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,536 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,536 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,536 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,537 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,537 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,537 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,537 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,537 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,537 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,537 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,537 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,537 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,538 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:03,538 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:03,539 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:46:03,539 - INFO  [DistributedHerder-connect-6-1:Worker@202] - Worker started
2023-07-30 13:46:03,539 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.primary.internal
2023-07-30 13:46:03,539 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,542 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,543 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,543 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,543 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163543
2023-07-30 13:46:03,544 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-24, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,545 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-24 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,545 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24
2023-07-30 13:46:03,545 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0
2023-07-30 13:46:03,549 - INFO  [kafka-admin-client-thread | adminclient-59:AppInfoParser@83] - App info kafka.admin.client for adminclient-59 unregistered
2023-07-30 13:46:03,550 - INFO  [kafka-admin-client-thread | adminclient-59:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,550 - INFO  [kafka-admin-client-thread | adminclient-59:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,550 - INFO  [kafka-admin-client-thread | adminclient-59:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,551 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-31, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,551 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:03,552 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-31 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,552 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31
2023-07-30 13:46:03,552 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
2023-07-30 13:46:03,553 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,554 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,554 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,554 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163554
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
2023-07-30 13:46:03,555 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:03,555 - INFO  [kafka-producer-network-thread | producer-19:Metadata@279] - [Producer clientId=producer-19] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,557 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,557 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,557 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163557
2023-07-30 13:46:03,559 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-46, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,559 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,560 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-46 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,560 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46
2023-07-30 13:46:03,560 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0
2023-07-30 13:46:03,563 - INFO  [DistributedHerder-connect-6-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Subscribed to partition(s): mm2-status.primary.internal-0, mm2-status.primary.internal-1, mm2-status.primary.internal-4, mm2-status.primary.internal-2, mm2-status.primary.internal-3
2023-07-30 13:46:03,563 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-0
2023-07-30 13:46:03,563 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-1
2023-07-30 13:46:03,563 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-4
2023-07-30 13:46:03,563 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-2
2023-07-30 13:46:03,563 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-3
2023-07-30 13:46:03,567 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,567 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,567 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,567 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,567 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2023-07-30 13:46:03,567 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,567 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2023-07-30 13:46:03,568 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,568 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,568 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:03,568 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:03,570 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:46:03,570 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.primary.internal
2023-07-30 13:46:03,571 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,573 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,573 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,573 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,573 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,574 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,574 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,574 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163574
2023-07-30 13:46:03,575 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-16, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,576 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-16 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,576 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16
2023-07-30 13:46:03,576 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0
2023-07-30 13:46:03,580 - INFO  [kafka-admin-client-thread | adminclient-60:AppInfoParser@83] - App info kafka.admin.client for adminclient-60 unregistered
2023-07-30 13:46:03,580 - INFO  [kafka-admin-client-thread | adminclient-60:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,580 - INFO  [kafka-admin-client-thread | adminclient-60:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,580 - INFO  [kafka-admin-client-thread | adminclient-60:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,581 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:03,582 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,583 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,584 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,584 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,584 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,584 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163584
2023-07-30 13:46:03,584 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:03,584 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,584 - INFO  [kafka-producer-network-thread | producer-20:Metadata@279] - [Producer clientId=producer-20] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,585 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,585 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2023-07-30 13:46:03,585 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2023-07-30 13:46:03,585 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,585 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:03,585 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:46:03,586 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,586 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,586 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163586
Jul 30, 2023 1:46:03 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:46:03,587 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@36f7d7b{/,null,AVAILABLE}
2023-07-30 13:46:03,588 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,588 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:46:03,588 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:46:03,588 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:37429/'}
2023-07-30 13:46:03,590 - INFO  [DistributedHerder-connect-6-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Subscribed to partition(s): mm2-configs.primary.internal-0
2023-07-30 13:46:03,590 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-configs.primary.internal-0
2023-07-30 13:46:03,592 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-25, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,593 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-25 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,593 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25
2023-07-30 13:46:03,593 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0
2023-07-30 13:46:03,595 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:40703/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"8pcqclJ2Qg-7GNfdkeMb6A"}
2023-07-30 13:46:03,595 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Resetting offset for partition mm2-configs.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:03,601 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:03,601 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:03,602 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:46:03,602 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@290] - [Worker clientId=connect-6, groupId=primary-mm2] Herder started
2023-07-30 13:46:03,611 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-40, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,611 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:38925/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"8pcqclJ2Qg-7GNfdkeMb6A"}
2023-07-30 13:46:03,612 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Worker clientId=connect-6, groupId=primary-mm2] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:03,612 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-40 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,612 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40
2023-07-30 13:46:03,612 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0
2023-07-30 13:46:03,617 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44117/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"8pcqclJ2Qg-7GNfdkeMb6A"}
2023-07-30 13:46:03,617 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-47, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,618 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,618 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-47 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,618 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47
2023-07-30 13:46:03,618 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0
2023-07-30 13:46:03,619 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,619 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,619 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163619
2023-07-30 13:46:03,623 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:46:03,625 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-17, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,626 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-17 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,626 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17
2023-07-30 13:46:03,626 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0
2023-07-30 13:46:03,633 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-32, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,634 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-32 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,634 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32
2023-07-30 13:46:03,634 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0
2023-07-30 13:46:03,636 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(test-topic-1-0, test-topic-1-3, test-topic-1-9, test-topic-1-5, test-topic-1-1, test-topic-1-6, test-topic-1-2, test-topic-1-8, test-topic-1-7, test-topic-1-4)
2023-07-30 13:46:03,637 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-0, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,638 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-0 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,638 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition test-topic-1-0
2023-07-30 13:46:03,638 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-0 broker=0] Log loaded for partition test-topic-1-0 with initial high watermark 0
2023-07-30 13:46:03,641 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-9, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,641 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-37, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,642 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-9 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,642 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-37 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,642 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-9 broker=0] No checkpointed highwatermark is found for partition test-topic-1-9
2023-07-30 13:46:03,642 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37
2023-07-30 13:46:03,642 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-9 broker=0] Log loaded for partition test-topic-1-9 with initial high watermark 0
2023-07-30 13:46:03,642 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0
2023-07-30 13:46:03,650 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-7, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,650 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-7, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,651 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-7 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,651 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7
2023-07-30 13:46:03,651 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0
2023-07-30 13:46:03,651 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-7 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,651 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-7 broker=0] No checkpointed highwatermark is found for partition test-topic-1-7
2023-07-30 13:46:03,651 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-7 broker=0] Log loaded for partition test-topic-1-7 with initial high watermark 0
2023-07-30 13:46:03,658 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-22, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,658 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-8, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,659 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-22 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,659 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22
2023-07-30 13:46:03,659 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-8 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,659 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0
2023-07-30 13:46:03,659 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-8 broker=0] No checkpointed highwatermark is found for partition test-topic-1-8
2023-07-30 13:46:03,659 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-8 broker=0] Log loaded for partition test-topic-1-8 with initial high watermark 0
2023-07-30 13:46:03,666 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-29, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,666 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-5, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,667 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-29 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,667 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29
2023-07-30 13:46:03,667 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-5 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,667 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0
2023-07-30 13:46:03,667 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-5 broker=0] No checkpointed highwatermark is found for partition test-topic-1-5
2023-07-30 13:46:03,667 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-5 broker=0] Log loaded for partition test-topic-1-5 with initial high watermark 0
2023-07-30 13:46:03,674 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-44, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,675 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-6, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,675 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-44 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,675 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44
2023-07-30 13:46:03,675 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0
2023-07-30 13:46:03,675 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-6 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,675 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-6 broker=0] No checkpointed highwatermark is found for partition test-topic-1-6
2023-07-30 13:46:03,675 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-6 broker=0] Log loaded for partition test-topic-1-6 with initial high watermark 0
2023-07-30 13:46:03,683 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-14, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,683 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-3, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,683 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-14 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,684 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14
2023-07-30 13:46:03,684 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-3 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,684 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0
2023-07-30 13:46:03,684 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-3 broker=0] No checkpointed highwatermark is found for partition test-topic-1-3
2023-07-30 13:46:03,684 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-3 broker=0] Log loaded for partition test-topic-1-3 with initial high watermark 0
2023-07-30 13:46:03,691 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-23, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,691 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-4, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,692 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-23 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,692 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23
2023-07-30 13:46:03,692 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-4 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,692 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0
2023-07-30 13:46:03,692 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-4 broker=0] No checkpointed highwatermark is found for partition test-topic-1-4
2023-07-30 13:46:03,692 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-4 broker=0] Log loaded for partition test-topic-1-4 with initial high watermark 0
2023-07-30 13:46:03,742 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-1, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,742 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-38, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,743 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-38 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,743 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-1 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,743 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38
2023-07-30 13:46:03,743 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-1 broker=0] No checkpointed highwatermark is found for partition test-topic-1-1
2023-07-30 13:46:03,743 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0
2023-07-30 13:46:03,743 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-1 broker=0] Log loaded for partition test-topic-1-1 with initial high watermark 0
2023-07-30 13:46:03,746 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-8, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,746 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-1-2, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,746 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-8 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,746 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-1-2 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-1-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,746 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8
2023-07-30 13:46:03,746 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-2 broker=0] No checkpointed highwatermark is found for partition test-topic-1-2
2023-07-30 13:46:03,746 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0
2023-07-30 13:46:03,746 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-1-2 broker=0] Log loaded for partition test-topic-1-2 with initial high watermark 0
2023-07-30 13:46:03,754 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-45, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,755 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-45 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,755 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45
2023-07-30 13:46:03,755 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0
2023-07-30 13:46:03,755 - INFO  [kafka-admin-client-thread | adminclient-61:AppInfoParser@83] - App info kafka.admin.client for adminclient-61 unregistered
2023-07-30 13:46:03,756 - INFO  [kafka-admin-client-thread | adminclient-61:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,756 - INFO  [kafka-admin-client-thread | adminclient-61:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,756 - INFO  [kafka-admin-client-thread | adminclient-61:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,757 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,757 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,758 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,758 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163757
2023-07-30 13:46:03,764 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic backup.test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:46:03,770 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-15, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,771 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-15 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,771 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15
2023-07-30 13:46:03,771 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0
2023-07-30 13:46:03,774 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-30, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,774 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-30 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,775 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30
2023-07-30 13:46:03,775 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0
2023-07-30 13:46:03,776 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(backup.test-topic-1-0)
2023-07-30 13:46:03,778 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=backup.test-topic-1-0, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,779 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition backup.test-topic-1-0 in /tmp/junit6154326205091447453/junit85516104450203242/backup.test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,779 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition backup.test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition backup.test-topic-1-0
2023-07-30 13:46:03,779 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition backup.test-topic-1-0 broker=0] Log loaded for partition backup.test-topic-1-0 with initial high watermark 0
2023-07-30 13:46:03,782 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,783 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,784 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2023-07-30 13:46:03,784 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2023-07-30 13:46:03,784 - INFO  [kafka-admin-client-thread | adminclient-62:AppInfoParser@83] - App info kafka.admin.client for adminclient-62 unregistered
2023-07-30 13:46:03,784 - INFO  [kafka-admin-client-thread | adminclient-62:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,784 - INFO  [kafka-admin-client-thread | adminclient-62:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,784 - INFO  [kafka-admin-client-thread | adminclient-62:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,785 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,786 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,786 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,786 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163786
2023-07-30 13:46:03,790 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic heartbeats with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:46:03,791 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-35, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,791 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-35 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,791 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35
2023-07-30 13:46:03,791 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0
2023-07-30 13:46:03,795 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(heartbeats-0)
2023-07-30 13:46:03,797 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=heartbeats-0, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,797 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition heartbeats-0 in /tmp/junit6154326205091447453/junit85516104450203242/heartbeats-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,798 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition heartbeats-0 broker=0] No checkpointed highwatermark is found for partition heartbeats-0
2023-07-30 13:46:03,798 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition heartbeats-0 broker=0] Log loaded for partition heartbeats-0 with initial high watermark 0
2023-07-30 13:46:03,799 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-5, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,800 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-5 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,800 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5
2023-07-30 13:46:03,800 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0
2023-07-30 13:46:03,807 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-20, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,809 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-20 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,809 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20
2023-07-30 13:46:03,809 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0
2023-07-30 13:46:03,809 - INFO  [kafka-admin-client-thread | adminclient-63:AppInfoParser@83] - App info kafka.admin.client for adminclient-63 unregistered
2023-07-30 13:46:03,810 - INFO  [kafka-admin-client-thread | adminclient-63:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,810 - INFO  [kafka-admin-client-thread | adminclient-63:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,810 - INFO  [kafka-admin-client-thread | adminclient-63:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,810 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,811 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,811 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,811 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163811
2023-07-30 13:46:03,815 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:46:03,816 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-27, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,816 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-27 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,816 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27
2023-07-30 13:46:03,817 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0
2023-07-30 13:46:03,824 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-42, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,824 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-42 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,825 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42
2023-07-30 13:46:03,825 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0
2023-07-30 13:46:03,832 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-12, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,833 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-12 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,833 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12
2023-07-30 13:46:03,833 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0
2023-07-30 13:46:03,840 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-21, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,841 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-21 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,841 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21
2023-07-30 13:46:03,841 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0
2023-07-30 13:46:03,849 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-36, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,850 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-36 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,850 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36
2023-07-30 13:46:03,850 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0
2023-07-30 13:46:03,857 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-6, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,858 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-6 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,858 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6
2023-07-30 13:46:03,858 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0
2023-07-30 13:46:03,865 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-43, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,866 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-43 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,866 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43
2023-07-30 13:46:03,866 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0
2023-07-30 13:46:03,874 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-13, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,874 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-13 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,874 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13
2023-07-30 13:46:03,874 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0
2023-07-30 13:46:03,882 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=__consumer_offsets-28, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,882 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition __consumer_offsets-28 in /tmp/junit5301514646100141969/junit2114663958254445386/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,882 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28
2023-07-30 13:46:03,882 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0
2023-07-30 13:46:03,889 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2023-07-30 13:46:03,890 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18
2023-07-30 13:46:03,890 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41
2023-07-30 13:46:03,890 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,890 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10
2023-07-30 13:46:03,890 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33
2023-07-30 13:46:03,890 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,891 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48
2023-07-30 13:46:03,891 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,891 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19
2023-07-30 13:46:03,891 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34
2023-07-30 13:46:03,891 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,891 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2023-07-30 13:46:03,891 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11
2023-07-30 13:46:03,891 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,891 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26
2023-07-30 13:46:03,891 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49
2023-07-30 13:46:03,891 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,891 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39
2023-07-30 13:46:03,892 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,892 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,892 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,892 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,892 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,892 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,892 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,892 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9
2023-07-30 13:46:03,893 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24
2023-07-30 13:46:03,893 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,893 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31
2023-07-30 13:46:03,893 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46
2023-07-30 13:46:03,893 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,893 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2023-07-30 13:46:03,893 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16
2023-07-30 13:46:03,893 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,893 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2023-07-30 13:46:03,893 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25
2023-07-30 13:46:03,893 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,893 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40
2023-07-30 13:46:03,893 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47
2023-07-30 13:46:03,894 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32
2023-07-30 13:46:03,894 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37
2023-07-30 13:46:03,894 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7
2023-07-30 13:46:03,894 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22
2023-07-30 13:46:03,894 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44
2023-07-30 13:46:03,894 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23
2023-07-30 13:46:03,894 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38
2023-07-30 13:46:03,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8
2023-07-30 13:46:03,894 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15
2023-07-30 13:46:03,895 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2023-07-30 13:46:03,895 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27
2023-07-30 13:46:03,895 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12
2023-07-30 13:46:03,895 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36
2023-07-30 13:46:03,895 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6
2023-07-30 13:46:03,896 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43
2023-07-30 13:46:03,896 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 2 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,896 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13
2023-07-30 13:46:03,896 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,896 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28
2023-07-30 13:46:03,896 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,896 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,896 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,896 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,896 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,896 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,897 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 2 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,897 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,897 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,897 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,897 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,897 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,897 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,897 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,897 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,898 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 2 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,898 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:46:03,899 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(test-topic-1-0, test-topic-1-3, test-topic-1-9, test-topic-1-5, test-topic-1-1, test-topic-1-6, test-topic-1-2, test-topic-1-8, test-topic-1-7, test-topic-1-4)
2023-07-30 13:46:03,901 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,902 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-0 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,902 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition test-topic-1-0
2023-07-30 13:46:03,902 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-0 broker=0] Log loaded for partition test-topic-1-0 with initial high watermark 0
2023-07-30 13:46:03,905 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-9, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,905 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-9 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,906 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-9 broker=0] No checkpointed highwatermark is found for partition test-topic-1-9
2023-07-30 13:46:03,906 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-9 broker=0] Log loaded for partition test-topic-1-9 with initial high watermark 0
2023-07-30 13:46:03,912 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-7, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,913 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-7 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,913 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-7 broker=0] No checkpointed highwatermark is found for partition test-topic-1-7
2023-07-30 13:46:03,913 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-7 broker=0] Log loaded for partition test-topic-1-7 with initial high watermark 0
2023-07-30 13:46:03,921 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-8, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,921 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-8 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,921 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-8 broker=0] No checkpointed highwatermark is found for partition test-topic-1-8
2023-07-30 13:46:03,921 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-8 broker=0] Log loaded for partition test-topic-1-8 with initial high watermark 0
2023-07-30 13:46:03,929 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-5, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,930 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-5 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,930 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-5 broker=0] No checkpointed highwatermark is found for partition test-topic-1-5
2023-07-30 13:46:03,930 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-5 broker=0] Log loaded for partition test-topic-1-5 with initial high watermark 0
2023-07-30 13:46:03,937 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-6, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,938 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-6 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,938 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-6 broker=0] No checkpointed highwatermark is found for partition test-topic-1-6
2023-07-30 13:46:03,938 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-6 broker=0] Log loaded for partition test-topic-1-6 with initial high watermark 0
2023-07-30 13:46:03,945 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-6, groupId=primary-mm2] Discovered group coordinator localhost:43245 (id: 2147483647 rack: null)
2023-07-30 13:46:03,946 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:03,947 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:03,947 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-3, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,948 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-3 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,948 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-3 broker=0] No checkpointed highwatermark is found for partition test-topic-1-3
2023-07-30 13:46:03,948 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-3 broker=0] Log loaded for partition test-topic-1-3 with initial high watermark 0
2023-07-30 13:46:03,948 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@468] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:46:03,949 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:03,950 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 0 (__consumer_offsets-17) (reason: Adding new member connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51 with group instance id None)
2023-07-30 13:46:03,951 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 1 (__consumer_offsets-17)
2023-07-30 13:46:03,952 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=1, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:03,957 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 1
2023-07-30 13:46:03,958 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-4, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,960 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=1, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:03,960 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-4 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,960 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:03,960 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-4 broker=0] No checkpointed highwatermark is found for partition test-topic-1-4
2023-07-30 13:46:03,960 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-4 broker=0] Log loaded for partition test-topic-1-4 with initial high watermark 0
2023-07-30 13:46:03,960 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset -1
2023-07-30 13:46:03,960 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:03,964 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-1, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,966 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-1 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,966 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-1 broker=0] No checkpointed highwatermark is found for partition test-topic-1-1
2023-07-30 13:46:03,966 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-1 broker=0] Log loaded for partition test-topic-1-1 with initial high watermark 0
2023-07-30 13:46:03,973 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-2, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,974 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-5, groupId=primary-mm2] Session key updated
2023-07-30 13:46:03,974 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-4, groupId=primary-mm2] Session key updated
2023-07-30 13:46:03,975 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-2 in /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,975 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-2 broker=0] No checkpointed highwatermark is found for partition test-topic-1-2
2023-07-30 13:46:03,975 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-6, groupId=primary-mm2] Session key updated
2023-07-30 13:46:03,975 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-2 broker=0] Log loaded for partition test-topic-1-2 with initial high watermark 0
2023-07-30 13:46:03,980 - INFO  [kafka-admin-client-thread | adminclient-64:AppInfoParser@83] - App info kafka.admin.client for adminclient-64 unregistered
2023-07-30 13:46:03,981 - INFO  [kafka-admin-client-thread | adminclient-64:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:03,981 - INFO  [kafka-admin-client-thread | adminclient-64:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:03,981 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-5, groupId=primary-mm2] Discovered group coordinator localhost:43245 (id: 2147483647 rack: null)
2023-07-30 13:46:03,981 - INFO  [kafka-admin-client-thread | adminclient-64:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:03,982 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:03,982 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:03,982 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:03,983 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:03,984 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:03,984 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739163983
2023-07-30 13:46:03,984 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@468] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:46:03,984 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:03,986 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 1 (__consumer_offsets-17) (reason: Adding new member connect-5-6540e570-7125-4144-8383-1ffd804835f9 with group instance id None)
2023-07-30 13:46:03,988 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic primary.test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:46:03,992 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-4, groupId=primary-mm2] Discovered group coordinator localhost:43245 (id: 2147483647 rack: null)
2023-07-30 13:46:03,992 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:03,992 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:03,994 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@468] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:46:03,995 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:03,997 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(primary.test-topic-1-0)
2023-07-30 13:46:03,998 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=primary.test-topic-1-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:03,999 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition primary.test-topic-1-0 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:03,999 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition primary.test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-0
2023-07-30 13:46:03,999 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition primary.test-topic-1-0 broker=0] Log loaded for partition primary.test-topic-1-0 with initial high watermark 0
2023-07-30 13:46:04,003 - INFO  [kafka-admin-client-thread | adminclient-65:AppInfoParser@83] - App info kafka.admin.client for adminclient-65 unregistered
2023-07-30 13:46:04,003 - INFO  [kafka-admin-client-thread | adminclient-65:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:04,004 - INFO  [kafka-admin-client-thread | adminclient-65:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:04,004 - INFO  [kafka-admin-client-thread | adminclient-65:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:04,004 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:04,005 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:04,005 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:04,005 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739164005
2023-07-30 13:46:04,009 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Creating topic heartbeats with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:46:04,015 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(heartbeats-0)
2023-07-30 13:46:04,016 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=heartbeats-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,017 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition heartbeats-0 in /tmp/junit5301514646100141969/junit2114663958254445386/heartbeats-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,017 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition heartbeats-0 broker=0] No checkpointed highwatermark is found for partition heartbeats-0
2023-07-30 13:46:04,017 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition heartbeats-0 broker=0] Log loaded for partition heartbeats-0 with initial high watermark 0
2023-07-30 13:46:04,021 - INFO  [kafka-admin-client-thread | adminclient-66:AppInfoParser@83] - App info kafka.admin.client for adminclient-66 unregistered
2023-07-30 13:46:04,022 - INFO  [kafka-admin-client-thread | adminclient-66:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:04,022 - INFO  [kafka-admin-client-thread | adminclient-66:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:04,022 - INFO  [kafka-admin-client-thread | adminclient-66:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:04,069 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-consumer-group-dummy-19
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-dummy
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:04,071 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:04,071 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:04,071 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739164071
2023-07-30 13:46:04,071 - INFO  [main:KafkaConsumer@961] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Subscribed to topic(s): test-topic-1
2023-07-30 13:46:04,075 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:04,075 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Discovered group coordinator localhost:40623 (id: 2147483647 rack: null)
2023-07-30 13:46:04,076 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:46:04,081 - INFO  [main:AbstractCoordinator@468] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:46:04,081 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:46:04,083 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member consumer-consumer-group-dummy-19-509f28a2-1b27-4ac2-b730-ff94d8f3ec15 with group instance id None)
2023-07-30 13:46:04,085 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group consumer-group-dummy generation 1 (__consumer_offsets-35)
2023-07-30 13:46:04,086 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Successfully joined group with generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-19-509f28a2-1b27-4ac2-b730-ff94d8f3ec15', protocol='range'}
2023-07-30 13:46:04,088 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Finished assignment for group at generation 1: {consumer-consumer-group-dummy-19-509f28a2-1b27-4ac2-b730-ff94d8f3ec15=Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])}
2023-07-30 13:46:04,092 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group consumer-group-dummy for generation 1
2023-07-30 13:46:04,094 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@750] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Successfully synced group in generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-19-509f28a2-1b27-4ac2-b730-ff94d8f3ec15', protocol='range'}
2023-07-30 13:46:04,094 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Notifying assignor about the new Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])
2023-07-30 13:46:04,094 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Adding newly assigned partitions: test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:46:04,100 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-3
2023-07-30 13:46:04,100 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-2
2023-07-30 13:46:04,100 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-5
2023-07-30 13:46:04,100 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-4
2023-07-30 13:46:04,100 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-7
2023-07-30 13:46:04,101 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-6
2023-07-30 13:46:04,101 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-9
2023-07-30 13:46:04,101 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-8
2023-07-30 13:46:04,101 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-1
2023-07-30 13:46:04,101 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-0
2023-07-30 13:46:04,103 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,104 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,104 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,104 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,104 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,104 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,105 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,105 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,105 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,105 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,134 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Revoke previously assigned partitions test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:46:04,134 - INFO  [main:AbstractCoordinator@1016] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Member consumer-consumer-group-dummy-19-509f28a2-1b27-4ac2-b730-ff94d8f3ec15 sending LeaveGroup request to coordinator localhost:40623 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:46:04,138 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-consumer-group-dummy-19-509f28a2-1b27-4ac2-b730-ff94d8f3ec15] in group consumer-group-dummy has left, removing it from the group
2023-07-30 13:46:04,138 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member consumer-consumer-group-dummy-19-509f28a2-1b27-4ac2-b730-ff94d8f3ec15 on LeaveGroup)
2023-07-30 13:46:04,138 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Group consumer-group-dummy with generation 2 is now empty (__consumer_offsets-35)
2023-07-30 13:46:04,142 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:04,142 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:04,142 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:04,143 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-consumer-group-dummy-19 unregistered
2023-07-30 13:46:04,143 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-consumer-group-dummy-20
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-dummy
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:04,145 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:04,145 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:04,145 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739164145
2023-07-30 13:46:04,145 - INFO  [main:KafkaConsumer@961] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Subscribed to topic(s): test-topic-1
2023-07-30 13:46:04,147 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:04,148 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Discovered group coordinator localhost:43245 (id: 2147483647 rack: null)
2023-07-30 13:46:04,148 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:46:04,151 - INFO  [main:AbstractCoordinator@468] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:46:04,151 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:46:04,152 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member consumer-consumer-group-dummy-20-ad99f899-d5fe-446e-96e4-ede2b11fa215 with group instance id None)
2023-07-30 13:46:04,154 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group consumer-group-dummy generation 1 (__consumer_offsets-35)
2023-07-30 13:46:04,155 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Successfully joined group with generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-20-ad99f899-d5fe-446e-96e4-ede2b11fa215', protocol='range'}
2023-07-30 13:46:04,155 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Finished assignment for group at generation 1: {consumer-consumer-group-dummy-20-ad99f899-d5fe-446e-96e4-ede2b11fa215=Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])}
2023-07-30 13:46:04,160 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group consumer-group-dummy for generation 1
2023-07-30 13:46:04,163 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@750] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Successfully synced group in generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-20-ad99f899-d5fe-446e-96e4-ede2b11fa215', protocol='range'}
2023-07-30 13:46:04,163 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Notifying assignor about the new Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])
2023-07-30 13:46:04,163 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Adding newly assigned partitions: test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:46:04,164 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-3
2023-07-30 13:46:04,164 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-2
2023-07-30 13:46:04,164 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-5
2023-07-30 13:46:04,165 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-4
2023-07-30 13:46:04,165 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-7
2023-07-30 13:46:04,165 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-6
2023-07-30 13:46:04,165 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-9
2023-07-30 13:46:04,165 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-8
2023-07-30 13:46:04,165 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-1
2023-07-30 13:46:04,165 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-0
2023-07-30 13:46:04,167 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,168 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,168 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,168 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,168 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,168 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,169 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,169 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,169 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,169 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,179 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Revoke previously assigned partitions test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:46:04,180 - INFO  [main:AbstractCoordinator@1016] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Member consumer-consumer-group-dummy-20-ad99f899-d5fe-446e-96e4-ede2b11fa215 sending LeaveGroup request to coordinator localhost:43245 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:46:04,180 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-consumer-group-dummy-20-ad99f899-d5fe-446e-96e4-ede2b11fa215] in group consumer-group-dummy has left, removing it from the group
2023-07-30 13:46:04,181 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member consumer-consumer-group-dummy-20-ad99f899-d5fe-446e-96e4-ede2b11fa215 on LeaveGroup)
2023-07-30 13:46:04,181 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Group consumer-group-dummy with generation 2 is now empty (__consumer_offsets-35)
2023-07-30 13:46:04,182 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:04,182 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:04,182 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:04,183 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-consumer-group-dummy-20 unregistered
2023-07-30 13:46:04,185 - INFO  [main:MirrorConnectorsIntegrationTest@158] - primary REST service: http://localhost:40703/connectors
2023-07-30 13:46:04,185 - INFO  [main:MirrorConnectorsIntegrationTest@159] - backup REST service: http://localhost:37121/connectors
2023-07-30 13:46:04,185 - INFO  [main:MirrorConnectorsIntegrationTest@161] - primary brokers: localhost:40623
2023-07-30 13:46:04,185 - INFO  [main:MirrorConnectorsIntegrationTest@162] - backup brokers: localhost:43245
2023-07-30 13:46:04,186 - INFO  [main:AbstractConfig@361] - MirrorMakerConfig values: 
	clusters = [primary, backup]
	config.providers = []
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:04,411 - INFO  [main:Reflections@239] - Reflections took 224 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:46:04,413 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:46:04,415 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:46:04,417 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:46:04,420 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:46:04,420 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:04,420 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:04,421 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:04,421 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:04,421 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:04,421 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:46:04,421 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:46:04,422 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:46:04,422 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:46:04,422 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:46:04,423 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:04,424 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:04,424 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:04,424 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739164424
2023-07-30 13:46:04,428 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic test-topic-with-empty-partition with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:46:04,438 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(test-topic-with-empty-partition-7, test-topic-with-empty-partition-1, test-topic-with-empty-partition-6, test-topic-with-empty-partition-3, test-topic-with-empty-partition-5, test-topic-with-empty-partition-2, test-topic-with-empty-partition-0, test-topic-with-empty-partition-9, test-topic-with-empty-partition-4, test-topic-with-empty-partition-8)
2023-07-30 13:46:04,440 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-0, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,440 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-0 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,441 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-0 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-0
2023-07-30 13:46:04,441 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-0 broker=0] Log loaded for partition test-topic-with-empty-partition-0 with initial high watermark 0
2023-07-30 13:46:04,443 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-3, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,443 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-3 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,444 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-3 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-3
2023-07-30 13:46:04,444 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-3 broker=0] Log loaded for partition test-topic-with-empty-partition-3 with initial high watermark 0
2023-07-30 13:46:04,451 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-4, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,452 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-4 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,452 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-4 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-4
2023-07-30 13:46:04,452 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-4 broker=0] Log loaded for partition test-topic-with-empty-partition-4 with initial high watermark 0
2023-07-30 13:46:04,459 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-1, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,460 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-1 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,460 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-1 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-1
2023-07-30 13:46:04,460 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-1 broker=0] Log loaded for partition test-topic-with-empty-partition-1 with initial high watermark 0
2023-07-30 13:46:04,468 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-2, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,468 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-2 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,468 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-2 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-2
2023-07-30 13:46:04,468 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-2 broker=0] Log loaded for partition test-topic-with-empty-partition-2 with initial high watermark 0
2023-07-30 13:46:04,477 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-7, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,478 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-7 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,478 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-7 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-7
2023-07-30 13:46:04,478 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-7 broker=0] Log loaded for partition test-topic-with-empty-partition-7 with initial high watermark 0
2023-07-30 13:46:04,485 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-8, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,486 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-8 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,486 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-8 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-8
2023-07-30 13:46:04,486 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-8 broker=0] Log loaded for partition test-topic-with-empty-partition-8 with initial high watermark 0
2023-07-30 13:46:04,493 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-5, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,493 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-5 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,493 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-5 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-5
2023-07-30 13:46:04,493 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-5 broker=0] Log loaded for partition test-topic-with-empty-partition-5 with initial high watermark 0
2023-07-30 13:46:04,501 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-6, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,501 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-6 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,501 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-6 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-6
2023-07-30 13:46:04,501 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-6 broker=0] Log loaded for partition test-topic-with-empty-partition-6 with initial high watermark 0
2023-07-30 13:46:04,509 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=test-topic-with-empty-partition-9, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:04,510 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition test-topic-with-empty-partition-9 in /tmp/junit6154326205091447453/junit85516104450203242/test-topic-with-empty-partition-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:04,510 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-9 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-9
2023-07-30 13:46:04,510 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition test-topic-with-empty-partition-9 broker=0] Log loaded for partition test-topic-with-empty-partition-9 with initial high watermark 0
2023-07-30 13:46:04,519 - INFO  [kafka-admin-client-thread | adminclient-67:AppInfoParser@83] - App info kafka.admin.client for adminclient-67 unregistered
2023-07-30 13:46:04,520 - INFO  [kafka-admin-client-thread | adminclient-67:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:04,520 - INFO  [kafka-admin-client-thread | adminclient-67:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:04,520 - INFO  [kafka-admin-client-thread | adminclient-67:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:04,538 - INFO  [main:MirrorConnectorsIntegrationTest@359] - Exception while producing
2023-07-30 13:46:04,538 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-consumer-group-testReplicationWithEmptyPartition-21
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-testReplicationWithEmptyPartition
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:04,540 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:04,540 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:04,540 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739164540
2023-07-30 13:46:04,540 - INFO  [main:KafkaConsumer@961] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Subscribed to topic(s): test-topic-with-empty-partition
2023-07-30 13:46:04,542 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:04,542 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Discovered group coordinator localhost:40623 (id: 2147483647 rack: null)
2023-07-30 13:46:04,542 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] (Re-)joining group
2023-07-30 13:46:04,544 - INFO  [main:AbstractCoordinator@468] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:46:04,545 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] (Re-)joining group
2023-07-30 13:46:04,546 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-testReplicationWithEmptyPartition in state PreparingRebalance with old generation 0 (__consumer_offsets-34) (reason: Adding new member consumer-consumer-group-testReplicationWithEmptyPartition-21-c654bbe6-7afb-4897-9c3d-0c877ffccaf8 with group instance id None)
2023-07-30 13:46:04,547 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group consumer-group-testReplicationWithEmptyPartition generation 1 (__consumer_offsets-34)
2023-07-30 13:46:04,548 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Successfully joined group with generation Generation{generationId=1, memberId='consumer-consumer-group-testReplicationWithEmptyPartition-21-c654bbe6-7afb-4897-9c3d-0c877ffccaf8', protocol='range'}
2023-07-30 13:46:04,548 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Finished assignment for group at generation 1: {consumer-consumer-group-testReplicationWithEmptyPartition-21-c654bbe6-7afb-4897-9c3d-0c877ffccaf8=Assignment(partitions=[test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9])}
2023-07-30 13:46:04,551 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group consumer-group-testReplicationWithEmptyPartition for generation 1
2023-07-30 13:46:04,553 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@750] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Successfully synced group in generation Generation{generationId=1, memberId='consumer-consumer-group-testReplicationWithEmptyPartition-21-c654bbe6-7afb-4897-9c3d-0c877ffccaf8', protocol='range'}
2023-07-30 13:46:04,553 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Notifying assignor about the new Assignment(partitions=[test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9])
2023-07-30 13:46:04,554 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Adding newly assigned partitions: test-topic-with-empty-partition-3, test-topic-with-empty-partition-2, test-topic-with-empty-partition-5, test-topic-with-empty-partition-4, test-topic-with-empty-partition-1, test-topic-with-empty-partition-0, test-topic-with-empty-partition-7, test-topic-with-empty-partition-6, test-topic-with-empty-partition-9, test-topic-with-empty-partition-8
2023-07-30 13:46:04,554 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-3
2023-07-30 13:46:04,555 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-2
2023-07-30 13:46:04,555 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-5
2023-07-30 13:46:04,555 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-4
2023-07-30 13:46:04,555 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-1
2023-07-30 13:46:04,555 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-0
2023-07-30 13:46:04,555 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-7
2023-07-30 13:46:04,555 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-6
2023-07-30 13:46:04,555 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-9
2023-07-30 13:46:04,555 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-8
2023-07-30 13:46:04,557 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,558 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,558 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,558 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,558 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,558 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,558 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,558 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,559 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,559 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:04,568 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Revoke previously assigned partitions test-topic-with-empty-partition-3, test-topic-with-empty-partition-2, test-topic-with-empty-partition-5, test-topic-with-empty-partition-4, test-topic-with-empty-partition-1, test-topic-with-empty-partition-0, test-topic-with-empty-partition-7, test-topic-with-empty-partition-6, test-topic-with-empty-partition-9, test-topic-with-empty-partition-8
2023-07-30 13:46:04,568 - INFO  [main:AbstractCoordinator@1016] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Member consumer-consumer-group-testReplicationWithEmptyPartition-21-c654bbe6-7afb-4897-9c3d-0c877ffccaf8 sending LeaveGroup request to coordinator localhost:40623 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:46:04,569 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-consumer-group-testReplicationWithEmptyPartition-21-c654bbe6-7afb-4897-9c3d-0c877ffccaf8] in group consumer-group-testReplicationWithEmptyPartition has left, removing it from the group
2023-07-30 13:46:04,569 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-testReplicationWithEmptyPartition in state PreparingRebalance with old generation 1 (__consumer_offsets-34) (reason: removing member consumer-consumer-group-testReplicationWithEmptyPartition-21-c654bbe6-7afb-4897-9c3d-0c877ffccaf8 on LeaveGroup)
2023-07-30 13:46:04,569 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Group consumer-group-testReplicationWithEmptyPartition with generation 2 is now empty (__consumer_offsets-34)
2023-07-30 13:46:04,570 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:04,570 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:04,570 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:04,571 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-consumer-group-testReplicationWithEmptyPartition-21 unregistered
2023-07-30 13:46:04,990 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-2, groupId=backup-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:46:04,990 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-1, groupId=backup-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:46:04,990 - INFO  [DistributedHerder-connect-2-1:WorkerCoordinator@225] - [Worker clientId=connect-2, groupId=backup-mm2] Rebalance started
2023-07-30 13:46:04,990 - INFO  [DistributedHerder-connect-1-1:WorkerCoordinator@225] - [Worker clientId=connect-1, groupId=backup-mm2] Rebalance started
2023-07-30 13:46:04,990 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@534] - [Worker clientId=connect-1, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:46:04,990 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@534] - [Worker clientId=connect-2, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:46:04,992 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Stabilized group backup-mm2 generation 2 (__consumer_offsets-5)
2023-07-30 13:46:04,993 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3', protocol='sessioned'}
2023-07-30 13:46:04,993 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-3, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-3-617ca672-0829-41dc-b8e2-9d66aba06a1c', protocol='sessioned'}
2023-07-30 13:46:04,993 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-1-a8a55554-5291-434c-9af9-322de7d60f07', protocol='sessioned'}
2023-07-30 13:46:04,996 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group backup-mm2 for generation 2
2023-07-30 13:46:04,997 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3', protocol='sessioned'}
2023-07-30 13:46:04,998 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-3, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-3-617ca672-0829-41dc-b8e2-9d66aba06a1c', protocol='sessioned'}
2023-07-30 13:46:04,998 - INFO  [DistributedHerder-connect-2-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-2, groupId=backup-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3', leaderUrl='http://localhost:38925/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:04,998 - INFO  [DistributedHerder-connect-3-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-3, groupId=backup-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3', leaderUrl='http://localhost:38925/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:04,998 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-1-a8a55554-5291-434c-9af9-322de7d60f07', protocol='sessioned'}
2023-07-30 13:46:04,998 - WARN  [DistributedHerder-connect-3-1:DistributedHerder@1094] - [Worker clientId=connect-3, groupId=backup-mm2] Catching up to assignment's config offset.
2023-07-30 13:46:04,998 - WARN  [DistributedHerder-connect-2-1:DistributedHerder@1094] - [Worker clientId=connect-2, groupId=backup-mm2] Catching up to assignment's config offset.
2023-07-30 13:46:04,999 - INFO  [DistributedHerder-connect-1-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-1, groupId=backup-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3', leaderUrl='http://localhost:38925/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:04,999 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1157] - [Worker clientId=connect-3, groupId=backup-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:46:04,999 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1157] - [Worker clientId=connect-2, groupId=backup-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:46:04,999 - WARN  [DistributedHerder-connect-1-1:DistributedHerder@1094] - [Worker clientId=connect-1, groupId=backup-mm2] Catching up to assignment's config offset.
2023-07-30 13:46:04,999 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1157] - [Worker clientId=connect-1, groupId=backup-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:46:05,116 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1161] - [Worker clientId=connect-2, groupId=backup-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:46:05,116 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1215] - [Worker clientId=connect-2, groupId=backup-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:46:05,116 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1243] - [Worker clientId=connect-2, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:46:05,117 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1161] - [Worker clientId=connect-1, groupId=backup-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:46:05,117 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1215] - [Worker clientId=connect-1, groupId=backup-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:46:05,117 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1243] - [Worker clientId=connect-1, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:46:05,265 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1161] - [Worker clientId=connect-3, groupId=backup-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:46:05,266 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1215] - [Worker clientId=connect-3, groupId=backup-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:46:05,266 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1243] - [Worker clientId=connect-3, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:46:06,953 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-6, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:46:06,954 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:06,954 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:06,955 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 2 (__consumer_offsets-17)
2023-07-30 13:46:06,956 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:06,956 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:06,956 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:06,961 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 2
2023-07-30 13:46:06,962 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:06,963 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:06,963 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:06,964 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:06,964 - WARN  [DistributedHerder-connect-6-1:DistributedHerder@1094] - [Worker clientId=connect-6, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:46:06,964 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:06,965 - WARN  [DistributedHerder-connect-4-1:DistributedHerder@1094] - [Worker clientId=connect-4, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:46:06,965 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1157] - [Worker clientId=connect-6, groupId=primary-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:46:06,965 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1157] - [Worker clientId=connect-4, groupId=primary-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:46:06,965 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:06,967 - WARN  [DistributedHerder-connect-5-1:DistributedHerder@1094] - [Worker clientId=connect-5, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:46:06,967 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1157] - [Worker clientId=connect-5, groupId=primary-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:46:06,981 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1161] - [Worker clientId=connect-4, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:46:06,982 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:46:06,982 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:06,982 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1161] - [Worker clientId=connect-5, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:46:06,982 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:46:06,982 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1161] - [Worker clientId=connect-6, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:46:06,982 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:06,983 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:46:06,983 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:06,990 - INFO  [pool-25-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:46:07,087 - INFO  [pool-31-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:46:07,091 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorSourceConnector config updated
2023-07-30 13:46:07,091 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorSourceConnector config updated
2023-07-30 13:46:07,091 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorSourceConnector config updated
2023-07-30 13:46:07,091 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,091 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,091 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,091 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,092 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,092 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,093 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 2 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-d9814915-0e67-45fb-aae4-be935085a7e3 during Stable)
2023-07-30 13:46:07,094 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 3 (__consumer_offsets-17)
2023-07-30 13:46:07,094 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=3, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:07,094 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=3, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:07,094 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=3, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:07,097 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 3
2023-07-30 13:46:07,099 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=3, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:07,100 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=3, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:07,100 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=2, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,100 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=3, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:07,100 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 2
2023-07-30 13:46:07,100 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=2, connectorIds=[MirrorSourceConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,100 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,100 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=2, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,101 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 2
2023-07-30 13:46:07,101 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 2
2023-07-30 13:46:07,101 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,102 - INFO  [StartAndStopExecutor-connect-4-1:DistributedHerder@1298] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connector MirrorSourceConnector
2023-07-30 13:46:07,104 - INFO  [StartAndStopExecutor-connect-4-1:Worker@274] - Creating connector MirrorSourceConnector of type org.apache.kafka.connect.mirror.MirrorSourceConnector
2023-07-30 13:46:07,105 - INFO  [StartAndStopExecutor-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,105 - INFO  [StartAndStopExecutor-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,111 - INFO  [StartAndStopExecutor-connect-4-1:Worker@284] - Instantiated connector MirrorSourceConnector with version 1 of type class org.apache.kafka.connect.mirror.MirrorSourceConnector
2023-07-30 13:46:07,112 - INFO  [StartAndStopExecutor-connect-4-1:Worker@310] - Finished creating connector MirrorSourceConnector
2023-07-30 13:46:07,112 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,114 - INFO  [connector-thread-MirrorSourceConnector:AbstractConfig@361] - MirrorConnectorConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorSourceConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:46:07,116 - INFO  [connector-thread-MirrorSourceConnector:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:07,116 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:07,117 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:07,117 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739167116
2023-07-30 13:46:07,117 - INFO  [connector-thread-MirrorSourceConnector:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:07,118 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:07,118 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:07,118 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739167118
2023-07-30 13:46:07,120 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:07,121 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:07,121 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:07,121 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739167121
2023-07-30 13:46:07,125 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic mm2-offset-syncs.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:46:07,135 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offset-syncs.backup.internal-0)
2023-07-30 13:46:07,136 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-offset-syncs.backup.internal-0, dir=/tmp/junit6154326205091447453/junit85516104450203242] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,137 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-offset-syncs.backup.internal-0 in /tmp/junit6154326205091447453/junit85516104450203242/mm2-offset-syncs.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,137 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-offset-syncs.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-offset-syncs.backup.internal-0
2023-07-30 13:46:07,138 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-offset-syncs.backup.internal-0 broker=0] Log loaded for partition mm2-offset-syncs.backup.internal-0 with initial high watermark 0
2023-07-30 13:46:07,140 - INFO  [main:EmbeddedConnectCluster@689] - PUT response for URL=http://localhost:37121/connectors/MirrorSourceConnector/config is {"name":"MirrorSourceConnector","config":{"connector.class":"org.apache.kafka.connect.mirror.MirrorSourceConnector","offset-syncs.topic.replication.factor":"1","replication.factor":"1","source.cluster.producer.bootstrap.servers":"localhost:40623","sync.topic.acls.enabled":"false","topics":"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*","emit.checkpoints.interval.seconds":"1","heartbeats.topic.replication.factor":"1","source.cluster.alias":"primary","groups":"consumer-group-.*","source.cluster.bootstrap.servers":"localhost:40623","target.cluster.producer.bootstrap.servers":"localhost:43245","enabled":"true","target.cluster.admin.bootstrap.servers":"localhost:43245","target.cluster.alias":"backup","target.cluster.consumer.bootstrap.servers":"localhost:43245","name":"MirrorSourceConnector","target.cluster.bootstrap.servers":"localhost:43245","emit.heartbeats.interval.seconds":"1","checkpoints.topic.replication.factor":"1","source.cluster.admin.bootstrap.servers":"localhost:40623","refresh.groups.interval.seconds":"1","refresh.topics.interval.seconds":"1","source.cluster.consumer.bootstrap.servers":"localhost:40623"},"tasks":[],"type":"source"}
2023-07-30 13:46:07,141 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:TopicAdmin@284] - Created topic (name=mm2-offset-syncs.backup.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:40623
2023-07-30 13:46:07,141 - INFO  [kafka-admin-client-thread | adminclient-70:AppInfoParser@83] - App info kafka.admin.client for adminclient-70 unregistered
2023-07-30 13:46:07,142 - INFO  [kafka-admin-client-thread | adminclient-70:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:07,142 - INFO  [kafka-admin-client-thread | adminclient-70:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:07,142 - INFO  [kafka-admin-client-thread | adminclient-70:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:07,142 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:Scheduler@95] - creating upstream offset-syncs topic took 23 ms
2023-07-30 13:46:07,149 - INFO  [pool-25-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:46:07,160 - INFO  [pool-31-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:46:07,160 - INFO  [Scheduler for MirrorSourceConnector-loading initial set of topic-partitions:Scheduler@95] - loading initial set of topic-partitions took 15 ms
2023-07-30 13:46:07,163 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorCheckpointConnector config updated
2023-07-30 13:46:07,163 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorCheckpointConnector config updated
2023-07-30 13:46:07,164 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,164 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,164 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,164 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorCheckpointConnector config updated
2023-07-30 13:46:07,164 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,164 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,164 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,165 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 3 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-d9814915-0e67-45fb-aae4-be935085a7e3 during Stable)
2023-07-30 13:46:07,168 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 4 (__consumer_offsets-17)
2023-07-30 13:46:07,168 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic primary.test-topic-with-empty-partition with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:46:07,168 - INFO  [Scheduler for MirrorSourceConnector-creating downstream topic-partitions:Scheduler@95] - creating downstream topic-partitions took 7 ms
2023-07-30 13:46:07,171 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=4, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:07,172 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=4, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:07,172 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=4, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:07,176 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 4
2023-07-30 13:46:07,177 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=4, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:07,178 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=4, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:07,179 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=3, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,178 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=4, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:07,179 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 3
2023-07-30 13:46:07,179 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=3, connectorIds=[MirrorCheckpointConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,179 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=3, connectorIds=[MirrorSourceConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,179 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 3
2023-07-30 13:46:07,179 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,179 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 3
2023-07-30 13:46:07,179 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic primary.heartbeats with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:46:07,179 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,179 - INFO  [main:EmbeddedConnectCluster@689] - PUT response for URL=http://localhost:37121/connectors/MirrorCheckpointConnector/config is {"name":"MirrorCheckpointConnector","config":{"connector.class":"org.apache.kafka.connect.mirror.MirrorCheckpointConnector","offset-syncs.topic.replication.factor":"1","replication.factor":"1","source.cluster.producer.bootstrap.servers":"localhost:40623","sync.topic.acls.enabled":"false","topics":"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*","emit.checkpoints.interval.seconds":"1","heartbeats.topic.replication.factor":"1","source.cluster.alias":"primary","groups":"consumer-group-.*","source.cluster.bootstrap.servers":"localhost:40623","target.cluster.producer.bootstrap.servers":"localhost:43245","enabled":"true","target.cluster.admin.bootstrap.servers":"localhost:43245","target.cluster.alias":"backup","target.cluster.consumer.bootstrap.servers":"localhost:43245","name":"MirrorCheckpointConnector","target.cluster.bootstrap.servers":"localhost:43245","emit.heartbeats.interval.seconds":"1","checkpoints.topic.replication.factor":"1","source.cluster.admin.bootstrap.servers":"localhost:40623","refresh.groups.interval.seconds":"1","refresh.topics.interval.seconds":"1","source.cluster.consumer.bootstrap.servers":"localhost:40623"},"tasks":[],"type":"source"}
2023-07-30 13:46:07,179 - INFO  [StartAndStopExecutor-connect-5-1:DistributedHerder@1298] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connector MirrorCheckpointConnector
2023-07-30 13:46:07,180 - INFO  [StartAndStopExecutor-connect-5-1:Worker@274] - Creating connector MirrorCheckpointConnector of type org.apache.kafka.connect.mirror.MirrorCheckpointConnector
2023-07-30 13:46:07,181 - INFO  [StartAndStopExecutor-connect-5-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,181 - INFO  [StartAndStopExecutor-connect-5-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,182 - INFO  [StartAndStopExecutor-connect-5-1:Worker@284] - Instantiated connector MirrorCheckpointConnector with version 1 of type class org.apache.kafka.connect.mirror.MirrorCheckpointConnector
2023-07-30 13:46:07,182 - INFO  [StartAndStopExecutor-connect-5-1:Worker@310] - Finished creating connector MirrorCheckpointConnector
2023-07-30 13:46:07,182 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,183 - INFO  [connector-thread-MirrorCheckpointConnector:AbstractConfig@361] - MirrorConnectorConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorCheckpointConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:46:07,184 - INFO  [connector-thread-MirrorCheckpointConnector:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:07,186 - INFO  [connector-thread-MirrorCheckpointConnector:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:07,186 - INFO  [connector-thread-MirrorCheckpointConnector:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:07,186 - INFO  [connector-thread-MirrorCheckpointConnector:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739167186
2023-07-30 13:46:07,186 - INFO  [pool-25-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:46:07,188 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:07,189 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:07,189 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:07,189 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739167189
2023-07-30 13:46:07,192 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(primary.test-topic-with-empty-partition-3, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-7, primary.test-topic-with-empty-partition-6, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-2, primary.test-topic-with-empty-partition-0, primary.test-topic-with-empty-partition-4, primary.test-topic-with-empty-partition-5, primary.test-topic-with-empty-partition-1)
2023-07-30 13:46:07,197 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-9, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,198 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-9 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,198 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-9 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-9
2023-07-30 13:46:07,198 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-9 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-9 with initial high watermark 0
2023-07-30 13:46:07,199 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Creating topic primary.checkpoints.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:46:07,201 - INFO  [pool-31-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:46:07,202 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,202 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-0 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,202 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-0 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-0
2023-07-30 13:46:07,202 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-0 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-0 with initial high watermark 0
2023-07-30 13:46:07,205 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorHeartbeatConnector config updated
2023-07-30 13:46:07,205 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,205 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorHeartbeatConnector config updated
2023-07-30 13:46:07,205 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,206 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,206 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,206 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorHeartbeatConnector config updated
2023-07-30 13:46:07,206 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,206 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,206 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 4 (__consumer_offsets-17) (reason: Updating metadata for member connect-5-6540e570-7125-4144-8383-1ffd804835f9 during Stable)
2023-07-30 13:46:07,208 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 5 (__consumer_offsets-17)
2023-07-30 13:46:07,208 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=5, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:07,208 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=5, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:07,209 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=5, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:07,211 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 5
2023-07-30 13:46:07,212 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-3, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,213 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-3 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,213 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-3 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-3
2023-07-30 13:46:07,213 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-3 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-3 with initial high watermark 0
2023-07-30 13:46:07,213 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=5, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:07,213 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=5, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:07,213 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=4, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,213 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=5, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:07,214 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 4
2023-07-30 13:46:07,214 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=4, connectorIds=[MirrorCheckpointConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,213 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=4, connectorIds=[MirrorSourceConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,214 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 4
2023-07-30 13:46:07,214 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 4
2023-07-30 13:46:07,214 - INFO  [StartAndStopExecutor-connect-6-1:DistributedHerder@1298] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connector MirrorHeartbeatConnector
2023-07-30 13:46:07,214 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,214 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,214 - INFO  [StartAndStopExecutor-connect-6-1:Worker@274] - Creating connector MirrorHeartbeatConnector of type org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
2023-07-30 13:46:07,215 - INFO  [StartAndStopExecutor-connect-6-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,215 - INFO  [StartAndStopExecutor-connect-6-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,215 - INFO  [StartAndStopExecutor-connect-6-1:Worker@284] - Instantiated connector MirrorHeartbeatConnector with version 1 of type class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
2023-07-30 13:46:07,215 - INFO  [main:EmbeddedConnectCluster@689] - PUT response for URL=http://localhost:37121/connectors/MirrorHeartbeatConnector/config is {"name":"MirrorHeartbeatConnector","config":{"connector.class":"org.apache.kafka.connect.mirror.MirrorHeartbeatConnector","offset-syncs.topic.replication.factor":"1","replication.factor":"1","source.cluster.producer.bootstrap.servers":"localhost:40623","sync.topic.acls.enabled":"false","topics":"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*","emit.checkpoints.interval.seconds":"1","heartbeats.topic.replication.factor":"1","source.cluster.alias":"primary","groups":"consumer-group-.*","source.cluster.bootstrap.servers":"localhost:40623","target.cluster.producer.bootstrap.servers":"localhost:43245","enabled":"true","target.cluster.admin.bootstrap.servers":"localhost:43245","target.cluster.alias":"backup","target.cluster.consumer.bootstrap.servers":"localhost:43245","name":"MirrorHeartbeatConnector","target.cluster.bootstrap.servers":"localhost:43245","emit.heartbeats.interval.seconds":"1","checkpoints.topic.replication.factor":"1","source.cluster.admin.bootstrap.servers":"localhost:40623","refresh.groups.interval.seconds":"1","refresh.topics.interval.seconds":"1","source.cluster.consumer.bootstrap.servers":"localhost:40623"},"tasks":[],"type":"source"}
2023-07-30 13:46:07,216 - INFO  [StartAndStopExecutor-connect-6-1:Worker@310] - Finished creating connector MirrorHeartbeatConnector
2023-07-30 13:46:07,216 - INFO  [connector-thread-MirrorHeartbeatConnector:AbstractConfig@361] - MirrorConnectorConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorHeartbeatConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:46:07,216 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,217 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:07,218 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-4, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,218 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:07,218 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:07,219 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739167218
2023-07-30 13:46:07,219 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-4 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,219 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-4 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-4
2023-07-30 13:46:07,219 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-4 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-4 with initial high watermark 0
2023-07-30 13:46:07,222 - INFO  [kafka-admin-client-thread | adminclient-73:AppInfoParser@83] - App info kafka.admin.client for adminclient-73 unregistered
2023-07-30 13:46:07,222 - INFO  [kafka-admin-client-thread | adminclient-73:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:07,223 - INFO  [kafka-admin-client-thread | adminclient-73:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:07,223 - INFO  [kafka-admin-client-thread | adminclient-73:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:07,223 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:Scheduler@95] - creating internal topics took 6 ms
2023-07-30 13:46:07,226 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-1, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,227 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-1 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,227 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-1 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-1
2023-07-30 13:46:07,227 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-1 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-1 with initial high watermark 0
2023-07-30 13:46:07,231 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,231 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,236 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-2, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,237 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-2 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,237 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-2 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-2
2023-07-30 13:46:07,237 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-2 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-2 with initial high watermark 0
2023-07-30 13:46:07,240 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
2023-07-30 13:46:07,240 - ERROR [main:EmbeddedConnectClusterAssertions@420] - Could not check connector state info.
org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not read connector state. Error response: {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.connectorStatus(EmbeddedConnectCluster.java:459)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.checkConnectorState(EmbeddedConnectClusterAssertions.java:413)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.lambda$assertConnectorAndAtLeastNumTasksAreRunning$16(EmbeddedConnectClusterAssertions.java:286)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:400)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:448)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:416)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:397)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:387)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.assertConnectorAndAtLeastNumTasksAreRunning(EmbeddedConnectClusterAssertions.java:285)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.waitUntilMirrorMakerIsRunning(MirrorConnectorsIntegrationTest.java:191)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplicationWithEmptyPartition(MirrorConnectorsIntegrationTest.java:367)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2023-07-30 13:46:07,243 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-7, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,243 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-7 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,243 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-7 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-7
2023-07-30 13:46:07,243 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-7 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-7 with initial high watermark 0
2023-07-30 13:46:07,251 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-8, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,251 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-8 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,251 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-8 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-8
2023-07-30 13:46:07,251 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-8 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-8 with initial high watermark 0
2023-07-30 13:46:07,259 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-5, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,260 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-5 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,260 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-5 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-5
2023-07-30 13:46:07,260 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-5 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-5 with initial high watermark 0
2023-07-30 13:46:07,267 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-6, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,268 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-6 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,268 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-6 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-6
2023-07-30 13:46:07,268 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-with-empty-partition-6 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-6 with initial high watermark 0
2023-07-30 13:46:07,277 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(primary.heartbeats-0)
2023-07-30 13:46:07,278 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=primary.heartbeats-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,279 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition primary.heartbeats-0 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.heartbeats-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,279 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition primary.heartbeats-0 broker=0] No checkpointed highwatermark is found for partition primary.heartbeats-0
2023-07-30 13:46:07,279 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition primary.heartbeats-0 broker=0] Log loaded for partition primary.heartbeats-0 with initial high watermark 0
2023-07-30 13:46:07,285 - INFO  [kafka-admin-client-thread | adminclient-69:MirrorSourceConnector@308] - Created remote topic primary.test-topic-with-empty-partition with 10 partitions.
2023-07-30 13:46:07,285 - INFO  [kafka-admin-client-thread | adminclient-69:MirrorSourceConnector@308] - Created remote topic primary.heartbeats with 1 partitions.
2023-07-30 13:46:07,286 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(primary.checkpoints.internal-0)
2023-07-30 13:46:07,288 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.checkpoints.internal-0, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,288 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.checkpoints.internal-0 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.checkpoints.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,288 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.checkpoints.internal-0 broker=0] No checkpointed highwatermark is found for partition primary.checkpoints.internal-0
2023-07-30 13:46:07,289 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.checkpoints.internal-0 broker=0] Log loaded for partition primary.checkpoints.internal-0 with initial high watermark 0
2023-07-30 13:46:07,293 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Creating 9 partitions for 'primary.test-topic-1' with the following replica assignment: HashMap(1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=)).
2023-07-30 13:46:07,293 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:TopicAdmin@284] - Created topic (name=primary.checkpoints.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:43245
2023-07-30 13:46:07,294 - INFO  [kafka-admin-client-thread | adminclient-72:AppInfoParser@83] - App info kafka.admin.client for adminclient-72 unregistered
2023-07-30 13:46:07,294 - INFO  [kafka-admin-client-thread | adminclient-72:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:07,294 - INFO  [kafka-admin-client-thread | adminclient-72:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:07,294 - INFO  [kafka-admin-client-thread | adminclient-72:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:07,294 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:Scheduler@95] - creating internal topics took 107 ms
2023-07-30 13:46:07,302 - INFO  [Scheduler for MirrorCheckpointConnector-loading initial consumer groups:Scheduler@95] - loading initial consumer groups took 7 ms
2023-07-30 13:46:07,303 - INFO  [connector-thread-MirrorCheckpointConnector:MirrorCheckpointConnector@79] - Started MirrorCheckpointConnector with 2 consumer groups.
2023-07-30 13:46:07,304 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(primary.test-topic-1-1, primary.test-topic-1-8, primary.test-topic-1-4, primary.test-topic-1-7, primary.test-topic-1-3, primary.test-topic-1-2, primary.test-topic-1-6, primary.test-topic-1-5, primary.test-topic-1-9)
2023-07-30 13:46:07,307 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,307 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,307 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-4, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,308 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-4 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,309 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-4 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-4
2023-07-30 13:46:07,309 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-4 broker=0] Log loaded for partition primary.test-topic-1-4 with initial high watermark 0
2023-07-30 13:46:07,312 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-3, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,313 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-3 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,313 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-3 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-3
2023-07-30 13:46:07,313 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-3 broker=0] Log loaded for partition primary.test-topic-1-3 with initial high watermark 0
2023-07-30 13:46:07,326 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-2, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,327 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-2 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,327 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-2 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-2
2023-07-30 13:46:07,327 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-2 broker=0] Log loaded for partition primary.test-topic-1-2 with initial high watermark 0
2023-07-30 13:46:07,329 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-1, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,330 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-1 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,330 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-1 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-1
2023-07-30 13:46:07,330 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-1 broker=0] Log loaded for partition primary.test-topic-1-1 with initial high watermark 0
2023-07-30 13:46:07,338 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-9, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,338 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-9 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,339 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-9 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-9
2023-07-30 13:46:07,339 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-9 broker=0] Log loaded for partition primary.test-topic-1-9 with initial high watermark 0
2023-07-30 13:46:07,344 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
2023-07-30 13:46:07,345 - ERROR [main:EmbeddedConnectClusterAssertions@420] - Could not check connector state info.
org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not read connector state. Error response: {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.connectorStatus(EmbeddedConnectCluster.java:459)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.checkConnectorState(EmbeddedConnectClusterAssertions.java:413)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.lambda$assertConnectorAndAtLeastNumTasksAreRunning$16(EmbeddedConnectClusterAssertions.java:286)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:400)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:448)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:416)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:397)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:387)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.assertConnectorAndAtLeastNumTasksAreRunning(EmbeddedConnectClusterAssertions.java:285)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.waitUntilMirrorMakerIsRunning(MirrorConnectorsIntegrationTest.java:191)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplicationWithEmptyPartition(MirrorConnectorsIntegrationTest.java:367)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2023-07-30 13:46:07,346 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-8, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,347 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-8 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,347 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-8 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-8
2023-07-30 13:46:07,347 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-8 broker=0] Log loaded for partition primary.test-topic-1-8 with initial high watermark 0
2023-07-30 13:46:07,354 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-7, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,355 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-7 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,355 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-7 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-7
2023-07-30 13:46:07,355 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-7 broker=0] Log loaded for partition primary.test-topic-1-7 with initial high watermark 0
2023-07-30 13:46:07,362 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-6, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,363 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-6 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,363 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-6 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-6
2023-07-30 13:46:07,363 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-6 broker=0] Log loaded for partition primary.test-topic-1-6 with initial high watermark 0
2023-07-30 13:46:07,371 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-5, dir=/tmp/junit5301514646100141969/junit2114663958254445386] Loading producer state till offset 0 with message format version 2
2023-07-30 13:46:07,372 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-5 in /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:46:07,372 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-5 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-5
2023-07-30 13:46:07,372 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-5 broker=0] Log loaded for partition primary.test-topic-1-5 with initial high watermark 0
2023-07-30 13:46:07,381 - INFO  [kafka-admin-client-thread | adminclient-69:MirrorSourceConnector@317] - Increased size of primary.test-topic-1 to 10 partitions.
2023-07-30 13:46:07,383 - INFO  [Scheduler for MirrorSourceConnector-refreshing known target topics:Scheduler@95] - refreshing known target topics took 210 ms
2023-07-30 13:46:07,384 - INFO  [connector-thread-MirrorSourceConnector:MirrorSourceConnector@126] - Started MirrorSourceConnector with 21 topic-partitions.
2023-07-30 13:46:07,385 - INFO  [connector-thread-MirrorSourceConnector:MirrorSourceConnector@127] - Starting MirrorSourceConnector took 272 ms.
2023-07-30 13:46:07,388 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,388 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,394 - INFO  [Scheduler for MirrorSourceConnector-syncing topic configs:Scheduler@95] - syncing topic configs took 10 ms
2023-07-30 13:46:07,402 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Admin Manager on Broker 0]: Updating topic primary.test-topic-1 with new configuration kafka.server.KafkaConfig@8f36320b
2023-07-30 13:46:07,405 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Admin Manager on Broker 0]: Updating topic primary.test-topic-with-empty-partition with new configuration kafka.server.KafkaConfig@8f36320b
2023-07-30 13:46:07,406 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing notification(s) to /config/changes
2023-07-30 13:46:07,407 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Admin Manager on Broker 0]: Updating topic primary.heartbeats with new configuration kafka.server.KafkaConfig@8f36320b
2023-07-30 13:46:07,409 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing override for entityPath: topics/primary.test-topic-1 with config: HashMap()
2023-07-30 13:46:07,415 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing notification(s) to /config/changes
2023-07-30 13:46:07,416 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing override for entityPath: topics/primary.test-topic-with-empty-partition with config: HashMap()
2023-07-30 13:46:07,417 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing override for entityPath: topics/primary.heartbeats with config: HashMap()
2023-07-30 13:46:07,451 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:07,557 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:07,666 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:07,711 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-5, groupId=primary-mm2] Tasks [MirrorHeartbeatConnector-0] configs updated
2023-07-30 13:46:07,711 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-4, groupId=primary-mm2] Tasks [MirrorHeartbeatConnector-0] configs updated
2023-07-30 13:46:07,712 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-6, groupId=primary-mm2] Tasks [MirrorHeartbeatConnector-0] configs updated
2023-07-30 13:46:07,712 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@646] - [Worker clientId=connect-5, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:46:07,712 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@646] - [Worker clientId=connect-4, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:46:07,712 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@646] - [Worker clientId=connect-6, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:46:07,712 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,712 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,713 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,713 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:07,713 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,713 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:07,713 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 5 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-d9814915-0e67-45fb-aae4-be935085a7e3 during Stable)
2023-07-30 13:46:07,715 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 6 (__consumer_offsets-17)
2023-07-30 13:46:07,715 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=6, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:07,715 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=6, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:07,715 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=6, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:07,716 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 6
2023-07-30 13:46:07,718 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=6, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:07,718 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=6, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:07,718 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=6, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,718 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=6, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:07,718 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=6, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,718 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=6, connectorIds=[MirrorCheckpointConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:07,718 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 6
2023-07-30 13:46:07,718 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 6
2023-07-30 13:46:07,718 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,719 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 6
2023-07-30 13:46:07,719 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,719 - INFO  [StartAndStopExecutor-connect-4-2:DistributedHerder@1257] - [Worker clientId=connect-4, groupId=primary-mm2] Starting task MirrorHeartbeatConnector-0
2023-07-30 13:46:07,719 - INFO  [StartAndStopExecutor-connect-4-2:Worker@509] - Creating task MirrorHeartbeatConnector-0
2023-07-30 13:46:07,720 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:46:07,720 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:46:07,721 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorHeartbeatTask

2023-07-30 13:46:07,721 - INFO  [StartAndStopExecutor-connect-4-2:Worker@524] - Instantiated task MirrorHeartbeatConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorHeartbeatTask
2023-07-30 13:46:07,721 - INFO  [StartAndStopExecutor-connect-4-2:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorHeartbeatConnector-0 using the worker config
2023-07-30 13:46:07,721 - INFO  [StartAndStopExecutor-connect-4-2:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorHeartbeatConnector-0 using the worker config
2023-07-30 13:46:07,721 - INFO  [StartAndStopExecutor-connect-4-2:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorHeartbeatConnector-0 using the worker config
2023-07-30 13:46:07,724 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,724 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:07,725 - INFO  [StartAndStopExecutor-connect-4-2:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:46:07,726 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorHeartbeatConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:07,728 - WARN  [StartAndStopExecutor-connect-4-2:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:07,728 - WARN  [StartAndStopExecutor-connect-4-2:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:07,731 - INFO  [StartAndStopExecutor-connect-4-2:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:07,731 - INFO  [StartAndStopExecutor-connect-4-2:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:07,731 - INFO  [StartAndStopExecutor-connect-4-2:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739167731
2023-07-30 13:46:07,732 - INFO  [kafka-producer-network-thread | connector-producer-MirrorHeartbeatConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorHeartbeatConnector-0] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:07,738 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:07,740 - INFO  [task-thread-MirrorHeartbeatConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorHeartbeatConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = null
	task.assigned.partitions = null
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:46:07,740 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@233] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} Source task finished initialization and start
2023-07-30 13:46:07,772 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:07,878 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:07,984 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:08,089 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:08,195 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:08,218 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-5, groupId=primary-mm2] Tasks [MirrorCheckpointConnector-0] configs updated
2023-07-30 13:46:08,218 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-4, groupId=primary-mm2] Tasks [MirrorCheckpointConnector-0] configs updated
2023-07-30 13:46:08,218 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@646] - [Worker clientId=connect-5, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:46:08,218 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:08,218 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@646] - [Worker clientId=connect-4, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:46:08,218 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:08,218 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:08,218 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-6, groupId=primary-mm2] Tasks [MirrorCheckpointConnector-0] configs updated
2023-07-30 13:46:08,218 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:08,219 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 6 (__consumer_offsets-17) (reason: Updating metadata for member connect-5-6540e570-7125-4144-8383-1ffd804835f9 during Stable)
2023-07-30 13:46:08,300 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:08,304 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:46:08,389 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:46:08,390 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 6 ms
2023-07-30 13:46:08,406 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:08,511 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:08,617 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:08,722 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[],"type":"source"}
2023-07-30 13:46:08,725 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-5, groupId=primary-mm2] Tasks [MirrorSourceConnector-0] configs updated
2023-07-30 13:46:08,725 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-4, groupId=primary-mm2] Tasks [MirrorSourceConnector-0] configs updated
2023-07-30 13:46:08,725 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-6, groupId=primary-mm2] Tasks [MirrorSourceConnector-0] configs updated
2023-07-30 13:46:08,726 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@646] - [Worker clientId=connect-6, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:46:08,726 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:08,726 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:08,728 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 7 (__consumer_offsets-17)
2023-07-30 13:46:08,729 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=7, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:08,729 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=7, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:08,729 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=7, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:08,730 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 7
2023-07-30 13:46:08,731 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=7, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:08,731 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=7, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:08,731 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=7, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:08,732 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=10, connectorIds=[MirrorCheckpointConnector], taskIds=[MirrorCheckpointConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:08,731 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=10, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:08,732 - WARN  [DistributedHerder-connect-5-1:DistributedHerder@1094] - [Worker clientId=connect-5, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:46:08,732 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=10, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:08,732 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1157] - [Worker clientId=connect-5, groupId=primary-mm2] Current config state offset 8 is behind group assignment 10, reading to end of config log
2023-07-30 13:46:08,732 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:46:08,732 - WARN  [DistributedHerder-connect-4-1:DistributedHerder@1094] - [Worker clientId=connect-4, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:46:08,732 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1157] - [Worker clientId=connect-4, groupId=primary-mm2] Current config state offset 8 is behind group assignment 10, reading to end of config log
2023-07-30 13:46:08,732 - INFO  [StartAndStopExecutor-connect-6-2:DistributedHerder@1257] - [Worker clientId=connect-6, groupId=primary-mm2] Starting task MirrorSourceConnector-0
2023-07-30 13:46:08,732 - INFO  [StartAndStopExecutor-connect-6-2:Worker@509] - Creating task MirrorSourceConnector-0
2023-07-30 13:46:08,733 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:46:08,734 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:46:08,735 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorSourceTask

2023-07-30 13:46:08,735 - INFO  [StartAndStopExecutor-connect-6-2:Worker@524] - Instantiated task MirrorSourceConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorSourceTask
2023-07-30 13:46:08,735 - INFO  [StartAndStopExecutor-connect-6-2:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:46:08,736 - INFO  [StartAndStopExecutor-connect-6-2:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:46:08,736 - INFO  [StartAndStopExecutor-connect-6-2:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:46:08,736 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:08,736 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:08,736 - INFO  [StartAndStopExecutor-connect-6-2:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:46:08,736 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorSourceConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:08,738 - WARN  [StartAndStopExecutor-connect-6-2:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:08,738 - WARN  [StartAndStopExecutor-connect-6-2:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:08,739 - INFO  [StartAndStopExecutor-connect-6-2:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:08,739 - INFO  [StartAndStopExecutor-connect-6-2:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:08,739 - INFO  [StartAndStopExecutor-connect-6-2:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739168739
2023-07-30 13:46:08,740 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:08,741 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorSourceConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = null
	task.assigned.partitions = [test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9, test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9, heartbeats-0]
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:46:08,742 - INFO  [kafka-producer-network-thread | connector-producer-MirrorSourceConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:08,750 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-22
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:08,751 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:08,752 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:08,752 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739168751
2023-07-30 13:46:08,752 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:40623]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:08,754 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:08,754 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:08,755 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739168754
2023-07-30 13:46:08,756 - INFO  [kafka-producer-network-thread | producer-21:Metadata@279] - [Producer clientId=producer-21] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:08,829 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:37121"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"localhost:37429"}],"type":"source"}
2023-07-30 13:46:08,834 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorHeartbeatConnector/status is {"name":"MirrorHeartbeatConnector","connector":{"state":"RUNNING","worker_id":"localhost:37429"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"localhost:37121"}],"type":"source"}
2023-07-30 13:46:08,840 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorCheckpointConnector/status is {"name":"MirrorCheckpointConnector","connector":{"state":"RUNNING","worker_id":"localhost:44923"},"tasks":[],"type":"source"}
2023-07-30 13:46:08,945 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorCheckpointConnector/status is {"name":"MirrorCheckpointConnector","connector":{"state":"RUNNING","worker_id":"localhost:44923"},"tasks":[],"type":"source"}
2023-07-30 13:46:09,051 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorCheckpointConnector/status is {"name":"MirrorCheckpointConnector","connector":{"state":"RUNNING","worker_id":"localhost:44923"},"tasks":[],"type":"source"}
2023-07-30 13:46:09,156 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorCheckpointConnector/status is {"name":"MirrorCheckpointConnector","connector":{"state":"RUNNING","worker_id":"localhost:44923"},"tasks":[],"type":"source"}
2023-07-30 13:46:09,225 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1161] - [Worker clientId=connect-5, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 10
2023-07-30 13:46:09,225 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:46:09,225 - INFO  [StartAndStopExecutor-connect-5-2:DistributedHerder@1257] - [Worker clientId=connect-5, groupId=primary-mm2] Starting task MirrorCheckpointConnector-0
2023-07-30 13:46:09,226 - INFO  [StartAndStopExecutor-connect-5-2:Worker@509] - Creating task MirrorCheckpointConnector-0
2023-07-30 13:46:09,226 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1161] - [Worker clientId=connect-4, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 10
2023-07-30 13:46:09,226 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:46:09,226 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:09,226 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:46:09,226 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:46:09,226 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:09,226 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:09,227 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@646] - [Worker clientId=connect-4, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:46:09,227 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:09,227 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:09,227 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorCheckpointTask

2023-07-30 13:46:09,227 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 7 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-d9814915-0e67-45fb-aae4-be935085a7e3 during Stable)
2023-07-30 13:46:09,228 - INFO  [StartAndStopExecutor-connect-5-2:Worker@524] - Instantiated task MirrorCheckpointConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorCheckpointTask
2023-07-30 13:46:09,228 - INFO  [StartAndStopExecutor-connect-5-2:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorCheckpointConnector-0 using the worker config
2023-07-30 13:46:09,228 - INFO  [StartAndStopExecutor-connect-5-2:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorCheckpointConnector-0 using the worker config
2023-07-30 13:46:09,228 - INFO  [StartAndStopExecutor-connect-5-2:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorCheckpointConnector-0 using the worker config
2023-07-30 13:46:09,228 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:09,228 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:09,228 - INFO  [StartAndStopExecutor-connect-5-2:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:46:09,229 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:43245]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorCheckpointConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:46:09,230 - WARN  [StartAndStopExecutor-connect-5-2:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:46:09,230 - WARN  [StartAndStopExecutor-connect-5-2:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:46:09,231 - INFO  [StartAndStopExecutor-connect-5-2:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,231 - INFO  [StartAndStopExecutor-connect-5-2:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,231 - INFO  [StartAndStopExecutor-connect-5-2:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169231
2023-07-30 13:46:09,233 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:09,234 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@646] - [Worker clientId=connect-5, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:46:09,234 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:09,234 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:09,235 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorCheckpointConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = [consumer-group-testReplicationWithEmptyPartition, consumer-group-dummy]
	task.assigned.partitions = null
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:46:09,235 - INFO  [kafka-producer-network-thread | connector-producer-MirrorCheckpointConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorCheckpointConnector-0] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:09,236 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40623]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-23
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:09,237 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,237 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,237 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169237
2023-07-30 13:46:09,238 - INFO  [task-thread-MirrorCheckpointConnector-0:KafkaConsumer@1116] - [Consumer clientId=consumer-null-23, groupId=null] Subscribed to partition(s): mm2-offset-syncs.backup.internal-0
2023-07-30 13:46:09,238 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,239 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,239 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,239 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169239
2023-07-30 13:46:09,239 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,240 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,240 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,241 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169240
2023-07-30 13:46:09,242 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@233] - WorkerSourceTask{id=MirrorCheckpointConnector-0} Source task finished initialization and start
2023-07-30 13:46:09,244 - INFO  [task-thread-MirrorCheckpointConnector-0:Metadata@279] - [Consumer clientId=consumer-null-23, groupId=null] Cluster ID: 8pcqclJ2Qg-7GNfdkeMb6A
2023-07-30 13:46:09,246 - INFO  [task-thread-MirrorCheckpointConnector-0:SubscriptionState@396] - [Consumer clientId=consumer-null-23, groupId=null] Resetting offset for partition mm2-offset-syncs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:40623 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:09,260 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors/MirrorCheckpointConnector/status is {"name":"MirrorCheckpointConnector","connector":{"state":"RUNNING","worker_id":"localhost:44923"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"localhost:44923"}],"type":"source"}
2023-07-30 13:46:09,263 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,264 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,264 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,264 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169264
2023-07-30 13:46:09,266 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,267 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,267 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,268 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:09,268 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,268 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,268 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,268 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:09,268 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,269 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,269 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,269 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169269
2023-07-30 13:46:09,270 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-24
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:09,271 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,271 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,271 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169271
2023-07-30 13:46:09,271 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-24, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:09,271 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-24, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:09,273 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-24, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:09,275 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-24, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:09,275 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:09,275 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,275 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,275 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,276 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-24 unregistered
2023-07-30 13:46:09,276 - INFO  [kafka-admin-client-thread | adminclient-77:AppInfoParser@83] - App info kafka.admin.client for adminclient-77 unregistered
2023-07-30 13:46:09,277 - INFO  [kafka-admin-client-thread | adminclient-77:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,277 - INFO  [kafka-admin-client-thread | adminclient-77:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,277 - INFO  [kafka-admin-client-thread | adminclient-77:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,304 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 0 ms
2023-07-30 13:46:09,381 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,381 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,381 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,382 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:09,382 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,382 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:09,383 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,384 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,384 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,384 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,384 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169384
2023-07-30 13:46:09,384 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-25
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:09,385 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,386 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,386 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169385
2023-07-30 13:46:09,386 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-25, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:09,386 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-25, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:09,387 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-25, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:09,388 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:46:09,389 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 5 ms
2023-07-30 13:46:09,389 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-25, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:09,389 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:09,390 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,390 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,390 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,391 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-25 unregistered
2023-07-30 13:46:09,391 - INFO  [kafka-admin-client-thread | adminclient-78:AppInfoParser@83] - App info kafka.admin.client for adminclient-78 unregistered
2023-07-30 13:46:09,392 - INFO  [kafka-admin-client-thread | adminclient-78:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,392 - INFO  [kafka-admin-client-thread | adminclient-78:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,392 - INFO  [kafka-admin-client-thread | adminclient-78:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,493 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,494 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,494 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,495 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:09,495 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,495 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,495 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,495 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:09,495 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,495 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:09,495 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,495 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,496 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,496 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,496 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,496 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:09,496 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,496 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,496 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,496 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,496 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169496
2023-07-30 13:46:09,497 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-26
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:09,498 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,498 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,498 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169498
2023-07-30 13:46:09,498 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-26, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:09,498 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-26, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:09,499 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-26, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:09,501 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-26, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:09,501 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:09,502 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,502 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,502 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,502 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-26 unregistered
2023-07-30 13:46:09,503 - INFO  [kafka-admin-client-thread | adminclient-79:AppInfoParser@83] - App info kafka.admin.client for adminclient-79 unregistered
2023-07-30 13:46:09,503 - INFO  [kafka-admin-client-thread | adminclient-79:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,503 - INFO  [kafka-admin-client-thread | adminclient-79:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,503 - INFO  [kafka-admin-client-thread | adminclient-79:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,604 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,605 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,605 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,606 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:09,606 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,606 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,606 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,606 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:09,606 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,607 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,608 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,608 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169607
2023-07-30 13:46:09,608 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-27
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:09,609 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,609 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,609 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169609
2023-07-30 13:46:09,610 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-27, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:09,610 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-27, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:09,611 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-27, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:09,613 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-27, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:09,613 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:09,614 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,614 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,614 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,615 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-27 unregistered
2023-07-30 13:46:09,615 - INFO  [kafka-admin-client-thread | adminclient-80:AppInfoParser@83] - App info kafka.admin.client for adminclient-80 unregistered
2023-07-30 13:46:09,615 - INFO  [kafka-admin-client-thread | adminclient-80:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,615 - INFO  [kafka-admin-client-thread | adminclient-80:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,615 - INFO  [kafka-admin-client-thread | adminclient-80:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,717 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,717 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,717 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,718 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:09,718 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,718 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,718 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,718 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:09,718 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,719 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,719 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,720 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169719
2023-07-30 13:46:09,720 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-28
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:09,721 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,721 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,721 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169721
2023-07-30 13:46:09,721 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-28, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:09,722 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-28, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:09,723 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-28, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:09,724 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-28, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:09,725 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:09,725 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,725 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,725 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,726 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-28 unregistered
2023-07-30 13:46:09,726 - INFO  [kafka-admin-client-thread | adminclient-81:AppInfoParser@83] - App info kafka.admin.client for adminclient-81 unregistered
2023-07-30 13:46:09,726 - INFO  [kafka-admin-client-thread | adminclient-81:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,727 - INFO  [kafka-admin-client-thread | adminclient-81:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,727 - INFO  [kafka-admin-client-thread | adminclient-81:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,828 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,828 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,828 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,829 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:09,829 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,829 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,829 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,829 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:09,829 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,830 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,830 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,831 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169830
2023-07-30 13:46:09,831 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-29
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:09,832 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,832 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,832 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169832
2023-07-30 13:46:09,832 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-29, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:09,833 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-29, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:09,834 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-29, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:09,836 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-29, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:09,836 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:09,836 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,836 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,836 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,837 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-29 unregistered
2023-07-30 13:46:09,837 - INFO  [kafka-admin-client-thread | adminclient-82:AppInfoParser@83] - App info kafka.admin.client for adminclient-82 unregistered
2023-07-30 13:46:09,838 - INFO  [kafka-admin-client-thread | adminclient-82:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,838 - INFO  [kafka-admin-client-thread | adminclient-82:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,838 - INFO  [kafka-admin-client-thread | adminclient-82:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,939 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,939 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,940 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:09,940 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,941 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:09,942 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:09,942 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:09,942 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,942 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,942 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169942
2023-07-30 13:46:09,942 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-30
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:09,943 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:09,944 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:09,944 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739169943
2023-07-30 13:46:09,944 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-30, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:09,944 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-30, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:09,946 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-30, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:09,947 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-30, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:09,947 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:09,948 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,948 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,948 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:09,948 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-30 unregistered
2023-07-30 13:46:09,949 - INFO  [kafka-admin-client-thread | adminclient-83:AppInfoParser@83] - App info kafka.admin.client for adminclient-83 unregistered
2023-07-30 13:46:09,949 - INFO  [kafka-admin-client-thread | adminclient-83:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:09,949 - INFO  [kafka-admin-client-thread | adminclient-83:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:09,949 - INFO  [kafka-admin-client-thread | adminclient-83:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:10,051 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:10,051 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:10,051 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:10,052 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:10,052 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:10,052 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:10,052 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:10,052 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:10,053 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:10,054 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:10,054 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739170053
2023-07-30 13:46:10,054 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-31
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:10,055 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:10,055 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:10,055 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739170055
2023-07-30 13:46:10,056 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-31, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:10,056 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-31, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:10,057 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-31, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:10,059 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-31, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:10,059 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:10,059 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:10,059 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:10,059 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:10,060 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-31 unregistered
2023-07-30 13:46:10,060 - INFO  [kafka-admin-client-thread | adminclient-84:AppInfoParser@83] - App info kafka.admin.client for adminclient-84 unregistered
2023-07-30 13:46:10,061 - INFO  [kafka-admin-client-thread | adminclient-84:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:10,061 - INFO  [kafka-admin-client-thread | adminclient-84:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:10,061 - INFO  [kafka-admin-client-thread | adminclient-84:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:10,162 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:10,162 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:10,163 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:10,163 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:10,164 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:10,165 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:10,165 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:10,165 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:10,165 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:10,165 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:10,165 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:10,165 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739170165
2023-07-30 13:46:10,166 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-32
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:10,167 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:10,167 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:10,167 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739170167
2023-07-30 13:46:10,167 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-32, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:10,167 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-32, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:10,169 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-32, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:10,170 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-32, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:10,171 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:10,171 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:10,171 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:10,171 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:10,172 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-32 unregistered
2023-07-30 13:46:10,172 - INFO  [kafka-admin-client-thread | adminclient-85:AppInfoParser@83] - App info kafka.admin.client for adminclient-85 unregistered
2023-07-30 13:46:10,172 - INFO  [kafka-admin-client-thread | adminclient-85:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:10,172 - INFO  [kafka-admin-client-thread | adminclient-85:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:10,172 - INFO  [kafka-admin-client-thread | adminclient-85:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:10,274 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:10,274 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:43245
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:10,274 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:10,275 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:46:10,275 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:10,275 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:46:10,276 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:46:10,277 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:46:10,277 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:10,277 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:10,277 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739170277
2023-07-30 13:46:10,277 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43245]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-33
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:46:10,278 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:10,279 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:10,279 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739170278
2023-07-30 13:46:10,279 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-33, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:46:10,279 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-33, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:46:10,280 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-33, groupId=null] Cluster ID: QHd5ioUfRoqWk5XtKhBw9g
2023-07-30 13:46:10,282 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-33, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43245 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:46:10,304 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:46:10,389 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:46:10,389 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 5 ms
2023-07-30 13:46:10,786 - INFO  [main:MirrorClient@177] - Consumed 10 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:46:10,786 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:10,786 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:10,786 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:10,787 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-33 unregistered
2023-07-30 13:46:10,787 - INFO  [kafka-admin-client-thread | adminclient-86:AppInfoParser@83] - App info kafka.admin.client for adminclient-86 unregistered
2023-07-30 13:46:10,788 - INFO  [kafka-admin-client-thread | adminclient-86:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:10,788 - INFO  [kafka-admin-client-thread | adminclient-86:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:10,788 - INFO  [kafka-admin-client-thread | adminclient-86:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:10,797 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:40703/connectors is []
2023-07-30 13:46:10,803 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37121/connectors is ["MirrorCheckpointConnector","MirrorSourceConnector","MirrorHeartbeatConnector"]
2023-07-30 13:46:11,303 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 0 ms
2023-07-30 13:46:11,388 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:46:11,388 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:46:11,730 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-6, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:46:11,730 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:11,730 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:11,732 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 8 (__consumer_offsets-17)
2023-07-30 13:46:11,732 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=8, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:11,732 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=8, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:11,732 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=8, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:11,733 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 8
2023-07-30 13:46:11,734 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=8, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=8, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=10, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=8, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=10, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=10, connectorIds=[MirrorCheckpointConnector], taskIds=[MirrorCheckpointConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:11,735 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:11,736 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:11,736 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:11,736 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:11,737 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:11,737 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:46:11,747 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorCheckpointConnector'
2023-07-30 13:46:11,747 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorCheckpointConnector'
2023-07-30 13:46:11,747 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorCheckpointConnector'
2023-07-30 13:46:11,748 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorCheckpointConnector config removed
2023-07-30 13:46:11,748 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorCheckpointConnector config removed
2023-07-30 13:46:11,748 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorCheckpointConnector config removed
2023-07-30 13:46:11,748 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:11,748 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@592] - [Worker clientId=connect-5, groupId=primary-mm2] Handling connector-only config update by stopping connector MirrorCheckpointConnector
2023-07-30 13:46:11,748 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:11,748 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:11,748 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:11,748 - INFO  [DistributedHerder-connect-5-1:Worker@387] - Stopping connector MirrorCheckpointConnector
2023-07-30 13:46:11,748 - INFO  [DistributedHerder-connect-5-1:WorkerConnector@249] - Scheduled shutdown for WorkerConnector{id=MirrorCheckpointConnector}
2023-07-30 13:46:11,750 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 8 (__consumer_offsets-17) (reason: leader connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51 re-joining group during Stable)
2023-07-30 13:46:11,750 - INFO  [kafka-admin-client-thread | adminclient-71:AppInfoParser@83] - App info kafka.admin.client for adminclient-71 unregistered
2023-07-30 13:46:11,752 - INFO  [kafka-admin-client-thread | adminclient-71:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:11,752 - INFO  [kafka-admin-client-thread | adminclient-71:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:11,752 - INFO  [kafka-admin-client-thread | adminclient-71:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:11,752 - INFO  [connector-thread-MirrorCheckpointConnector:WorkerConnector@269] - Completed shutdown for WorkerConnector{id=MirrorCheckpointConnector}
2023-07-30 13:46:11,754 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:11,754 - INFO  [main:EmbeddedConnectCluster@689] - DELETE response for URL=http://localhost:37121/connectors/MirrorCheckpointConnector is empty
2023-07-30 13:46:11,754 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:11,756 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 9 (__consumer_offsets-17)
2023-07-30 13:46:11,757 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=9, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:11,757 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=9, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:11,757 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=9, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:11,759 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 9
2023-07-30 13:46:11,760 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=9, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:11,760 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=9, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:11,760 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=9, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:11,760 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=12, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:11,760 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=12, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:11,760 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:46:11,760 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:11,760 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:46:11,760 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:11,761 - INFO  [StartAndStopExecutor-connect-5-3:Worker@387] - Stopping connector MirrorCheckpointConnector
2023-07-30 13:46:11,761 - WARN  [StartAndStopExecutor-connect-5-3:Worker@390] - Ignoring stop request for unowned connector MirrorCheckpointConnector
2023-07-30 13:46:11,761 - WARN  [StartAndStopExecutor-connect-5-3:Worker@415] - Ignoring await stop request for non-present connector MirrorCheckpointConnector
2023-07-30 13:46:11,761 - INFO  [StartAndStopExecutor-connect-5-4:Worker@836] - Stopping task MirrorCheckpointConnector-0
2023-07-30 13:46:11,772 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorSourceConnector'
2023-07-30 13:46:11,772 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorSourceConnector'
2023-07-30 13:46:11,772 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorSourceConnector config removed
2023-07-30 13:46:11,773 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorSourceConnector config removed
2023-07-30 13:46:11,773 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@592] - [Worker clientId=connect-4, groupId=primary-mm2] Handling connector-only config update by stopping connector MirrorSourceConnector
2023-07-30 13:46:11,773 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorSourceConnector'
2023-07-30 13:46:11,773 - INFO  [DistributedHerder-connect-4-1:Worker@387] - Stopping connector MirrorSourceConnector
2023-07-30 13:46:11,773 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorSourceConnector config removed
2023-07-30 13:46:11,773 - INFO  [DistributedHerder-connect-4-1:WorkerConnector@249] - Scheduled shutdown for WorkerConnector{id=MirrorSourceConnector}
2023-07-30 13:46:11,773 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:11,773 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:11,773 - INFO  [kafka-admin-client-thread | adminclient-68:AppInfoParser@83] - App info kafka.admin.client for adminclient-68 unregistered
2023-07-30 13:46:11,775 - INFO  [kafka-admin-client-thread | adminclient-68:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:11,775 - INFO  [kafka-admin-client-thread | adminclient-68:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:11,775 - INFO  [kafka-admin-client-thread | adminclient-68:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:11,775 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 9 (__consumer_offsets-17) (reason: leader connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51 re-joining group during Stable)
2023-07-30 13:46:11,776 - INFO  [kafka-admin-client-thread | adminclient-69:AppInfoParser@83] - App info kafka.admin.client for adminclient-69 unregistered
2023-07-30 13:46:11,777 - INFO  [kafka-admin-client-thread | adminclient-69:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:11,777 - INFO  [kafka-admin-client-thread | adminclient-69:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:11,777 - INFO  [kafka-admin-client-thread | adminclient-69:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:11,777 - INFO  [connector-thread-MirrorSourceConnector:MirrorSourceConnector@141] - Stopping MirrorSourceConnector took 4 ms.
2023-07-30 13:46:11,777 - INFO  [connector-thread-MirrorSourceConnector:WorkerConnector@269] - Completed shutdown for WorkerConnector{id=MirrorSourceConnector}
2023-07-30 13:46:11,778 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:11,778 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:11,779 - INFO  [main:EmbeddedConnectCluster@689] - DELETE response for URL=http://localhost:37121/connectors/MirrorSourceConnector is empty
2023-07-30 13:46:12,258 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorCheckpointConnector-0} Committing offsets
2023-07-30 13:46:12,259 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:12,259 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:12,259 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:12,262 - INFO  [StartAndStopExecutor-connect-5-4:AppInfoParser@83] - App info kafka.consumer for consumer-null-23 unregistered
2023-07-30 13:46:12,262 - INFO  [kafka-admin-client-thread | adminclient-74:AppInfoParser@83] - App info kafka.admin.client for adminclient-74 unregistered
2023-07-30 13:46:12,263 - INFO  [kafka-admin-client-thread | adminclient-74:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:12,263 - INFO  [kafka-admin-client-thread | adminclient-74:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:12,263 - INFO  [kafka-admin-client-thread | adminclient-74:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:12,263 - INFO  [kafka-admin-client-thread | adminclient-75:AppInfoParser@83] - App info kafka.admin.client for adminclient-75 unregistered
2023-07-30 13:46:12,263 - INFO  [kafka-admin-client-thread | adminclient-75:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:12,264 - INFO  [kafka-admin-client-thread | adminclient-75:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:12,264 - INFO  [kafka-admin-client-thread | adminclient-75:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:12,264 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:12,264 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:12,264 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:12,264 - INFO  [StartAndStopExecutor-connect-5-4:MirrorCheckpointTask@121] - Stopping StartAndStopExecutor-connect-5-4 took 503 ms.
2023-07-30 13:46:12,264 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorCheckpointConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:46:12,277 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@574] - WorkerSourceTask{id=MirrorCheckpointConnector-0} Finished commitOffsets successfully in 18 ms
2023-07-30 13:46:12,277 - INFO  [task-thread-MirrorCheckpointConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorCheckpointConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:46:12,278 - INFO  [task-thread-MirrorCheckpointConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:12,279 - INFO  [task-thread-MirrorCheckpointConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:12,279 - INFO  [task-thread-MirrorCheckpointConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:12,279 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorCheckpointConnector-0 unregistered
2023-07-30 13:46:12,283 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-5, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:46:12,286 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-5, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:46:12,287 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[MirrorCheckpointConnector], revokedTaskIds=[MirrorCheckpointConnector-0], delay=0} with rebalance delay: 0
2023-07-30 13:46:12,287 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:46:12,287 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:12,287 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:12,287 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:12,289 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 10 (__consumer_offsets-17)
2023-07-30 13:46:12,289 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=10, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:12,289 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=10, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:12,289 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=10, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:12,291 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 10
2023-07-30 13:46:12,292 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=10, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:12,292 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=10, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:12,292 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=10, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:12,292 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=14, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:12,292 - INFO  [StartAndStopExecutor-connect-6-3:Worker@836] - Stopping task MirrorSourceConnector-0
2023-07-30 13:46:12,292 - WARN  [DistributedHerder-connect-5-1:DistributedHerder@1094] - [Worker clientId=connect-5, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:46:12,292 - INFO  [StartAndStopExecutor-connect-4-3:Worker@387] - Stopping connector MirrorSourceConnector
2023-07-30 13:46:12,293 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1157] - [Worker clientId=connect-5, groupId=primary-mm2] Current config state offset 12 is behind group assignment 14, reading to end of config log
2023-07-30 13:46:12,294 - WARN  [StartAndStopExecutor-connect-4-3:Worker@390] - Ignoring stop request for unowned connector MirrorSourceConnector
2023-07-30 13:46:12,294 - WARN  [StartAndStopExecutor-connect-4-3:Worker@415] - Ignoring await stop request for non-present connector MirrorSourceConnector
2023-07-30 13:46:12,294 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-4, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:46:12,294 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-4, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:46:12,294 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=14, connectorIds=[], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[MirrorSourceConnector], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:12,294 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:46:12,294 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:12,294 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:12,294 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:12,295 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 10 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-d9814915-0e67-45fb-aae4-be935085a7e3 during Stable)
2023-07-30 13:46:12,773 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1161] - [Worker clientId=connect-5, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 14
2023-07-30 13:46:12,773 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:46:12,773 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:12,773 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:12,773 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:15,397 - INFO  [kafka-coordinator-heartbeat-thread | primary-mm2:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-6, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:46:17,294 - ERROR [StartAndStopExecutor-connect-6-3:Worker@867] - Graceful stop of task MirrorSourceConnector-0 failed.
2023-07-30 13:46:17,295 - ERROR [task-thread-MirrorSourceConnector-0:OffsetStorageReaderImpl@113] - Failed to fetch offsets from namespace MirrorSourceConnector: 
org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-07-30 13:46:17,296 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-6, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:46:17,296 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorSourceConnector-0} Committing offsets
2023-07-30 13:46:17,296 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorSourceConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:46:17,296 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-6, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:46:17,296 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@187] - WorkerSourceTask{id=MirrorSourceConnector-0} Task threw an uncaught and unrecoverable exception
org.apache.kafka.connect.errors.ConnectException: Failed to fetch offsets.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:114)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	... 21 more
2023-07-30 13:46:17,296 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=14, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[MirrorSourceConnector-0], delay=0} with rebalance delay: 0
2023-07-30 13:46:17,296 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@188] - WorkerSourceTask{id=MirrorSourceConnector-0} Task is being killed and will not recover until manually restarted
2023-07-30 13:46:17,297 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,297 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,297 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,297 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:17,297 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:17,297 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.consumer for consumer-null-22 unregistered
2023-07-30 13:46:17,297 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=producer-21] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,298 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 11 (__consumer_offsets-17)
2023-07-30 13:46:17,298 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,298 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,298 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,299 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=11, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:17,299 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=11, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:17,299 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=11, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:17,299 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for producer-21 unregistered
2023-07-30 13:46:17,299 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,299 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,299 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,299 - INFO  [task-thread-MirrorSourceConnector-0:MirrorSourceTask@120] - Stopping task-thread-MirrorSourceConnector-0 took 2 ms.
2023-07-30 13:46:17,299 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:46:17,300 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 11
2023-07-30 13:46:17,300 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,300 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,301 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=11, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=11, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=14, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:17,301 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorSourceConnector-0 unregistered
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=14, connectorIds=[], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=11, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=14, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:17,301 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:46:17,302 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:17,315 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorHeartbeatConnector'
2023-07-30 13:46:17,316 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorHeartbeatConnector config removed
2023-07-30 13:46:17,316 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorHeartbeatConnector'
2023-07-30 13:46:17,316 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:17,316 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorHeartbeatConnector config removed
2023-07-30 13:46:17,316 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorHeartbeatConnector'
2023-07-30 13:46:17,316 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:17,316 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:17,316 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorHeartbeatConnector config removed
2023-07-30 13:46:17,316 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:17,316 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@592] - [Worker clientId=connect-6, groupId=primary-mm2] Handling connector-only config update by stopping connector MirrorHeartbeatConnector
2023-07-30 13:46:17,317 - INFO  [DistributedHerder-connect-6-1:Worker@387] - Stopping connector MirrorHeartbeatConnector
2023-07-30 13:46:17,317 - INFO  [DistributedHerder-connect-6-1:WorkerConnector@249] - Scheduled shutdown for WorkerConnector{id=MirrorHeartbeatConnector}
2023-07-30 13:46:17,317 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 11 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-d9814915-0e67-45fb-aae4-be935085a7e3 during Stable)
2023-07-30 13:46:17,317 - INFO  [connector-thread-MirrorHeartbeatConnector:WorkerConnector@269] - Completed shutdown for WorkerConnector{id=MirrorHeartbeatConnector}
2023-07-30 13:46:17,318 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:17,318 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:17,322 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 12 (__consumer_offsets-17)
2023-07-30 13:46:17,323 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=12, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:17,323 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=12, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:17,323 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=12, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:17,324 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 12
2023-07-30 13:46:17,325 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=12, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:17,325 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=12, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:17,325 - INFO  [main:EmbeddedConnectCluster@689] - DELETE response for URL=http://localhost:37121/connectors/MirrorHeartbeatConnector is empty
2023-07-30 13:46:17,325 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=12, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:17,326 - INFO  [StartAndStopExecutor-connect-4-4:Worker@836] - Stopping task MirrorHeartbeatConnector-0
2023-07-30 13:46:17,326 - INFO  [StartAndStopExecutor-connect-6-4:Worker@387] - Stopping connector MirrorHeartbeatConnector
2023-07-30 13:46:17,326 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:17,326 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:40623]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:17,326 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} Committing offsets
2023-07-30 13:46:17,326 - WARN  [StartAndStopExecutor-connect-6-4:Worker@390] - Ignoring stop request for unowned connector MirrorHeartbeatConnector
2023-07-30 13:46:17,326 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:46:17,326 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:46:17,326 - WARN  [StartAndStopExecutor-connect-6-4:Worker@415] - Ignoring await stop request for non-present connector MirrorHeartbeatConnector
2023-07-30 13:46:17,326 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:17,326 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-6, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:46:17,327 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:17,327 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:17,327 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739177327
2023-07-30 13:46:17,328 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-6, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:46:17,328 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[MirrorHeartbeatConnector], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:17,330 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:46:17,331 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:17,331 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:17,331 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:17,331 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 12 (__consumer_offsets-17) (reason: leader connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51 re-joining group during Stable)
2023-07-30 13:46:17,333 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@574] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} Finished commitOffsets successfully in 7 ms
2023-07-30 13:46:17,333 - INFO  [task-thread-MirrorHeartbeatConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorHeartbeatConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:46:17,334 - INFO  [task-thread-MirrorHeartbeatConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,334 - INFO  [task-thread-MirrorHeartbeatConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,334 - INFO  [task-thread-MirrorHeartbeatConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,334 - INFO  [task-thread-MirrorHeartbeatConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorHeartbeatConnector-0 unregistered
2023-07-30 13:46:17,336 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:43245]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:46:17,338 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-4, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:46:17,339 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-4, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:46:17,339 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:46:17,339 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[MirrorHeartbeatConnector-0], delay=0} with rebalance delay: 0
2023-07-30 13:46:17,340 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:46:17,340 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:17,340 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:17,340 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:17,340 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:46:17,341 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739177339
2023-07-30 13:46:17,346 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:40703/'}
2023-07-30 13:46:17,347 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:46:17,347 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:46:17,351 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@3113a37{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:46:17,351 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:46:17,352 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:46:17,352 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-1, groupId=backup-mm2] Herder stopping
2023-07-30 13:46:17,352 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@655] - [Worker clientId=connect-1, groupId=backup-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:46:17,352 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@1016] - [Worker clientId=connect-1, groupId=backup-mm2] Member connect-1-a8a55554-5291-434c-9af9-322de7d60f07 sending LeaveGroup request to coordinator localhost:40623 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:46:17,353 - WARN  [DistributedHerder-connect-1-1:AbstractCoordinator@997] - [Worker clientId=connect-1, groupId=backup-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:46:17,353 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,353 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-1-a8a55554-5291-434c-9af9-322de7d60f07] in group backup-mm2 has left, removing it from the group
2023-07-30 13:46:17,353 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,353 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group backup-mm2 in state PreparingRebalance with old generation 2 (__consumer_offsets-5) (reason: removing member connect-1-a8a55554-5291-434c-9af9-322de7d60f07 on LeaveGroup)
2023-07-30 13:46:17,353 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,354 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.connect for connect-1 unregistered
2023-07-30 13:46:17,354 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:17,354 - INFO  [DistributedHerder-connect-1-1:KafkaProducer@1193] - [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,355 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,355 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,355 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,355 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.producer for producer-3 unregistered
2023-07-30 13:46:17,356 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,356 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,357 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,357 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-2 unregistered
2023-07-30 13:46:17,357 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:17,358 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:46:17,358 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:17,358 - INFO  [DistributedHerder-connect-1-1:KafkaProducer@1193] - [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,358 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,358 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,358 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,359 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.producer for producer-4 unregistered
2023-07-30 13:46:17,359 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,359 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,360 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,360 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Removed 0 offsets associated with deleted partitions: mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4.
2023-07-30 13:46:17,360 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Removed 0 offsets associated with deleted partitions: mm2-configs.backup.internal-0.
2023-07-30 13:46:17,360 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-3 unregistered
2023-07-30 13:46:17,360 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:17,360 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:46:17,360 - INFO  [DistributedHerder-connect-1-1:Worker@209] - Worker stopping
2023-07-30 13:46:17,360 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:46:17,360 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:17,360 - INFO  [DistributedHerder-connect-1-1:KafkaProducer@1193] - [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,361 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,361 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,361 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,361 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.producer for producer-2 unregistered
2023-07-30 13:46:17,362 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,362 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,362 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,363 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-1 unregistered
2023-07-30 13:46:17,363 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:17,363 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:46:17,363 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,363 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,363 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,363 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.connect for localhost:40703 unregistered
2023-07-30 13:46:17,363 - INFO  [DistributedHerder-connect-1-1:Worker@230] - Worker stopped
2023-07-30 13:46:17,364 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@299] - [Worker clientId=connect-1, groupId=backup-mm2] Herder stopped
2023-07-30 13:46:17,364 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-1, groupId=backup-mm2] Herder stopped
2023-07-30 13:46:17,364 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:46:17,364 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:38925/'}
2023-07-30 13:46:17,364 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:46:17,364 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:46:17,366 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@181e72d3{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:46:17,366 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:46:17,366 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:46:17,367 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-2, groupId=backup-mm2] Herder stopping
2023-07-30 13:46:17,367 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@655] - [Worker clientId=connect-2, groupId=backup-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:46:17,367 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@1016] - [Worker clientId=connect-2, groupId=backup-mm2] Member connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3 sending LeaveGroup request to coordinator localhost:40623 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:46:17,367 - WARN  [DistributedHerder-connect-2-1:AbstractCoordinator@997] - [Worker clientId=connect-2, groupId=backup-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:46:17,367 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,367 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,367 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-2-a69a1c91-abf1-47f6-aaaa-db40b2bee5b3] in group backup-mm2 has left, removing it from the group
2023-07-30 13:46:17,367 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,368 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.connect for connect-2 unregistered
2023-07-30 13:46:17,368 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:17,368 - INFO  [DistributedHerder-connect-2-1:KafkaProducer@1193] - [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,371 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,371 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,371 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,371 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.producer for producer-6 unregistered
2023-07-30 13:46:17,371 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,371 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,372 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,372 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-5 unregistered
2023-07-30 13:46:17,372 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:17,372 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:46:17,372 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:17,372 - INFO  [DistributedHerder-connect-2-1:KafkaProducer@1193] - [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,373 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,373 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,373 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,373 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.producer for producer-7 unregistered
2023-07-30 13:46:17,374 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,374 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,374 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,375 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-6 unregistered
2023-07-30 13:46:17,375 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:17,375 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:46:17,375 - INFO  [DistributedHerder-connect-2-1:Worker@209] - Worker stopping
2023-07-30 13:46:17,375 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:46:17,375 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:17,376 - INFO  [DistributedHerder-connect-2-1:KafkaProducer@1193] - [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,377 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,377 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,378 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,378 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.producer for producer-5 unregistered
2023-07-30 13:46:17,378 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,379 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,379 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,381 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-4 unregistered
2023-07-30 13:46:17,382 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:17,382 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:46:17,382 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,382 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,382 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,382 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.connect for localhost:38925 unregistered
2023-07-30 13:46:17,382 - INFO  [DistributedHerder-connect-2-1:Worker@230] - Worker stopped
2023-07-30 13:46:17,382 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:46:17,383 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@299] - [Worker clientId=connect-2, groupId=backup-mm2] Herder stopped
2023-07-30 13:46:17,382 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:46:17,384 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:46:17,384 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:46:17,384 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-2, groupId=backup-mm2] Herder stopped
2023-07-30 13:46:17,385 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:46:17,385 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:44117/'}
2023-07-30 13:46:17,385 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:46:17,385 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:46:17,388 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@28da7d11{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:46:17,389 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:46:17,390 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:46:17,390 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-3, groupId=backup-mm2] Herder stopping
2023-07-30 13:46:17,390 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@655] - [Worker clientId=connect-3, groupId=backup-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:46:17,391 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@1016] - [Worker clientId=connect-3, groupId=backup-mm2] Member connect-3-617ca672-0829-41dc-b8e2-9d66aba06a1c sending LeaveGroup request to coordinator localhost:40623 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:46:17,391 - WARN  [DistributedHerder-connect-3-1:AbstractCoordinator@997] - [Worker clientId=connect-3, groupId=backup-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:46:17,391 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,391 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,391 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-3-617ca672-0829-41dc-b8e2-9d66aba06a1c] in group backup-mm2 has left, removing it from the group
2023-07-30 13:46:17,391 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,392 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Group backup-mm2 with generation 3 is now empty (__consumer_offsets-5)
2023-07-30 13:46:17,393 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.connect for connect-3 unregistered
2023-07-30 13:46:17,393 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:17,393 - INFO  [DistributedHerder-connect-3-1:KafkaProducer@1193] - [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,394 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,394 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,394 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,394 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.producer for producer-9 unregistered
2023-07-30 13:46:17,395 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,395 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,396 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,396 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-8 unregistered
2023-07-30 13:46:17,397 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:46:17,397 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:46:17,397 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:17,397 - INFO  [DistributedHerder-connect-3-1:KafkaProducer@1193] - [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,397 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:46:17,397 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:46:17,398 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:46:17,398 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,398 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:46:17,399 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,399 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,399 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.producer for producer-10 unregistered
2023-07-30 13:46:17,399 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,399 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,399 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,400 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-9 unregistered
2023-07-30 13:46:17,400 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:46:17,400 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:46:17,400 - INFO  [DistributedHerder-connect-3-1:Worker@209] - Worker stopping
2023-07-30 13:46:17,401 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:46:17,402 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:17,402 - INFO  [DistributedHerder-connect-3-1:KafkaProducer@1193] - [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,403 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,404 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,404 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,404 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.producer for producer-8 unregistered
2023-07-30 13:46:17,405 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,405 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,405 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,406 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-7 unregistered
2023-07-30 13:46:17,407 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:46:17,407 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:46:17,407 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,407 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,407 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,407 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.connect for localhost:44117 unregistered
2023-07-30 13:46:17,407 - INFO  [DistributedHerder-connect-3-1:Worker@230] - Worker stopped
2023-07-30 13:46:17,408 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@299] - [Worker clientId=connect-3, groupId=backup-mm2] Herder stopped
2023-07-30 13:46:17,409 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-3, groupId=backup-mm2] Herder stopped
2023-07-30 13:46:17,409 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:46:17,409 - INFO  [main:KafkaProducer@1193] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:17,410 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 58 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,411 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:17,411 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:17,411 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 61 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,410 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 58 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,411 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:17,412 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-1 unregistered
2023-07-30 13:46:17,412 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2023-07-30 13:46:17,413 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Log for partition mm2-configs.backup.internal-0 is renamed to /tmp/junit6154326205091447453/junit85516104450203242/mm2-configs.backup.internal-0.26006660dcd7489fbd74aa0c3d9275c1-delete and is scheduled for deletion
2023-07-30 13:46:17,413 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-status.primary.internal-1 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-1.6f0ae68184ab4a819f20dc7bb52cae8e-delete and is scheduled for deletion
2023-07-30 13:46:17,414 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2023-07-30 13:46:17,419 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-status.primary.internal-0 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-0.f2a385b4bf714e6e9a946522fe407e80-delete and is scheduled for deletion
2023-07-30 13:46:17,420 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-status.primary.internal-3 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-3.51805966e6104b29af78d972df526505-delete and is scheduled for deletion
2023-07-30 13:46:17,421 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-status.primary.internal-2 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-2.97281e48b03c4f6ea9784a41543dac30-delete and is scheduled for deletion
2023-07-30 13:46:17,422 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-status.primary.internal-4 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-status.primary.internal-4.69189788c14246468f590ea5e61c476f-delete and is scheduled for deletion
2023-07-30 13:46:17,431 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2023-07-30 13:46:17,434 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2023-07-30 13:46:17,434 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2023-07-30 13:46:17,434 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2023-07-30 13:46:17,435 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Group consumer-group-testReplicationWithEmptyPartition transitioned to Dead in generation 2
2023-07-30 13:46:17,435 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopping socket server request processors
2023-07-30 13:46:17,438 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Group backup-mm2 transitioned to Dead in generation 3
2023-07-30 13:46:17,439 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Group consumer-group-dummy transitioned to Dead in generation 2
2023-07-30 13:46:17,440 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopped socket server request processors
2023-07-30 13:46:17,440 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Removed 20 offsets associated with deleted partitions: heartbeats-0, test-topic-with-empty-partition-3, test-topic-with-empty-partition-7, test-topic-with-empty-partition-2, test-topic-with-empty-partition-6, test-topic-with-empty-partition-5, test-topic-with-empty-partition-1, test-topic-with-empty-partition-9, test-topic-with-empty-partition-4, test-topic-with-empty-partition-0, test-topic-with-empty-partition-8, mm2-status.backup.internal-2, mm2-status.backup.internal-3, mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-offset-syncs.backup.internal-0, backup.test-topic-1-0, test-topic-1-3, test-topic-1-7, test-topic-1-2, test-topic-1-6, test-topic-1-5, test-topic-1-9, test-topic-1-1, test-topic-1-4, test-topic-1-8, test-topic-1-0, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-21, mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-15.
2023-07-30 13:46:17,441 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2023-07-30 13:46:17,443 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupMetadataManager brokerId=0] Group consumer-group-dummy transitioned to Dead in generation 2
2023-07-30 13:46:17,445 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Removed 10 offsets associated with deleted partitions: mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-21, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-7, heartbeats-0, primary.test-topic-1-9, primary.test-topic-1-0, primary.test-topic-1-4, primary.test-topic-1-6, primary.test-topic-1-1, primary.test-topic-1-5, primary.test-topic-1-7, primary.test-topic-1-2, primary.test-topic-1-8, primary.test-topic-1-3, mm2-configs.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-with-empty-partition-1, primary.test-topic-with-empty-partition-6, primary.test-topic-with-empty-partition-2, primary.test-topic-with-empty-partition-7, primary.test-topic-with-empty-partition-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-with-empty-partition-0, primary.heartbeats-0, test-topic-1-3, test-topic-1-7, test-topic-1-2, test-topic-1-6, test-topic-1-5, test-topic-1-9, test-topic-1-1, test-topic-1-4, test-topic-1-8, test-topic-1-0.
2023-07-30 13:46:17,451 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:46:17,451 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:46:17,454 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2023-07-30 13:46:17,457 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:46:17,457 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:46:17,458 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2023-07-30 13:46:17,459 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 41 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,459 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 41 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,459 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 60 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,469 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 65 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,469 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 51 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,469 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 51 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,476 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-2 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-2.f084a18ed2124798bb0b1cd87b06fc91-delete and is scheduled for deletion
2023-07-30 13:46:17,476 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-9 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-9.0ec90f9fc0184bbbbcc5dcac4fbdc1b4-delete and is scheduled for deletion
2023-07-30 13:46:17,477 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-5 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-5.d6afb9d4ee7e44038a1bd0b650e988c5-delete and is scheduled for deletion
2023-07-30 13:46:17,478 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-9 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-9.cc915ac499564f348c026e99e64ae25a-delete and is scheduled for deletion
2023-07-30 13:46:17,479 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-23 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-23.34c7a7c5bcfc4fcbb675ca3f601b38c7-delete and is scheduled for deletion
2023-07-30 13:46:17,480 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition heartbeats-0 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/heartbeats-0.59810563242641f2abf06fe32b6bd0f0-delete and is scheduled for deletion
2023-07-30 13:46:17,480 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-3 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-3.d99f5b89be9047a1ba749172f7fb2f82-delete and is scheduled for deletion
2023-07-30 13:46:17,481 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-18 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-18.dd67eedcf8604476a42c07479d07499c-delete and is scheduled for deletion
2023-07-30 13:46:17,481 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-0 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-0.4374aabfee3e4ed4b2f51f9f574c2554-delete and is scheduled for deletion
2023-07-30 13:46:17,482 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-7 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-7.90c7f138a4ee42ecbe31958e166905a1-delete and is scheduled for deletion
2023-07-30 13:46:17,482 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-14 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-14.dbd8c3e42d49412ebaf7394f05b45479-delete and is scheduled for deletion
2023-07-30 13:46:17,483 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-4 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-4.330116a8eebf41d79bcb0670aa034208-delete and is scheduled for deletion
2023-07-30 13:46:17,483 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-10 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-10.00d05ef99a0a4f3f811352b9166c8ddf-delete and is scheduled for deletion
2023-07-30 13:46:17,484 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-1 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-1.689fa7082e0d45548af3e56598b85256-delete and is scheduled for deletion
2023-07-30 13:46:17,484 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-6 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-6.8f18426a738d43b9a90c13fe7fb28ac0-delete and is scheduled for deletion
2023-07-30 13:46:17,485 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-6 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-6.56fd5444ce0a491398ba283008b1f291-delete and is scheduled for deletion
2023-07-30 13:46:17,485 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-1 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-1.f2b3f33af7c448b29fc8d4aac0d9e656-delete and is scheduled for deletion
2023-07-30 13:46:17,486 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-2 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-2.efd7f055252b465d9353d6a1d4cc2f2a-delete and is scheduled for deletion
2023-07-30 13:46:17,487 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-6 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-6.bb6a731ae78e46c78b8c5088d0c0f007-delete and is scheduled for deletion
2023-07-30 13:46:17,487 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-22 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-22.af6bb703fa4e4448969fcb017330fbd2-delete and is scheduled for deletion
2023-07-30 13:46:17,488 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-2 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-2.eb4bab277e2e49ee95f1c02033c4391d-delete and is scheduled for deletion
2023-07-30 13:46:17,489 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-17 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-17.abe82fdc8d9249f689b161120b2c07eb-delete and is scheduled for deletion
2023-07-30 13:46:17,489 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-1 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-1.f88ba72e8ebd4d16b7462f1d0f240a61-delete and is scheduled for deletion
2023-07-30 13:46:17,490 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-6 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-6.5c1b597224094b9892c7edf9bc8c3177-delete and is scheduled for deletion
2023-07-30 13:46:17,491 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-13 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-13.efb62bb2cc9741b3a31a9d8c1e92d5b1-delete and is scheduled for deletion
2023-07-30 13:46:17,491 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-5 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-5.156ca8c0885945d8b487a00ed11df107-delete and is scheduled for deletion
2023-07-30 13:46:17,492 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-9 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-9.90a5b66a8dcd43aea10dd12014ab33b4-delete and is scheduled for deletion
2023-07-30 13:46:17,492 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-5 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-5.c098ade339eb4492bd8114ac552ad641-delete and is scheduled for deletion
2023-07-30 13:46:17,493 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-7 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-7.5b4d8132b8cf43288c63847200ff4788-delete and is scheduled for deletion
2023-07-30 13:46:17,493 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-0 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-0.af84cad1e91b4e52a4b4da61ca60722b-delete and is scheduled for deletion
2023-07-30 13:46:17,494 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.checkpoints.internal-0 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.checkpoints.internal-0.a61429a667a4487ba53c52f962546620-delete and is scheduled for deletion
2023-07-30 13:46:17,495 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-3 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-3.16860407da1b4cfa82bfb93f58eb10cd-delete and is scheduled for deletion
2023-07-30 13:46:17,495 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-configs.primary.internal-0 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-configs.primary.internal-0.1653b1db760f4d029f75f2ceede5c154-delete and is scheduled for deletion
2023-07-30 13:46:17,496 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-7 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-7.e9a9e2d4acb34a8384b1a3f4c06da80e-delete and is scheduled for deletion
2023-07-30 13:46:17,496 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-21 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-21.d087704d520044fa866a2e0ffbd021b4-delete and is scheduled for deletion
2023-07-30 13:46:17,497 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-5 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-5.4148c83bf46342808cb2e0436d461c87-delete and is scheduled for deletion
2023-07-30 13:46:17,497 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-16 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-16.c73324086b21436cbf96bbb8a94f53e8-delete and is scheduled for deletion
2023-07-30 13:46:17,498 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-2 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-2.2f0bb4a1fec04d169d43e85dcd928a25-delete and is scheduled for deletion
2023-07-30 13:46:17,498 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-9 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-9.34aa865db0f047d1b9bd8bd20ddf6f33-delete and is scheduled for deletion
2023-07-30 13:46:17,499 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-12 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-12.d3630758d9ab462a92e4cd7518af09bd-delete and is scheduled for deletion
2023-07-30 13:46:17,499 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-8 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-8.eaf28a38670c436eba11fdc0fd8781d0-delete and is scheduled for deletion
2023-07-30 13:46:17,500 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-1 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-1.e5f612dd6e6449f0a946e42cfb467f4b-delete and is scheduled for deletion
2023-07-30 13:46:17,500 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-4 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-4.b11f355227af4fc587cfc76770addde1-delete and is scheduled for deletion
2023-07-30 13:46:17,501 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-3 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-3.1dda8041ce5c4d759a05d6468c24a461-delete and is scheduled for deletion
2023-07-30 13:46:17,501 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-8 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-8.c058b13079e447179e7aac077fa55a7d-delete and is scheduled for deletion
2023-07-30 13:46:17,502 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-4 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-4.61cb4ff6b78b4cf3b63f838c54cc6c24-delete and is scheduled for deletion
2023-07-30 13:46:17,502 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-8 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-8.eccc0b7188244035b93ba0c61cade924-delete and is scheduled for deletion
2023-07-30 13:46:17,503 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-24 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-24.b2f3851a29534dffb996e96b5f8e1d8f-delete and is scheduled for deletion
2023-07-30 13:46:17,503 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-20 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-20.32d83ddc615a4107a22053bc4f68a093-delete and is scheduled for deletion
2023-07-30 13:46:17,504 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-19 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-19.e422d939417a45e49a84514179a2110a-delete and is scheduled for deletion
2023-07-30 13:46:17,504 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-4 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-4.8848d2796e824d3a9d5faad5dc5a008d-delete and is scheduled for deletion
2023-07-30 13:46:17,505 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-15 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-15.005dfccbf3b94ec4a14a22c6047d3fb3-delete and is scheduled for deletion
2023-07-30 13:46:17,505 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-3 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-1-3.6041d9a771c048d2a006a6ab34b32cd7-delete and is scheduled for deletion
2023-07-30 13:46:17,506 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-8 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-8.37e40812230a4b029745f67f443070f2-delete and is scheduled for deletion
2023-07-30 13:46:17,506 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-11 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-11.7d7c223f77a84c52a7738f1740adde4d-delete and is scheduled for deletion
2023-07-30 13:46:17,507 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.heartbeats-0 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.heartbeats-0.d094d5a800a8454c86e005599b072b07-delete and is scheduled for deletion
2023-07-30 13:46:17,508 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-0 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/primary.test-topic-with-empty-partition-0.0f5038930fe3446a8d171a09b7006010-delete and is scheduled for deletion
2023-07-30 13:46:17,508 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-7 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/mm2-offsets.primary.internal-7.7e16c528877b46f4b68ec1d6ee5ac0ec-delete and is scheduled for deletion
2023-07-30 13:46:17,508 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-0 is renamed to /tmp/junit5301514646100141969/junit2114663958254445386/test-topic-1-0.85e77f2eac98429c8c321414dcd7653f-delete and is scheduled for deletion
2023-07-30 13:46:17,511 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 182 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,512 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 180 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,512 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 176 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,539 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:17,541 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:17,559 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 136 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,559 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 131 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,560 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 143 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,569 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 214 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,570 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 176 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,570 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 197 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,612 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 314 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,612 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 337 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,612 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 350 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,635 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2023-07-30 13:46:17,635 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2023-07-30 13:46:17,636 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2023-07-30 13:46:17,637 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2023-07-30 13:46:17,639 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:17,643 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:17,659 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 252 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,659 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 240 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,660 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 250 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,669 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 368 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,669 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 418 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,670 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 328 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,712 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 478 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,712 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 541 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,713 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 525 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,743 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:17,759 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 395 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,760 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 369 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,760 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 379 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,769 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 652 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,769 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 562 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,771 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 495 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,819 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 723 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,819 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 638 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,819 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 746 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,835 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2023-07-30 13:46:17,835 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2023-07-30 13:46:17,837 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2023-07-30 13:46:17,838 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2023-07-30 13:46:17,839 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2023-07-30 13:46:17,839 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2023-07-30 13:46:17,841 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2023-07-30 13:46:17,841 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2023-07-30 13:46:17,842 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2023-07-30 13:46:17,842 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2023-07-30 13:46:17,843 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2023-07-30 13:46:17,844 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:17,859 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 523 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,859 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 485 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,860 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 487 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,861 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:17,870 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 733 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,870 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 856 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,872 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 641 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,918 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 991 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,919 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 956 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,919 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 829 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,945 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:17,959 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 689 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,959 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 629 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,960 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 626 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,970 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 947 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,971 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1135 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:17,972 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 810 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,018 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1228 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,019 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1203 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,020 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1021 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,029 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2023-07-30 13:46:18,029 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2023-07-30 13:46:18,030 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2023-07-30 13:46:18,046 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,059 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 862 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,059 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 777 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,060 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 767 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,070 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1159 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,071 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1414 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,072 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 976 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,118 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1497 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,119 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1458 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,120 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1219 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,147 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,153 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2023-07-30 13:46:18,153 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2023-07-30 13:46:18,154 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2023-07-30 13:46:18,155 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2023-07-30 13:46:18,155 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2023-07-30 13:46:18,155 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2023-07-30 13:46:18,155 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2023-07-30 13:46:18,156 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2023-07-30 13:46:18,158 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2023-07-30 13:46:18,158 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2023-07-30 13:46:18,159 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2023-07-30 13:46:18,159 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2023-07-30 13:46:18,159 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1046 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,160 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 936 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,160 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 920 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,171 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1371 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,171 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1707 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,172 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1152 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,219 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1795 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,219 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1744 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,220 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1425 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,248 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,259 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1243 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,260 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1095 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,260 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1072 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,271 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2020 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,271 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1598 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,273 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1332 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,319 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2108 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,319 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2040 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,320 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1643 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,338 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,349 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,354 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2023-07-30 13:46:18,354 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2023-07-30 13:46:18,354 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2023-07-30 13:46:18,359 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1440 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,360 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1255 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,361 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1229 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,371 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2339 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,371 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1832 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,374 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1519 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,419 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2343 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,419 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2424 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,420 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1872 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,436 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2023-07-30 13:46:18,436 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2023-07-30 13:46:18,436 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2023-07-30 13:46:18,450 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,459 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1641 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,460 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1423 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,461 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1389 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,471 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2675 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,475 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2077 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,475 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1704 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,521 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2646 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,521 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2737 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,522 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2107 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,551 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,559 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1809 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,561 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1560 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,561 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1522 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,571 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2954 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,574 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2289 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,576 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1873 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,621 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 3025 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,621 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2912 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,622 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2317 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,636 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2023-07-30 13:46:18,636 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2023-07-30 13:46:18,636 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2023-07-30 13:46:18,651 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,659 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2030 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,661 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1688 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,662 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1733 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,671 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 3311 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,675 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2552 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,677 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2073 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,721 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 3270 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,721 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 3411 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,722 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2574 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,752 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,759 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2266 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,761 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1865 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,761 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1922 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,771 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 3745 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,775 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2853 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,776 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2290 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,821 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 3689 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,821 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 3861 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,822 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2874 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,836 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2023-07-30 13:46:18,836 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2023-07-30 13:46:18,838 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2023-07-30 13:46:18,838 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutting down
2023-07-30 13:46:18,839 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Stopped
2023-07-30 13:46:18,839 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:46:18,839 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:46:18,839 - INFO  [main:Logging@66] - Shutting down.
2023-07-30 13:46:18,853 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:18,855 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-4] Writing producer snapshot at offset 1
2023-07-30 13:46:18,859 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-9] Writing producer snapshot at offset 1
2023-07-30 13:46:18,859 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2520 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,860 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-4] Writing producer snapshot at offset 1
2023-07-30 13:46:18,861 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2049 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,861 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-6] Writing producer snapshot at offset 1
2023-07-30 13:46:18,861 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2108 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,862 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-6] Writing producer snapshot at offset 1
2023-07-30 13:46:18,866 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-0] Writing producer snapshot at offset 1
2023-07-30 13:46:18,868 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-5] Writing producer snapshot at offset 1
2023-07-30 13:46:18,870 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-1] Writing producer snapshot at offset 1
2023-07-30 13:46:18,871 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 4196 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,874 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-5] Writing producer snapshot at offset 1
2023-07-30 13:46:18,875 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-35] Writing producer snapshot at offset 23
2023-07-30 13:46:18,875 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 3161 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,876 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2518 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,878 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-7] Writing producer snapshot at offset 1
2023-07-30 13:46:18,881 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-8] Writing producer snapshot at offset 1
2023-07-30 13:46:18,882 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-7] Writing producer snapshot at offset 1
2023-07-30 13:46:18,883 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-8] Writing producer snapshot at offset 1
2023-07-30 13:46:18,884 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-1] Writing producer snapshot at offset 1
2023-07-30 13:46:18,886 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-3] Writing producer snapshot at offset 1
2023-07-30 13:46:18,888 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-3] Writing producer snapshot at offset 1
2023-07-30 13:46:18,888 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-0] Writing producer snapshot at offset 1
2023-07-30 13:46:18,889 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-2] Writing producer snapshot at offset 1
2023-07-30 13:46:18,890 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-2] Writing producer snapshot at offset 1
2023-07-30 13:46:18,891 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-5] Writing producer snapshot at offset 4
2023-07-30 13:46:18,893 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-34] Writing producer snapshot at offset 23
2023-07-30 13:46:18,906 - INFO  [main:Logging@66] - Shutdown complete.
2023-07-30 13:46:18,914 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2023-07-30 13:46:18,914 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2023-07-30 13:46:18,914 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2023-07-30 13:46:18,915 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2023-07-30 13:46:18,921 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 4082 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,921 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 4270 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,922 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 3156 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,959 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2754 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,961 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2303 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,961 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2233 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,971 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 4639 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,975 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 3428 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:18,976 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2753 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,019 - INFO  [main:ZooKeeper@1422] - Session: 0x1087f2e35810000 closed
2023-07-30 13:46:19,019 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x1087f2e35810000
2023-07-30 13:46:19,020 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2023-07-30 13:46:19,020 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2023-07-30 13:46:19,021 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 4695 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,021 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 4471 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,022 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 3450 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,050 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:19,059 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3022 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,061 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2484 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,061 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2411 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,071 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 5056 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,075 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 3761 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,076 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2971 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,122 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 5082 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,122 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 4832 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,122 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 3723 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,159 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3258 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,161 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2662 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,161 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2578 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,171 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 5495 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,175 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4066 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,177 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3175 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,222 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 5253 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,222 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 5533 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,222 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 4017 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,259 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3515 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,261 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2780 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,261 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2875 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,271 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 5971 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,275 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4375 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,277 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3437 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,322 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 6037 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,322 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 4342 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,322 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 5723 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,359 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3788 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,361 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2983 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,361 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3090 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,371 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 6473 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,375 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4701 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,378 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3688 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,422 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 6502 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,422 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 4642 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,422 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 6153 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,459 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 4037 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,461 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3322 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,461 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3204 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,471 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 6931 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,475 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4992 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,478 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3996 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,522 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 6581 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,522 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 6963 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,522 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 4939 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,559 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 4320 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,561 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3556 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,561 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3425 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,571 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 7473 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,575 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 5346 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,578 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 4264 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,622 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 7444 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,622 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 5288 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,622 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 7049 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,659 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 4591 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,661 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3772 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,661 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3627 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,671 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 7982 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,687 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 5705 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,687 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 4531 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,722 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 7898 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,722 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 5568 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,722 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 7459 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,759 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 4820 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,761 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3812 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,761 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3964 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,771 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 8483 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,787 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 6029 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,787 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 4801 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,822 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 7938 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,822 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 5897 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,822 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 8425 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,838 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2023-07-30 13:46:19,838 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2023-07-30 13:46:19,838 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2023-07-30 13:46:19,859 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 5099 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,861 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 4027 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,861 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 4180 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,871 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 8997 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,887 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 5054 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,887 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 6425 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,922 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 8903 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,922 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 6287 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,922 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 8389 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,959 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 5387 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,961 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 4396 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,961 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 4236 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,971 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 9527 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,987 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 6780 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:19,987 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 5338 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,022 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 8867 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,022 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 6616 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,022 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 9416 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,059 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 5673 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,061 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 4628 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,061 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 4457 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,071 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 10037 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,087 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 7122 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,087 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 5621 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,122 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 9882 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,122 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 6955 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,122 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 9308 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,159 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 5973 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,161 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 4859 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,161 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 4674 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,171 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 10548 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,183 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:20,187 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 7476 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,188 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 5899 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,222 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 10339 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,222 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 7317 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,222 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 9724 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,259 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 6215 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,261 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 4865 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,261 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 5060 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,271 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 10961 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,287 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 7817 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,288 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 6148 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,322 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 10758 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,322 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 10117 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,322 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 7645 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,324 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-5, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:46:20,324 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:46:20,324 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:46:20,326 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 13 (__consumer_offsets-17)
2023-07-30 13:46:20,326 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=13, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:20,326 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=13, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:20,326 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=13, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:20,327 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 13
2023-07-30 13:46:20,328 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=13, memberId='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', protocol='sessioned'}
2023-07-30 13:46:20,328 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=13, memberId='connect-4-d9814915-0e67-45fb-aae4-be935085a7e3', protocol='sessioned'}
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=13, memberId='connect-5-6540e570-7125-4144-8383-1ffd804835f9', protocol='sessioned'}
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51', leaderUrl='http://localhost:37429/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:46:20,329 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:46:20,359 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 6504 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,361 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 5291 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,361 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 5086 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,371 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 11505 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,387 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 8194 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,388 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 6442 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,422 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 11279 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,422 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 7996 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,422 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 10585 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,459 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 6820 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,461 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 5313 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,461 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 5524 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,471 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 12028 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,487 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 8581 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,489 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 6730 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,522 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 11782 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,522 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 11051 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,522 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 8336 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,559 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 7106 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,561 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 5761 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,561 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 5536 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,571 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 12583 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,587 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 8910 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,589 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 7047 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,622 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 11550 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,622 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 12323 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,622 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 8654 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,659 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 7386 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,661 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 5764 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,661 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 6009 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,671 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 13141 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,687 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 9270 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,689 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 7352 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,722 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 12061 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,722 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 12867 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,722 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 9011 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,759 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 7658 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,761 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 5997 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,761 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 6249 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,771 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 13720 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,787 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 9610 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,789 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 7652 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,822 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 13412 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,822 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 12566 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,822 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 9345 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,838 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2023-07-30 13:46:20,838 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2023-07-30 13:46:20,838 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2023-07-30 13:46:20,838 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2023-07-30 13:46:20,838 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2023-07-30 13:46:20,839 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2023-07-30 13:46:20,840 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2023-07-30 13:46:20,840 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2023-07-30 13:46:20,841 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutting down socket server
2023-07-30 13:46:20,857 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutdown completed
2023-07-30 13:46:20,858 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,858 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,858 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,859 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 7932 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,860 - INFO  [main:Logging@66] - Broker and topic stats closed
2023-07-30 13:46:20,861 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2023-07-30 13:46:20,861 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 6207 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,861 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 6470 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,861 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2023-07-30 13:46:20,861 - INFO  [main:EmbeddedKafkaCluster@198] - Cleaning up kafka log dirs at ArraySeq(/tmp/junit6154326205091447453/junit85516104450203242)
2023-07-30 13:46:20,871 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 14210 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,881 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2023-07-30 13:46:20,882 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:46:20,882 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:46:20,882 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2023-07-30 13:46:20,882 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:46:20,883 - INFO  [main:ZooKeeperServer@558] - shutting down
2023-07-30 13:46:20,883 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2023-07-30 13:46:20,883 - INFO  [main:PrepRequestProcessor@1007] - Shutting down
2023-07-30 13:46:20,883 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2023-07-30 13:46:20,883 - INFO  [ProcessThread(sid:0 cport:41995)::PrepRequestProcessor@155] - PrepRequestProcessor exited loop!
2023-07-30 13:46:20,883 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2023-07-30 13:46:20,884 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete
2023-07-30 13:46:20,887 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 9962 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,889 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 7919 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,894 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:37121/'}
2023-07-30 13:46:20,894 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:46:20,894 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:46:20,909 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@630b6190{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:46:20,909 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:46:20,910 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:46:20,910 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-4, groupId=primary-mm2] Herder stopping
2023-07-30 13:46:20,910 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@655] - [Worker clientId=connect-4, groupId=primary-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:46:20,910 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@1016] - [Worker clientId=connect-4, groupId=primary-mm2] Member connect-4-d9814915-0e67-45fb-aae4-be935085a7e3 sending LeaveGroup request to coordinator localhost:43245 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:46:20,910 - WARN  [DistributedHerder-connect-4-1:AbstractCoordinator@997] - [Worker clientId=connect-4, groupId=primary-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:46:20,911 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,911 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,911 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,911 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-4-d9814915-0e67-45fb-aae4-be935085a7e3] in group primary-mm2 has left, removing it from the group
2023-07-30 13:46:20,911 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 13 (__consumer_offsets-17) (reason: removing member connect-4-d9814915-0e67-45fb-aae4-be935085a7e3 on LeaveGroup)
2023-07-30 13:46:20,911 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.connect for connect-4 unregistered
2023-07-30 13:46:20,911 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:20,912 - INFO  [DistributedHerder-connect-4-1:KafkaProducer@1193] - [Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,912 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,912 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,912 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,912 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.producer for producer-15 unregistered
2023-07-30 13:46:20,913 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,913 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,913 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,914 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-13 unregistered
2023-07-30 13:46:20,914 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:20,914 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:46:20,914 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:20,914 - INFO  [DistributedHerder-connect-4-1:KafkaProducer@1193] - [Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,914 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,914 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,914 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,915 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.producer for producer-16 unregistered
2023-07-30 13:46:20,915 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,915 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,915 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,916 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-14 unregistered
2023-07-30 13:46:20,916 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:20,916 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:46:20,916 - INFO  [DistributedHerder-connect-4-1:Worker@209] - Worker stopping
2023-07-30 13:46:20,916 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:46:20,916 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:20,916 - INFO  [DistributedHerder-connect-4-1:KafkaProducer@1193] - [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,917 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,917 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,917 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,917 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.producer for producer-12 unregistered
2023-07-30 13:46:20,918 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,918 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,918 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,918 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-10 unregistered
2023-07-30 13:46:20,918 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:20,918 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:46:20,918 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,919 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,919 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,919 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.connect for localhost:37121 unregistered
2023-07-30 13:46:20,919 - INFO  [DistributedHerder-connect-4-1:Worker@230] - Worker stopped
2023-07-30 13:46:20,919 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@299] - [Worker clientId=connect-4, groupId=primary-mm2] Herder stopped
2023-07-30 13:46:20,920 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-4, groupId=primary-mm2] Herder stopped
2023-07-30 13:46:20,920 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:46:20,920 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:44923/'}
2023-07-30 13:46:20,920 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:46:20,920 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:46:20,922 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@25d0cb3a{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:46:20,922 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:46:20,922 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 9613 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,922 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 13749 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:46:20,922 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:46:20,922 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-5, groupId=primary-mm2] Herder stopping
2023-07-30 13:46:20,923 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@655] - [Worker clientId=connect-5, groupId=primary-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:46:20,923 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@1016] - [Worker clientId=connect-5, groupId=primary-mm2] Member connect-5-6540e570-7125-4144-8383-1ffd804835f9 sending LeaveGroup request to coordinator localhost:43245 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:46:20,923 - WARN  [DistributedHerder-connect-5-1:AbstractCoordinator@997] - [Worker clientId=connect-5, groupId=primary-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:46:20,923 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,923 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,923 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,923 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-5-6540e570-7125-4144-8383-1ffd804835f9] in group primary-mm2 has left, removing it from the group
2023-07-30 13:46:20,924 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.connect for connect-5 unregistered
2023-07-30 13:46:20,924 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:20,924 - INFO  [DistributedHerder-connect-5-1:KafkaProducer@1193] - [Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,924 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,924 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,925 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,925 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.producer for producer-14 unregistered
2023-07-30 13:46:20,925 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,925 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,925 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,926 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-12 unregistered
2023-07-30 13:46:20,926 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:20,926 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:46:20,926 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:20,926 - INFO  [DistributedHerder-connect-5-1:KafkaProducer@1193] - [Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,927 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,927 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,927 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,927 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.producer for producer-17 unregistered
2023-07-30 13:46:20,927 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,927 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,928 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,928 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-15 unregistered
2023-07-30 13:46:20,928 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:20,928 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:46:20,928 - INFO  [DistributedHerder-connect-5-1:Worker@209] - Worker stopping
2023-07-30 13:46:20,928 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:46:20,929 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:20,929 - INFO  [DistributedHerder-connect-5-1:KafkaProducer@1193] - [Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,930 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,930 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,930 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,930 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.producer for producer-13 unregistered
2023-07-30 13:46:20,930 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,930 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,931 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,931 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-11 unregistered
2023-07-30 13:46:20,931 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:20,931 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:46:20,931 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,931 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,931 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,931 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.connect for localhost:44923 unregistered
2023-07-30 13:46:20,931 - INFO  [DistributedHerder-connect-5-1:Worker@230] - Worker stopped
2023-07-30 13:46:20,932 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@299] - [Worker clientId=connect-5, groupId=primary-mm2] Herder stopped
2023-07-30 13:46:20,932 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-5, groupId=primary-mm2] Herder stopped
2023-07-30 13:46:20,933 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:46:20,933 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:37429/'}
2023-07-30 13:46:20,933 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:46:20,933 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:46:20,934 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@1984212d{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:46:20,934 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:46:20,935 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:46:20,935 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-6, groupId=primary-mm2] Herder stopping
2023-07-30 13:46:20,935 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@655] - [Worker clientId=connect-6, groupId=primary-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:46:20,936 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@1016] - [Worker clientId=connect-6, groupId=primary-mm2] Member connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51 sending LeaveGroup request to coordinator localhost:43245 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:46:20,936 - WARN  [DistributedHerder-connect-6-1:AbstractCoordinator@997] - [Worker clientId=connect-6, groupId=primary-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:46:20,936 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,936 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,936 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-6-a8b455f7-1747-4768-9b5a-f4619121cc51] in group primary-mm2 has left, removing it from the group
2023-07-30 13:46:20,936 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,937 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Group primary-mm2 with generation 14 is now empty (__consumer_offsets-17)
2023-07-30 13:46:20,937 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.connect for connect-6 unregistered
2023-07-30 13:46:20,937 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:20,937 - INFO  [DistributedHerder-connect-6-1:KafkaProducer@1193] - [Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,938 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,938 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,938 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,938 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.producer for producer-19 unregistered
2023-07-30 13:46:20,939 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,939 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,939 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,939 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-17 unregistered
2023-07-30 13:46:20,940 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:46:20,940 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:46:20,940 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:20,940 - INFO  [DistributedHerder-connect-6-1:KafkaProducer@1193] - [Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,940 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,940 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,940 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,941 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.producer for producer-20 unregistered
2023-07-30 13:46:20,941 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,941 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,941 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,943 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-18 unregistered
2023-07-30 13:46:20,943 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:46:20,943 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:46:20,943 - INFO  [DistributedHerder-connect-6-1:Worker@209] - Worker stopping
2023-07-30 13:46:20,943 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:46:20,943 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:20,943 - INFO  [DistributedHerder-connect-6-1:KafkaProducer@1193] - [Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,943 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,944 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,944 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,944 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.producer for producer-18 unregistered
2023-07-30 13:46:20,944 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,944 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,944 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,945 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-16 unregistered
2023-07-30 13:46:20,945 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:46:20,945 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:46:20,945 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,945 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,945 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,945 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.connect for localhost:37429 unregistered
2023-07-30 13:46:20,945 - INFO  [DistributedHerder-connect-6-1:Worker@230] - Worker stopped
2023-07-30 13:46:20,945 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@299] - [Worker clientId=connect-6, groupId=primary-mm2] Herder stopped
2023-07-30 13:46:20,945 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-6, groupId=primary-mm2] Herder stopped
2023-07-30 13:46:20,946 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:46:20,946 - INFO  [main:KafkaProducer@1193] - [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:46:20,947 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:20,947 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:20,947 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:20,947 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-11 unregistered
2023-07-30 13:46:20,947 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2023-07-30 13:46:20,947 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2023-07-30 13:46:20,951 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2023-07-30 13:46:20,952 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2023-07-30 13:46:20,952 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2023-07-30 13:46:20,952 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2023-07-30 13:46:20,953 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopping socket server request processors
2023-07-30 13:46:20,954 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopped socket server request processors
2023-07-30 13:46:20,954 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2023-07-30 13:46:20,954 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2023-07-30 13:46:20,955 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2023-07-30 13:46:21,019 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2023-07-30 13:46:21,019 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2023-07-30 13:46:21,019 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2023-07-30 13:46:21,019 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2023-07-30 13:46:21,054 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:21,054 - WARN  [kafka-admin-client-thread | adminclient-88:NetworkClient@780] - [AdminClient clientId=adminclient-88] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:21,155 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:21,219 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2023-07-30 13:46:21,219 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2023-07-30 13:46:21,220 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2023-07-30 13:46:21,220 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2023-07-30 13:46:21,220 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2023-07-30 13:46:21,220 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2023-07-30 13:46:21,220 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2023-07-30 13:46:21,220 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2023-07-30 13:46:21,221 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2023-07-30 13:46:21,221 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2023-07-30 13:46:21,221 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2023-07-30 13:46:21,226 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2023-07-30 13:46:21,226 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2023-07-30 13:46:21,226 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2023-07-30 13:46:21,255 - WARN  [kafka-admin-client-thread | adminclient-88:NetworkClient@780] - [AdminClient clientId=adminclient-88] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:21,332 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:21,419 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2023-07-30 13:46:21,419 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2023-07-30 13:46:21,419 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2023-07-30 13:46:21,419 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2023-07-30 13:46:21,420 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2023-07-30 13:46:21,420 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2023-07-30 13:46:21,420 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2023-07-30 13:46:21,420 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2023-07-30 13:46:21,420 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2023-07-30 13:46:21,420 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2023-07-30 13:46:21,420 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2023-07-30 13:46:21,420 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2023-07-30 13:46:21,426 - INFO  [SessionTracker:SessionTrackerImpl@163] - SessionTrackerImpl exited loop!
2023-07-30 13:46:21,428 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2023-07-30 13:46:21,428 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2023-07-30 13:46:21,428 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2023-07-30 13:46:21,456 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:21,456 - WARN  [kafka-admin-client-thread | adminclient-88:NetworkClient@780] - [AdminClient clientId=adminclient-88] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:21,619 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2023-07-30 13:46:21,619 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2023-07-30 13:46:21,619 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2023-07-30 13:46:21,819 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2023-07-30 13:46:21,819 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2023-07-30 13:46:21,820 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2023-07-30 13:46:21,857 - WARN  [kafka-admin-client-thread | adminclient-88:NetworkClient@780] - [AdminClient clientId=adminclient-88] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:21,957 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:22,019 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2023-07-30 13:46:22,019 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2023-07-30 13:46:22,022 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2023-07-30 13:46:22,022 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutting down
2023-07-30 13:46:22,022 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Stopped
2023-07-30 13:46:22,022 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:46:22,023 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:46:22,023 - INFO  [main:Logging@66] - Shutting down.
2023-07-30 13:46:22,025 - INFO  [pool-38-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-35] Writing producer snapshot at offset 23
2023-07-30 13:46:22,030 - INFO  [pool-38-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-17] Writing producer snapshot at offset 14
2023-07-30 13:46:22,044 - INFO  [main:Logging@66] - Shutdown complete.
2023-07-30 13:46:22,047 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2023-07-30 13:46:22,047 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2023-07-30 13:46:22,047 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2023-07-30 13:46:22,048 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2023-07-30 13:46:22,149 - INFO  [main:ZooKeeper@1422] - Session: 0x1087f2e484f0000 closed
2023-07-30 13:46:22,149 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x1087f2e484f0000
2023-07-30 13:46:22,149 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2023-07-30 13:46:22,149 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2023-07-30 13:46:22,173 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:22,659 - WARN  [kafka-admin-client-thread | adminclient-88:NetworkClient@780] - [AdminClient clientId=adminclient-88] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:22,673 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2023-07-30 13:46:22,673 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2023-07-30 13:46:22,674 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2023-07-30 13:46:22,960 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:23,260 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:23,673 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2023-07-30 13:46:23,673 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2023-07-30 13:46:23,674 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2023-07-30 13:46:23,687 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2023-07-30 13:46:23,687 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2023-07-30 13:46:23,687 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2023-07-30 13:46:23,762 - WARN  [kafka-admin-client-thread | adminclient-88:NetworkClient@780] - [AdminClient clientId=adminclient-88] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:24,163 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:24,284 - WARN  [kafka-admin-client-thread | adminclient-87:NetworkClient@780] - [AdminClient clientId=adminclient-87] Connection to node 0 (localhost/127.0.0.1:40623) could not be established. Broker may not be available.
2023-07-30 13:46:24,664 - WARN  [kafka-admin-client-thread | adminclient-88:NetworkClient@780] - [AdminClient clientId=adminclient-88] Connection to node 0 (localhost/127.0.0.1:43245) could not be established. Broker may not be available.
2023-07-30 13:46:24,687 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2023-07-30 13:46:24,687 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2023-07-30 13:46:24,687 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutting down socket server
2023-07-30 13:46:24,694 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutdown completed
2023-07-30 13:46:24,695 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:46:24,695 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:46:24,695 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:46:24,695 - INFO  [main:Logging@66] - Broker and topic stats closed
2023-07-30 13:46:24,695 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2023-07-30 13:46:24,695 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2023-07-30 13:46:24,695 - INFO  [main:EmbeddedKafkaCluster@198] - Cleaning up kafka log dirs at ArraySeq(/tmp/junit5301514646100141969/junit2114663958254445386)
2023-07-30 13:46:24,705 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2023-07-30 13:46:24,705 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:46:24,705 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:46:24,706 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:46:24,706 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2023-07-30 13:46:24,706 - INFO  [main:ZooKeeperServer@558] - shutting down
2023-07-30 13:46:24,706 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2023-07-30 13:46:24,706 - INFO  [main:PrepRequestProcessor@1007] - Shutting down
2023-07-30 13:46:24,707 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2023-07-30 13:46:24,707 - INFO  [ProcessThread(sid:0 cport:33443)::PrepRequestProcessor@155] - PrepRequestProcessor exited loop!
2023-07-30 13:46:24,707 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2023-07-30 13:46:24,707 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete

Thanks for using JUnit! Support its development at https://junit.org/sponsoring

[36m[0m
[36m[0m [36mJUnit Jupiter[0m [32m[0m
[36m[0m [36mJUnit Vintage[0m [32m[0m
[36m   [0m [36mMirrorConnectorsIntegrationTest[0m [32m[0m
[36m      [0m [34mtestReplicationWithEmptyPartition[0m [36m27628 ms[0m [32m[0m

Test run finished after 27675 ms
[         3 containers found      ]
[         0 containers skipped    ]
[         3 containers started    ]
[         0 containers aborted    ]
[         3 containers successful ]
[         0 containers failed     ]
[         1 tests found           ]
[         0 tests skipped         ]
[         1 tests started         ]
[         0 tests aborted         ]
[         1 tests successful      ]
[         0 tests failed          ]

