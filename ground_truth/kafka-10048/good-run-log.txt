2023-07-30 13:48:14,943 - INFO  [main:AbstractConfig@361] - MirrorMakerConfig values: 
	clusters = [primary, backup]
	config.providers = []
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:15,177 - INFO  [main:Reflections@239] - Reflections took 205 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:48:15,184 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:48:15,191 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:48:15,193 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:48:15,203 - WARN  [main:AppInfoParser@46] - Error while loading kafka-version.properties: null
2023-07-30 13:48:15,216 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:48:15,216 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:15,216 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:15,216 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:15,217 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:15,217 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:15,221 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:15,221 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:15,221 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:15,221 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:15,222 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:15,329 - INFO  [main:Log4jControllerRegistration$@31] - Registered kafka:type=kafka.Log4jController MBean
2023-07-30 13:48:15,390 - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
2023-07-30 13:48:15,390 - INFO  [main:Environment@109] - Server environment:host.name=razor15
2023-07-30 13:48:15,390 - INFO  [main:Environment@109] - Server environment:java.version=1.8.0_275
2023-07-30 13:48:15,390 - INFO  [main:Environment@109] - Server environment:java.vendor=Private Build
2023-07-30 13:48:15,390 - INFO  [main:Environment@109] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2023-07-30 13:48:15,391 - INFO  [main:Environment@109] - Server environment:java.class.path=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048/junit-platform-console-standalone-1.7.0.jar
2023-07-30 13:48:15,391 - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2023-07-30 13:48:15,391 - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2023-07-30 13:48:15,391 - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2023-07-30 13:48:15,391 - INFO  [main:Environment@109] - Server environment:os.name=Linux
2023-07-30 13:48:15,392 - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2023-07-30 13:48:15,392 - INFO  [main:Environment@109] - Server environment:os.version=4.15.0-128-generic
2023-07-30 13:48:15,392 - INFO  [main:Environment@109] - Server environment:user.name=tonypan
2023-07-30 13:48:15,392 - INFO  [main:Environment@109] - Server environment:user.home=/home/tonypan
2023-07-30 13:48:15,392 - INFO  [main:Environment@109] - Server environment:user.dir=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048
2023-07-30 13:48:15,392 - INFO  [main:Environment@109] - Server environment:os.memory.free=372MB
2023-07-30 13:48:15,392 - INFO  [main:Environment@109] - Server environment:os.memory.max=7051MB
2023-07-30 13:48:15,392 - INFO  [main:Environment@109] - Server environment:os.memory.total=475MB
2023-07-30 13:48:15,396 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2023-07-30 13:48:15,415 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2023-07-30 13:48:15,417 - INFO  [main:ZooKeeperServer@938] - minSessionTimeout set to 1600
2023-07-30 13:48:15,417 - INFO  [main:ZooKeeperServer@947] - maxSessionTimeout set to 16000
2023-07-30 13:48:15,418 - INFO  [main:ZooKeeperServer@166] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-5685440562087857328/version-2 snapdir /tmp/kafka-6610587641007278312/version-2
2023-07-30 13:48:15,428 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2023-07-30 13:48:15,433 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2023-07-30 13:48:15,442 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-6610587641007278312/version-2/snapshot.0
2023-07-30 13:48:15,445 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-6610587641007278312/version-2/snapshot.0
2023-07-30 13:48:15,802 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit1823696247009190315/junit2950113589444295251
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:37775
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:48:15,820 - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2023-07-30 13:48:15,885 - INFO  [main:Logging@66] - starting
2023-07-30 13:48:15,886 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:37775
2023-07-30 13:48:15,913 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:37775.
2023-07-30 13:48:15,919 - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
2023-07-30 13:48:15,919 - INFO  [main:Environment@109] - Client environment:host.name=razor15
2023-07-30 13:48:15,919 - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_275
2023-07-30 13:48:15,920 - INFO  [main:Environment@109] - Client environment:java.vendor=Private Build
2023-07-30 13:48:15,920 - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2023-07-30 13:48:15,920 - INFO  [main:Environment@109] - Client environment:java.class.path=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048/junit-platform-console-standalone-1.7.0.jar
2023-07-30 13:48:15,920 - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2023-07-30 13:48:15,920 - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2023-07-30 13:48:15,920 - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2023-07-30 13:48:15,920 - INFO  [main:Environment@109] - Client environment:os.name=Linux
2023-07-30 13:48:15,921 - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2023-07-30 13:48:15,921 - INFO  [main:Environment@109] - Client environment:os.version=4.15.0-128-generic
2023-07-30 13:48:15,921 - INFO  [main:Environment@109] - Client environment:user.name=tonypan
2023-07-30 13:48:15,921 - INFO  [main:Environment@109] - Client environment:user.home=/home/tonypan
2023-07-30 13:48:15,921 - INFO  [main:Environment@109] - Client environment:user.dir=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048
2023-07-30 13:48:15,921 - INFO  [main:Environment@109] - Client environment:os.memory.free=216MB
2023-07-30 13:48:15,921 - INFO  [main:Environment@109] - Client environment:os.memory.max=7051MB
2023-07-30 13:48:15,921 - INFO  [main:Environment@109] - Client environment:os.memory.total=330MB
2023-07-30 13:48:15,925 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:37775 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3b96c42e
2023-07-30 13:48:15,929 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2023-07-30 13:48:15,937 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2023-07-30 13:48:15,939 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2023-07-30 13:48:15,944 - INFO  [main-SendThread(127.0.0.1:37775):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:37775. Will not attempt to authenticate using SASL (unknown error)
2023-07-30 13:48:15,946 - INFO  [main-SendThread(127.0.0.1:37775):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:33064, server: localhost/127.0.0.1:37775
2023-07-30 13:48:15,956 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2023-07-30 13:48:15,969 - INFO  [main-SendThread(127.0.0.1:37775):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:37775, sessionid = 0x1087f304f460000, negotiated timeout = 16000
2023-07-30 13:48:15,973 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2023-07-30 13:48:16,061 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2023-07-30 13:48:16,072 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2023-07-30 13:48:16,072 - INFO  [feature-zk-node-event-process-thread:FinalizedFeatureCache$@42] - Cleared cache
2023-07-30 13:48:16,333 - INFO  [main:Logging@66] - Cluster ID = WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:16,337 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit1823696247009190315/junit2950113589444295251/meta.properties
2023-07-30 13:48:16,393 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit1823696247009190315/junit2950113589444295251
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:37775
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:48:16,403 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit1823696247009190315/junit2950113589444295251
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:37775
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:48:16,430 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2023-07-30 13:48:16,430 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2023-07-30 13:48:16,431 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2023-07-30 13:48:16,433 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2023-07-30 13:48:16,465 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit1823696247009190315/junit2950113589444295251)
2023-07-30 13:48:16,467 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit1823696247009190315/junit2950113589444295251 since no clean shutdown file was found
2023-07-30 13:48:16,473 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2023-07-30 13:48:16,488 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2023-07-30 13:48:16,492 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2023-07-30 13:48:17,317 - INFO  [main:Logging@66] - Created ConnectionAcceptRate sensor, quotaLimit=2147483647
2023-07-30 13:48:17,322 - INFO  [main:Logging@66] - Created ConnectionAcceptRate-PLAINTEXT sensor, quotaLimit=2147483647
2023-07-30 13:48:17,328 - INFO  [main:Logging@66] - Updated PLAINTEXT max connection creation rate to 2147483647
2023-07-30 13:48:17,332 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:41091.
2023-07-30 13:48:17,383 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:48:17,422 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2023-07-30 13:48:17,422 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2023-07-30 13:48:17,423 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2023-07-30 13:48:17,423 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2023-07-30 13:48:17,440 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2023-07-30 13:48:17,440 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Starting
2023-07-30 13:48:17,493 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2023-07-30 13:48:17,520 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 24,24,1690739297513,1690739297513,1,0,0,74449239316758528,204,0,24

2023-07-30 13:48:17,521 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:41091, czxid (broker epoch): 24
2023-07-30 13:48:17,612 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2023-07-30 13:48:17,618 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2023-07-30 13:48:17,619 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2023-07-30 13:48:17,622 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2023-07-30 13:48:17,653 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2023-07-30 13:48:17,654 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2023-07-30 13:48:17,677 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2023-07-30 13:48:17,697 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2023-07-30 13:48:17,699 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2023-07-30 13:48:17,699 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2023-07-30 13:48:17,737 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2023-07-30 13:48:17,770 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2023-07-30 13:48:17,779 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Starting socket server acceptors and processors
2023-07-30 13:48:17,785 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:48:17,786 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started socket server acceptors and processors
2023-07-30 13:48:17,787 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:17,787 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:17,787 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739295238
2023-07-30 13:48:17,789 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2023-07-30 13:48:17,800 - INFO  [main:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:17,821 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:17,821 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:17,821 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739297821
2023-07-30 13:48:17,822 - INFO  [main:EmbeddedConnectCluster@235] - Starting Connect cluster 'primary-connect-cluster' with 3 workers
2023-07-30 13:48:17,827 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:48:17,842 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Recorded new controller, from now on will use broker 0
2023-07-30 13:48:17,849 - INFO  [kafka-producer-network-thread | producer-1:Metadata@279] - [Producer clientId=producer-1] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:17,994 - INFO  [main:Reflections@239] - Reflections took 165 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:48:17,997 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:18,002 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:18,005 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:18,013 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:48:18,014 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:18,014 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:18,014 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:18,014 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:18,014 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:18,016 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:18,016 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:18,016 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:18,016 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:18,016 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:18,066 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.backup.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = backup-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.backup.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.backup.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:48:18,067 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:48:18,071 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:18,073 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:18,103 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,104 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:18,104 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,104 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:18,104 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,104 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,104 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:18,104 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,105 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,105 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,105 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,105 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,105 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,105 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,105 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298105
2023-07-30 13:48:18,122 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:18,123 - INFO  [kafka-admin-client-thread | adminclient-1:AppInfoParser@83] - App info kafka.admin.client for adminclient-1 unregistered
2023-07-30 13:48:18,129 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:18,129 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:18,129 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:18,191 - INFO  [main:Log@169] - Logging initialized @3944ms to org.eclipse.jetty.util.log.Slf4jLog
2023-07-30 13:48:18,324 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:48:18,325 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:48:18,343 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:48:18,425 - INFO  [main:AbstractConnector@331] - Started http_localhost0@3113a37{HTTP/1.1, (http/1.1)}{localhost:41979}
2023-07-30 13:48:18,426 - INFO  [main:Server@400] - Started @4180ms
2023-07-30 13:48:18,488 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:41979/
2023-07-30 13:48:18,489 - INFO  [main:RestServer@219] - REST server listening at http://localhost:41979/, advertising URL http://localhost:41979/
2023-07-30 13:48:18,489 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:41979/
2023-07-30 13:48:18,489 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:41979/
2023-07-30 13:48:18,489 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:41979/
2023-07-30 13:48:18,491 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:18,492 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:18,494 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,494 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:18,494 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,494 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:18,495 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,495 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,495 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:18,495 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,495 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,495 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,495 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,495 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,496 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,496 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,496 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298496
2023-07-30 13:48:18,520 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:18,521 - INFO  [kafka-admin-client-thread | adminclient-2:AppInfoParser@83] - App info kafka.admin.client for adminclient-2 unregistered
2023-07-30 13:48:18,524 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:18,524 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:18,524 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:18,530 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:48:18,540 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:18,541 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:18,543 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,543 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:18,543 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,543 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:18,543 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,543 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,544 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:18,544 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,544 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,544 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,544 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,544 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,544 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,544 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,545 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298544
2023-07-30 13:48:18,557 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:18,558 - INFO  [kafka-admin-client-thread | adminclient-3:AppInfoParser@83] - App info kafka.admin.client for adminclient-3 unregistered
2023-07-30 13:48:18,560 - INFO  [kafka-admin-client-thread | adminclient-3:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:18,560 - INFO  [kafka-admin-client-thread | adminclient-3:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:18,560 - INFO  [kafka-admin-client-thread | adminclient-3:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:18,563 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,563 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,564 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298563
2023-07-30 13:48:18,581 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:18,583 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:18,583 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:18,584 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:18,585 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,586 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:18,586 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,586 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:18,586 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,586 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,586 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:18,586 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,586 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,586 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,587 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,587 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,587 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,587 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,587 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298587
2023-07-30 13:48:18,603 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:18,604 - INFO  [kafka-admin-client-thread | adminclient-4:AppInfoParser@83] - App info kafka.admin.client for adminclient-4 unregistered
2023-07-30 13:48:18,605 - INFO  [kafka-admin-client-thread | adminclient-4:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:18,605 - INFO  [kafka-admin-client-thread | adminclient-4:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:18,605 - INFO  [kafka-admin-client-thread | adminclient-4:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:18,613 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:18,613 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:18,615 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,615 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:18,615 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,615 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:18,615 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,615 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,615 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:18,615 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,616 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,616 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,616 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,616 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,616 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,616 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,616 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298616
2023-07-30 13:48:18,627 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:18,628 - INFO  [kafka-admin-client-thread | adminclient-5:AppInfoParser@83] - App info kafka.admin.client for adminclient-5 unregistered
2023-07-30 13:48:18,630 - INFO  [kafka-admin-client-thread | adminclient-5:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:18,630 - INFO  [kafka-admin-client-thread | adminclient-5:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:18,630 - INFO  [kafka-admin-client-thread | adminclient-5:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:18,633 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:18,634 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:18,635 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,636 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:18,636 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,636 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:18,636 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,636 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,636 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:18,636 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,636 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,636 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,637 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,637 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,637 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,637 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,637 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298637
2023-07-30 13:48:18,647 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:18,648 - INFO  [kafka-admin-client-thread | adminclient-6:AppInfoParser@83] - App info kafka.admin.client for adminclient-6 unregistered
2023-07-30 13:48:18,650 - INFO  [kafka-admin-client-thread | adminclient-6:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:18,650 - INFO  [kafka-admin-client-thread | adminclient-6:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:18,650 - INFO  [kafka-admin-client-thread | adminclient-6:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:18,670 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:18,670 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:18,673 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,673 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:18,673 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,673 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:18,673 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,673 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,673 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:18,673 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,674 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,674 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,674 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,674 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,674 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,674 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,674 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298674
2023-07-30 13:48:18,686 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:18,687 - INFO  [kafka-admin-client-thread | adminclient-7:AppInfoParser@83] - App info kafka.admin.client for adminclient-7 unregistered
2023-07-30 13:48:18,688 - INFO  [kafka-admin-client-thread | adminclient-7:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:18,688 - INFO  [kafka-admin-client-thread | adminclient-7:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:18,688 - INFO  [kafka-admin-client-thread | adminclient-7:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:18,709 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,710 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,710 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298709
2023-07-30 13:48:18,713 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 886ms
2023-07-30 13:48:18,713 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:48:18,714 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:48:18,714 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@286] - [Worker clientId=connect-1, groupId=backup-mm2] Herder starting
2023-07-30 13:48:18,714 - INFO  [DistributedHerder-connect-1-1:Worker@195] - Worker starting
2023-07-30 13:48:18,714 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:48:18,714 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.backup.internal
2023-07-30 13:48:18,715 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:18,716 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,716 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:18,716 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:18,716 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,716 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:18,717 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:18,717 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:18,717 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739298717
2023-07-30 13:48:18,761 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic mm2-offsets.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0))
2023-07-30 13:48:18,792 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:48:18,906 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-21, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-13)
2023-07-30 13:48:18,936 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:48:18,937 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:48:18,939 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 660000ms
2023-07-30 13:48:18,999 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-16, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,014 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-16 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,015 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-16 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-16
2023-07-30 13:48:19,016 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-16 broker=0] Log loaded for partition mm2-offsets.backup.internal-16 with initial high watermark 0
2023-07-30 13:48:19,032 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-12, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,033 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-12 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,033 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-12 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-12
2023-07-30 13:48:19,033 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-12 broker=0] Log loaded for partition mm2-offsets.backup.internal-12 with initial high watermark 0
2023-07-30 13:48:19,038 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-23, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,039 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-23 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,039 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-23 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-23
2023-07-30 13:48:19,039 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-23 broker=0] Log loaded for partition mm2-offsets.backup.internal-23 with initial high watermark 0
2023-07-30 13:48:19,046 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-8, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,048 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-8 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,048 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-8 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-8
2023-07-30 13:48:19,048 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-8 broker=0] Log loaded for partition mm2-offsets.backup.internal-8 with initial high watermark 0
2023-07-30 13:48:19,055 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-19, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,057 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-19 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,057 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-19 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-19
2023-07-30 13:48:19,057 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-19 broker=0] Log loaded for partition mm2-offsets.backup.internal-19 with initial high watermark 0
2023-07-30 13:48:19,063 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-4, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,065 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-4 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,065 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-4
2023-07-30 13:48:19,065 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-4 broker=0] Log loaded for partition mm2-offsets.backup.internal-4 with initial high watermark 0
2023-07-30 13:48:19,071 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-13, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,072 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-13 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,073 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-13 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-13
2023-07-30 13:48:19,073 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-13 broker=0] Log loaded for partition mm2-offsets.backup.internal-13 with initial high watermark 0
2023-07-30 13:48:19,080 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-9, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,081 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-9 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,081 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-9 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-9
2023-07-30 13:48:19,081 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-9 broker=0] Log loaded for partition mm2-offsets.backup.internal-9 with initial high watermark 0
2023-07-30 13:48:19,088 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-24, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,089 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-24 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,089 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-24 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-24
2023-07-30 13:48:19,090 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-24 broker=0] Log loaded for partition mm2-offsets.backup.internal-24 with initial high watermark 0
2023-07-30 13:48:19,096 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-5, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,097 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-5 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,098 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-5 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-5
2023-07-30 13:48:19,098 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-5 broker=0] Log loaded for partition mm2-offsets.backup.internal-5 with initial high watermark 0
2023-07-30 13:48:19,105 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-20, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,106 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-20 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,106 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-20 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-20
2023-07-30 13:48:19,106 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-20 broker=0] Log loaded for partition mm2-offsets.backup.internal-20 with initial high watermark 0
2023-07-30 13:48:19,113 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-1, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,114 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-1 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,114 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-1
2023-07-30 13:48:19,114 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-1 broker=0] Log loaded for partition mm2-offsets.backup.internal-1 with initial high watermark 0
2023-07-30 13:48:19,122 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-14, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,123 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-14 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,124 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-14 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-14
2023-07-30 13:48:19,124 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-14 broker=0] Log loaded for partition mm2-offsets.backup.internal-14 with initial high watermark 0
2023-07-30 13:48:19,130 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-10, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,131 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-10 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,131 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-10 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-10
2023-07-30 13:48:19,131 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-10 broker=0] Log loaded for partition mm2-offsets.backup.internal-10 with initial high watermark 0
2023-07-30 13:48:19,138 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-21, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,140 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-21 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,140 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-21 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-21
2023-07-30 13:48:19,140 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-21 broker=0] Log loaded for partition mm2-offsets.backup.internal-21 with initial high watermark 0
2023-07-30 13:48:19,147 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-6, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,148 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-6 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,149 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-6 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-6
2023-07-30 13:48:19,149 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-6 broker=0] Log loaded for partition mm2-offsets.backup.internal-6 with initial high watermark 0
2023-07-30 13:48:19,155 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-17, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,156 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-17 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,156 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-17 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-17
2023-07-30 13:48:19,156 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-17 broker=0] Log loaded for partition mm2-offsets.backup.internal-17 with initial high watermark 0
2023-07-30 13:48:19,163 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-2, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,165 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-2 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,165 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-2
2023-07-30 13:48:19,165 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-2 broker=0] Log loaded for partition mm2-offsets.backup.internal-2 with initial high watermark 0
2023-07-30 13:48:19,172 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-15, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,173 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-15 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,173 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-15 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-15
2023-07-30 13:48:19,173 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-15 broker=0] Log loaded for partition mm2-offsets.backup.internal-15 with initial high watermark 0
2023-07-30 13:48:19,181 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-0, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,182 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-0 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,182 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-0
2023-07-30 13:48:19,182 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-0 broker=0] Log loaded for partition mm2-offsets.backup.internal-0 with initial high watermark 0
2023-07-30 13:48:19,189 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-11, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,191 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-11 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,191 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-11 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-11
2023-07-30 13:48:19,191 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-11 broker=0] Log loaded for partition mm2-offsets.backup.internal-11 with initial high watermark 0
2023-07-30 13:48:19,197 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-7, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,198 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-7 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,199 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-7 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-7
2023-07-30 13:48:19,199 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-7 broker=0] Log loaded for partition mm2-offsets.backup.internal-7 with initial high watermark 0
2023-07-30 13:48:19,206 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-22, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,207 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-22 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,208 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-22 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-22
2023-07-30 13:48:19,208 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-22 broker=0] Log loaded for partition mm2-offsets.backup.internal-22 with initial high watermark 0
2023-07-30 13:48:19,214 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-3, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,215 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-3 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,215 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-3
2023-07-30 13:48:19,215 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-3 broker=0] Log loaded for partition mm2-offsets.backup.internal-3 with initial high watermark 0
2023-07-30 13:48:19,222 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-offsets.backup.internal-18, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,223 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-offsets.backup.internal-18 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offsets.backup.internal-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,223 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-18 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-18
2023-07-30 13:48:19,224 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-offsets.backup.internal-18 broker=0] Log loaded for partition mm2-offsets.backup.internal-18 with initial high watermark 0
2023-07-30 13:48:19,277 - INFO  [DistributedHerder-connect-1-1:TopicAdmin@284] - Created topic (name=mm2-offsets.backup.internal, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:41091
2023-07-30 13:48:19,279 - INFO  [kafka-admin-client-thread | adminclient-8:AppInfoParser@83] - App info kafka.admin.client for adminclient-8 unregistered
2023-07-30 13:48:19,280 - INFO  [kafka-admin-client-thread | adminclient-8:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:19,280 - INFO  [kafka-admin-client-thread | adminclient-8:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:19,280 - INFO  [kafka-admin-client-thread | adminclient-8:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:19,281 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:19,283 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,284 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,284 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,284 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,284 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:19,284 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,284 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,284 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:19,284 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:19,284 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,285 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,285 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,285 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,285 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,285 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:19,285 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:19,285 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739299285
2023-07-30 13:48:19,288 - INFO  [kafka-producer-network-thread | producer-2:Metadata@279] - [Producer clientId=producer-2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:19,295 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:19,321 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,321 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,321 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,321 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:19,321 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,322 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,322 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:19,322 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:19,322 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,322 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,322 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,322 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,322 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,322 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:19,322 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:19,322 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739299322
2023-07-30 13:48:19,329 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:19,350 - INFO  [DistributedHerder-connect-1-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Subscribed to partition(s): mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-21
2023-07-30 13:48:19,353 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-0
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-5
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-10
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-20
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-15
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-9
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-11
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-16
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-4
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-17
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-3
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-24
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-23
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-13
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-18
2023-07-30 13:48:19,354 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-22
2023-07-30 13:48:19,355 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-2
2023-07-30 13:48:19,355 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-8
2023-07-30 13:48:19,355 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-12
2023-07-30 13:48:19,355 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-19
2023-07-30 13:48:19,355 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-14
2023-07-30 13:48:19,355 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-1
2023-07-30 13:48:19,355 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-6
2023-07-30 13:48:19,355 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-7
2023-07-30 13:48:19,355 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-21
2023-07-30 13:48:19,389 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,390 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,390 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,390 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,390 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,390 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,391 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,391 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,391 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,391 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,391 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,391 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,391 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,391 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,391 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,392 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,393 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:19,393 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:19,393 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:48:19,396 - INFO  [DistributedHerder-connect-1-1:Worker@202] - Worker started
2023-07-30 13:48:19,396 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.backup.internal
2023-07-30 13:48:19,397 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,398 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,399 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:19,399 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:19,399 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739299399
2023-07-30 13:48:19,413 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Creating topic mm2-status.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2023-07-30 13:48:19,437 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-status.backup.internal-0, mm2-status.backup.internal-3, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2)
2023-07-30 13:48:19,441 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.backup.internal-4, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,443 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.backup.internal-4 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-status.backup.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,444 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-4
2023-07-30 13:48:19,445 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-4 broker=0] Log loaded for partition mm2-status.backup.internal-4 with initial high watermark 0
2023-07-30 13:48:19,449 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.backup.internal-3, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,450 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.backup.internal-3 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-status.backup.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,450 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-3
2023-07-30 13:48:19,451 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-3 broker=0] Log loaded for partition mm2-status.backup.internal-3 with initial high watermark 0
2023-07-30 13:48:19,458 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.backup.internal-2, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,460 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.backup.internal-2 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-status.backup.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,460 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-2
2023-07-30 13:48:19,460 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-2 broker=0] Log loaded for partition mm2-status.backup.internal-2 with initial high watermark 0
2023-07-30 13:48:19,468 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.backup.internal-1, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,470 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.backup.internal-1 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-status.backup.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,470 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-1
2023-07-30 13:48:19,470 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-1 broker=0] Log loaded for partition mm2-status.backup.internal-1 with initial high watermark 0
2023-07-30 13:48:19,475 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.backup.internal-0, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,476 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.backup.internal-0 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-status.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,476 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-0
2023-07-30 13:48:19,476 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.backup.internal-0 broker=0] Log loaded for partition mm2-status.backup.internal-0 with initial high watermark 0
2023-07-30 13:48:19,486 - INFO  [DistributedHerder-connect-1-1:TopicAdmin@284] - Created topic (name=mm2-status.backup.internal, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:41091
2023-07-30 13:48:19,487 - INFO  [kafka-admin-client-thread | adminclient-9:AppInfoParser@83] - App info kafka.admin.client for adminclient-9 unregistered
2023-07-30 13:48:19,489 - INFO  [kafka-admin-client-thread | adminclient-9:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:19,489 - INFO  [kafka-admin-client-thread | adminclient-9:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:19,489 - INFO  [kafka-admin-client-thread | adminclient-9:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:19,489 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:19,492 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,492 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,492 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,492 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,492 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:19,492 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,492 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,492 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:19,493 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:19,493 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,493 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,493 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,493 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,493 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,493 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:19,493 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:19,493 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739299493
2023-07-30 13:48:19,494 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:19,497 - INFO  [kafka-producer-network-thread | producer-3:Metadata@279] - [Producer clientId=producer-3] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:19,498 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,498 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,499 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:19,499 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:19,499 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739299499
2023-07-30 13:48:19,504 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:19,511 - INFO  [DistributedHerder-connect-1-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Subscribed to partition(s): mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2, mm2-status.backup.internal-3
2023-07-30 13:48:19,512 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-0
2023-07-30 13:48:19,512 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-4
2023-07-30 13:48:19,512 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-1
2023-07-30 13:48:19,512 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-2
2023-07-30 13:48:19,512 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-3
2023-07-30 13:48:19,520 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,521 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,521 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,521 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,521 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,521 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:19,521 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:19,525 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:48:19,525 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.backup.internal
2023-07-30 13:48:19,525 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:19,527 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,527 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,527 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,527 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,527 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:19,527 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,528 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,528 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:19,528 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:19,528 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,528 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,528 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,528 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,528 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,528 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:19,528 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:19,528 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739299528
2023-07-30 13:48:19,540 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic mm2-configs.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:48:19,559 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:48:19,562 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-configs.backup.internal-0, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,563 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-configs.backup.internal-0 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-configs.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,564 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-configs.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-configs.backup.internal-0
2023-07-30 13:48:19,564 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-configs.backup.internal-0 broker=0] Log loaded for partition mm2-configs.backup.internal-0 with initial high watermark 0
2023-07-30 13:48:19,572 - INFO  [DistributedHerder-connect-1-1:TopicAdmin@284] - Created topic (name=mm2-configs.backup.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:41091
2023-07-30 13:48:19,573 - INFO  [kafka-admin-client-thread | adminclient-10:AppInfoParser@83] - App info kafka.admin.client for adminclient-10 unregistered
2023-07-30 13:48:19,574 - INFO  [kafka-admin-client-thread | adminclient-10:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:19,575 - INFO  [kafka-admin-client-thread | adminclient-10:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:19,575 - INFO  [kafka-admin-client-thread | adminclient-10:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:19,575 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:19,577 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,578 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,578 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,578 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,578 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:19,578 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,578 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,578 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:19,578 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:19,579 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,579 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,579 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,579 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,579 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,579 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:19,579 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:19,579 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739299579
2023-07-30 13:48:19,580 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:19,582 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,582 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:19,582 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,582 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:19,582 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,582 - INFO  [kafka-producer-network-thread | producer-4:Metadata@279] - [Producer clientId=producer-4] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:19,582 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,583 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:19,584 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:19,584 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,584 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:19,584 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:19,584 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,584 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:19,584 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:19,584 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:19,584 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739299584
2023-07-30 13:48:19,588 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:19,594 - INFO  [DistributedHerder-connect-1-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Subscribed to partition(s): mm2-configs.backup.internal-0
2023-07-30 13:48:19,594 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-configs.backup.internal-0
2023-07-30 13:48:19,601 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Resetting offset for partition mm2-configs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:19,602 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:19,602 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:19,603 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:48:19,603 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@290] - [Worker clientId=connect-1, groupId=backup-mm2] Herder started
2023-07-30 13:48:19,613 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Worker clientId=connect-1, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:19,625 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0))
2023-07-30 13:48:19,630 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
Jul 30, 2023 1:48:19 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:48:19 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Jul 30, 2023 1:48:19 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:48:19 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:48:19,700 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40)
2023-07-30 13:48:19,706 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,708 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,709 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2023-07-30 13:48:19,709 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2023-07-30 13:48:19,715 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-18, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,717 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-18 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,717 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18
2023-07-30 13:48:19,717 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0
2023-07-30 13:48:19,723 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-41, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,725 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-41 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,725 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41
2023-07-30 13:48:19,725 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0
2023-07-30 13:48:19,733 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-10, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,734 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-10 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,735 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10
2023-07-30 13:48:19,736 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0
2023-07-30 13:48:19,741 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-33, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,742 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-33 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,743 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33
2023-07-30 13:48:19,743 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0
2023-07-30 13:48:19,750 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-48, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,752 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-48 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,752 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48
2023-07-30 13:48:19,752 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0
2023-07-30 13:48:19,758 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-19, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,760 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-19 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,760 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19
2023-07-30 13:48:19,760 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0
2023-07-30 13:48:19,767 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-34, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,768 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-34 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,769 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34
2023-07-30 13:48:19,769 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0
2023-07-30 13:48:19,775 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,776 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,777 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2023-07-30 13:48:19,777 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2023-07-30 13:48:19,783 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-11, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,784 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-11 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,785 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11
2023-07-30 13:48:19,785 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0
2023-07-30 13:48:19,792 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-26, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,958 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-26 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,959 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26
2023-07-30 13:48:19,959 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0
2023-07-30 13:48:19,964 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-49, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,967 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-49 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,967 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49
2023-07-30 13:48:19,967 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0
2023-07-30 13:48:19,972 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-39, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,973 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-39 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,973 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39
2023-07-30 13:48:19,973 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0
2023-07-30 13:48:19,980 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-9, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,981 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-9 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,982 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9
2023-07-30 13:48:19,982 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0
2023-07-30 13:48:19,988 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-24, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,989 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-24 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,990 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24
2023-07-30 13:48:19,990 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0
2023-07-30 13:48:19,997 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-31, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:19,998 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-31 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:19,998 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31
2023-07-30 13:48:19,998 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0
2023-07-30 13:48:20,006 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-46, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,007 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-46 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,007 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46
2023-07-30 13:48:20,007 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0
2023-07-30 13:48:20,017 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,018 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,018 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2023-07-30 13:48:20,018 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2023-07-30 13:48:20,022 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-16, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,023 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-16 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,023 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16
2023-07-30 13:48:20,023 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0
2023-07-30 13:48:20,031 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,032 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,032 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2023-07-30 13:48:20,032 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2023-07-30 13:48:20,039 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-25, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,040 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-25 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,040 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25
2023-07-30 13:48:20,040 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0
2023-07-30 13:48:20,048 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-40, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,049 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-40 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,049 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40
2023-07-30 13:48:20,050 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0
2023-07-30 13:48:20,056 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-47, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,057 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-47 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,057 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47
2023-07-30 13:48:20,057 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0
2023-07-30 13:48:20,064 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-17, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,065 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-17 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,066 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17
2023-07-30 13:48:20,066 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0
2023-07-30 13:48:20,073 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-32, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,074 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-32 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,074 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32
2023-07-30 13:48:20,074 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0
2023-07-30 13:48:20,081 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-37, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,082 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-37 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,082 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37
2023-07-30 13:48:20,082 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:48:20,090 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-7, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,091 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-7 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,091 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7
2023-07-30 13:48:20,091 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0
2023-07-30 13:48:20,092 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@2424686b{/,null,AVAILABLE}
2023-07-30 13:48:20,094 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:48:20,094 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:48:20,095 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:41979/'}
2023-07-30 13:48:20,095 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:48:20,097 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-22, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,098 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-22 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,099 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22
2023-07-30 13:48:20,099 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0
2023-07-30 13:48:20,107 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-29, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,109 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-29 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,109 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29
2023-07-30 13:48:20,109 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0
2023-07-30 13:48:20,114 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-44, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,115 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-44 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,115 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44
2023-07-30 13:48:20,115 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0
2023-07-30 13:48:20,122 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-14, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,123 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-14 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,123 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14
2023-07-30 13:48:20,123 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0
2023-07-30 13:48:20,131 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-23, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,132 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-23 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,133 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23
2023-07-30 13:48:20,133 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0
2023-07-30 13:48:20,139 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-38, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,140 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-38 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,140 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38
2023-07-30 13:48:20,140 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0
2023-07-30 13:48:20,148 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-8, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,149 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-8 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,149 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8
2023-07-30 13:48:20,149 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0
2023-07-30 13:48:20,157 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-45, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,158 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-45 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,158 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45
2023-07-30 13:48:20,158 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0
2023-07-30 13:48:20,165 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-15, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,167 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-15 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,167 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15
2023-07-30 13:48:20,167 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0
2023-07-30 13:48:20,173 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-30, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,174 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-30 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,174 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30
2023-07-30 13:48:20,175 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0
2023-07-30 13:48:20,182 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,183 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,183 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2023-07-30 13:48:20,183 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2023-07-30 13:48:20,190 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-35, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,191 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-35 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,191 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35
2023-07-30 13:48:20,191 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0
2023-07-30 13:48:20,198 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-5, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,199 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-5 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,199 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5
2023-07-30 13:48:20,199 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0
2023-07-30 13:48:20,207 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-20, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,208 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-20 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,208 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20
2023-07-30 13:48:20,208 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0
2023-07-30 13:48:20,215 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-27, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,217 - INFO  [main:Reflections@239] - Reflections took 120 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:48:20,217 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-27 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,218 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27
2023-07-30 13:48:20,218 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0
2023-07-30 13:48:20,220 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:20,224 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:20,224 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-42, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,227 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-42 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,227 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:20,228 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42
2023-07-30 13:48:20,229 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0
2023-07-30 13:48:20,233 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:48:20,233 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-12, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,233 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:20,234 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:20,234 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,234 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,234 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,234 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-12 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,234 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12
2023-07-30 13:48:20,234 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0
2023-07-30 13:48:20,235 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:20,235 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:20,235 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,235 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,236 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,236 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.backup.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = backup-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.backup.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.backup.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:48:20,237 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:48:20,237 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,237 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,238 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,239 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,239 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,239 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,239 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,239 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,239 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,240 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,240 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,240 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,240 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,240 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,240 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,241 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,241 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300240
2023-07-30 13:48:20,243 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-21, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,267 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-21 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,267 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21
2023-07-30 13:48:20,268 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0
2023-07-30 13:48:20,269 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,270 - INFO  [kafka-admin-client-thread | adminclient-11:AppInfoParser@83] - App info kafka.admin.client for adminclient-11 unregistered
2023-07-30 13:48:20,272 - INFO  [kafka-admin-client-thread | adminclient-11:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,272 - INFO  [kafka-admin-client-thread | adminclient-11:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,272 - INFO  [kafka-admin-client-thread | adminclient-11:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,272 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:48:20,273 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-36, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,273 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:48:20,273 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:48:20,274 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-36 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,274 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36
2023-07-30 13:48:20,274 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0
2023-07-30 13:48:20,282 - INFO  [main:AbstractConnector@331] - Started http_localhost0@181e72d3{HTTP/1.1, (http/1.1)}{localhost:44883}
2023-07-30 13:48:20,283 - INFO  [main:Server@400] - Started @6036ms
2023-07-30 13:48:20,283 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-6, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,283 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44883/
2023-07-30 13:48:20,283 - INFO  [main:RestServer@219] - REST server listening at http://localhost:44883/, advertising URL http://localhost:44883/
2023-07-30 13:48:20,283 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44883/
2023-07-30 13:48:20,283 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:44883/
2023-07-30 13:48:20,284 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44883/
2023-07-30 13:48:20,284 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,284 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-6 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,284 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6
2023-07-30 13:48:20,284 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,284 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0
2023-07-30 13:48:20,285 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,286 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,287 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,287 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,287 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,287 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300287
2023-07-30 13:48:20,290 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-43, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,291 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-43 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,291 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43
2023-07-30 13:48:20,291 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0
2023-07-30 13:48:20,295 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,296 - INFO  [kafka-admin-client-thread | adminclient-12:AppInfoParser@83] - App info kafka.admin.client for adminclient-12 unregistered
2023-07-30 13:48:20,297 - INFO  [kafka-admin-client-thread | adminclient-12:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,297 - INFO  [kafka-admin-client-thread | adminclient-12:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,297 - INFO  [kafka-admin-client-thread | adminclient-12:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,298 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:48:20,298 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,298 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,298 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-13, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,300 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,300 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,300 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,300 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-13 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,300 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,300 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13
2023-07-30 13:48:20,301 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,301 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,301 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,301 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,301 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,302 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,302 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,302 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,302 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,302 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,302 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300302
2023-07-30 13:48:20,301 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0
2023-07-30 13:48:20,309 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-28, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:20,311 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-28 in /tmp/junit1823696247009190315/junit2950113589444295251/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:20,311 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28
2023-07-30 13:48:20,311 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0
2023-07-30 13:48:20,312 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,313 - INFO  [kafka-admin-client-thread | adminclient-13:AppInfoParser@83] - App info kafka.admin.client for adminclient-13 unregistered
2023-07-30 13:48:20,314 - INFO  [kafka-admin-client-thread | adminclient-13:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,314 - INFO  [kafka-admin-client-thread | adminclient-13:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,315 - INFO  [kafka-admin-client-thread | adminclient-13:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,315 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,315 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,315 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300315
2023-07-30 13:48:20,316 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:20,316 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2023-07-30 13:48:20,316 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:20,316 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,317 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,318 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,318 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,318 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,318 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18
2023-07-30 13:48:20,318 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,319 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,319 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,319 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,319 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,319 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41
2023-07-30 13:48:20,319 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,320 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10
2023-07-30 13:48:20,320 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,320 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33
2023-07-30 13:48:20,320 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,320 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48
2023-07-30 13:48:20,320 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,320 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19
2023-07-30 13:48:20,321 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,321 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34
2023-07-30 13:48:20,321 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,321 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2023-07-30 13:48:20,321 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300321
2023-07-30 13:48:20,321 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11
2023-07-30 13:48:20,321 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26
2023-07-30 13:48:20,322 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49
2023-07-30 13:48:20,322 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39
2023-07-30 13:48:20,322 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9
2023-07-30 13:48:20,322 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24
2023-07-30 13:48:20,323 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31
2023-07-30 13:48:20,323 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 6 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,323 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46
2023-07-30 13:48:20,323 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2023-07-30 13:48:20,325 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16
2023-07-30 13:48:20,325 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,325 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2023-07-30 13:48:20,325 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25
2023-07-30 13:48:20,325 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40
2023-07-30 13:48:20,325 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47
2023-07-30 13:48:20,325 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,325 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17
2023-07-30 13:48:20,325 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,325 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,325 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32
2023-07-30 13:48:20,326 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,326 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37
2023-07-30 13:48:20,326 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7
2023-07-30 13:48:20,326 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,326 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22
2023-07-30 13:48:20,326 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29
2023-07-30 13:48:20,326 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,326 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44
2023-07-30 13:48:20,326 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14
2023-07-30 13:48:20,326 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,326 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23
2023-07-30 13:48:20,327 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 6 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,327 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38
2023-07-30 13:48:20,327 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8
2023-07-30 13:48:20,327 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,327 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45
2023-07-30 13:48:20,327 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15
2023-07-30 13:48:20,327 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,327 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30
2023-07-30 13:48:20,327 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2023-07-30 13:48:20,327 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,327 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35
2023-07-30 13:48:20,327 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5
2023-07-30 13:48:20,327 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,328 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20
2023-07-30 13:48:20,328 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27
2023-07-30 13:48:20,328 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,328 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42
2023-07-30 13:48:20,328 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12
2023-07-30 13:48:20,328 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,328 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21
2023-07-30 13:48:20,328 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36
2023-07-30 13:48:20,328 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,328 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6
2023-07-30 13:48:20,328 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,329 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43
2023-07-30 13:48:20,329 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,329 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13
2023-07-30 13:48:20,329 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,329 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28
2023-07-30 13:48:20,329 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,329 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,329 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,329 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,330 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 4 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,330 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,330 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,330 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,330 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,331 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 5 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,331 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,332 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,332 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 5 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,332 - INFO  [kafka-admin-client-thread | adminclient-14:AppInfoParser@83] - App info kafka.admin.client for adminclient-14 unregistered
2023-07-30 13:48:20,333 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,333 - INFO  [kafka-admin-client-thread | adminclient-14:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,333 - INFO  [kafka-admin-client-thread | adminclient-14:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,333 - INFO  [kafka-admin-client-thread | adminclient-14:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,333 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,334 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,334 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,334 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,334 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,334 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,335 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 8 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,335 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,335 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,335 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,335 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,335 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,336 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,336 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,336 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,336 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,336 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,336 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,336 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,335 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,336 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,337 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 9 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,337 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,337 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,337 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,337 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,337 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,337 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,337 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,337 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300337
2023-07-30 13:48:20,337 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,338 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,339 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 10 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,339 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:48:20,346 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,346 - INFO  [kafka-admin-client-thread | adminclient-15:AppInfoParser@83] - App info kafka.admin.client for adminclient-15 unregistered
2023-07-30 13:48:20,348 - INFO  [kafka-admin-client-thread | adminclient-15:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,348 - INFO  [kafka-admin-client-thread | adminclient-15:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,348 - INFO  [kafka-admin-client-thread | adminclient-15:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,348 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,348 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,350 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,350 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,350 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,350 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,350 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,350 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,350 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,351 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,351 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,351 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,351 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,351 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,351 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,351 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,351 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300351
2023-07-30 13:48:20,359 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,360 - INFO  [kafka-admin-client-thread | adminclient-16:AppInfoParser@83] - App info kafka.admin.client for adminclient-16 unregistered
2023-07-30 13:48:20,361 - INFO  [kafka-admin-client-thread | adminclient-16:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,361 - INFO  [kafka-admin-client-thread | adminclient-16:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,361 - INFO  [kafka-admin-client-thread | adminclient-16:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,362 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,362 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,363 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,364 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,364 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,364 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,364 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,364 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,364 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,364 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,364 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,364 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,365 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,365 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,365 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,365 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,365 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300365
2023-07-30 13:48:20,372 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,373 - INFO  [kafka-admin-client-thread | adminclient-17:AppInfoParser@83] - App info kafka.admin.client for adminclient-17 unregistered
2023-07-30 13:48:20,375 - INFO  [kafka-admin-client-thread | adminclient-17:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,375 - INFO  [kafka-admin-client-thread | adminclient-17:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,375 - INFO  [kafka-admin-client-thread | adminclient-17:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,375 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-1, groupId=backup-mm2] Discovered group coordinator localhost:41091 (id: 2147483647 rack: null)
2023-07-30 13:48:20,376 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,377 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,377 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300376
2023-07-30 13:48:20,377 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 282ms
2023-07-30 13:48:20,377 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:48:20,377 - INFO  [DistributedHerder-connect-1-1:WorkerCoordinator@225] - [Worker clientId=connect-1, groupId=backup-mm2] Rebalance started
2023-07-30 13:48:20,378 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@534] - [Worker clientId=connect-1, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:48:20,378 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:48:20,378 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@286] - [Worker clientId=connect-2, groupId=backup-mm2] Herder starting
2023-07-30 13:48:20,379 - INFO  [DistributedHerder-connect-2-1:Worker@195] - Worker starting
2023-07-30 13:48:20,379 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:48:20,380 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.backup.internal
2023-07-30 13:48:20,381 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:48:20,381 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,382 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,382 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,382 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,382 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,382 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,382 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,382 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,383 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,383 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,383 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,383 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,383 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,383 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,383 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,383 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,383 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,383 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300383
2023-07-30 13:48:20,383 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:48:20,383 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:48:20,384 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 660000ms
2023-07-30 13:48:20,408 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@468] - [Worker clientId=connect-1, groupId=backup-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:48:20,408 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@534] - [Worker clientId=connect-1, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:48:20,416 - INFO  [kafka-admin-client-thread | adminclient-18:AppInfoParser@83] - App info kafka.admin.client for adminclient-18 unregistered
2023-07-30 13:48:20,417 - INFO  [kafka-admin-client-thread | adminclient-18:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,417 - INFO  [kafka-admin-client-thread | adminclient-18:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,417 - INFO  [kafka-admin-client-thread | adminclient-18:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,418 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group backup-mm2 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member connect-1-a67aef66-336a-4e9e-8e54-170e720354ff with group instance id None)
2023-07-30 13:48:20,418 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:20,421 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,422 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,422 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,422 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,422 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,422 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,423 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,423 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,423 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,423 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,423 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,423 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,423 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,423 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,423 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,423 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,423 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300423
2023-07-30 13:48:20,424 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:20,427 - INFO  [kafka-producer-network-thread | producer-5:Metadata@279] - [Producer clientId=producer-5] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,428 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group backup-mm2 generation 1 (__consumer_offsets-5)
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,428 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,429 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,429 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,429 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300429
2023-07-30 13:48:20,430 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=1, memberId='connect-1-a67aef66-336a-4e9e-8e54-170e720354ff', protocol='sessioned'}
2023-07-30 13:48:20,433 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,439 - INFO  [DistributedHerder-connect-2-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Subscribed to partition(s): mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-21
2023-07-30 13:48:20,440 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-0
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-5
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-10
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-20
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-15
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-9
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-11
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-16
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-4
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-17
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-3
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-24
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-23
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-13
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-18
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-22
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-2
2023-07-30 13:48:20,441 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-8
2023-07-30 13:48:20,442 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-12
2023-07-30 13:48:20,442 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-19
2023-07-30 13:48:20,442 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-14
2023-07-30 13:48:20,442 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-1
2023-07-30 13:48:20,442 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-6
2023-07-30 13:48:20,442 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-7
2023-07-30 13:48:20,442 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-21
2023-07-30 13:48:20,455 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,455 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,455 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,456 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,456 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,456 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,456 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,456 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,456 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,456 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group backup-mm2 for generation 1
2023-07-30 13:48:20,456 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,457 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,457 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,457 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,457 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,457 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,457 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,457 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,457 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
2023-07-30 13:48:20,458 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,458 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,458 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,458 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,458 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:48:20,458 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,458 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:48:20,462 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:20,462 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:20,463 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:48:20,463 - INFO  [DistributedHerder-connect-2-1:Worker@202] - Worker started
2023-07-30 13:48:20,464 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.backup.internal
2023-07-30 13:48:20,465 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,469 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,469 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,469 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,469 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,469 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,470 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,470 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,470 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,470 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,470 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,470 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,470 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,471 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,471 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,471 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,471 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,471 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300471
2023-07-30 13:48:20,491 - INFO  [kafka-admin-client-thread | adminclient-19:AppInfoParser@83] - App info kafka.admin.client for adminclient-19 unregistered
2023-07-30 13:48:20,492 - INFO  [kafka-admin-client-thread | adminclient-19:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,492 - INFO  [kafka-admin-client-thread | adminclient-19:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,493 - INFO  [kafka-admin-client-thread | adminclient-19:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,493 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:20,495 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,496 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,496 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,497 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,497 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,497 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,497 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,497 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,497 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,497 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,497 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,497 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,498 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,498 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,498 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,498 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,498 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300498
2023-07-30 13:48:20,499 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:20,500 - INFO  [kafka-producer-network-thread | producer-6:Metadata@279] - [Producer clientId=producer-6] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,502 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,502 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,502 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300502
2023-07-30 13:48:20,507 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,513 - INFO  [DistributedHerder-connect-2-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Subscribed to partition(s): mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2, mm2-status.backup.internal-3
2023-07-30 13:48:20,514 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-0
2023-07-30 13:48:20,514 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-4
2023-07-30 13:48:20,514 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-1
2023-07-30 13:48:20,514 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-2
2023-07-30 13:48:20,514 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-3
2023-07-30 13:48:20,521 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,521 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,521 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,521 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,521 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,523 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:20,523 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:20,524 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:48:20,525 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.backup.internal
2023-07-30 13:48:20,525 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,529 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,530 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,530 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,530 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,530 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,530 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,530 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,530 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300530
2023-07-30 13:48:20,539 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=1, memberId='connect-1-a67aef66-336a-4e9e-8e54-170e720354ff', protocol='sessioned'}
2023-07-30 13:48:20,540 - INFO  [DistributedHerder-connect-1-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-1, groupId=backup-mm2] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-a67aef66-336a-4e9e-8e54-170e720354ff', leaderUrl='http://localhost:41979/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:20,541 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1215] - [Worker clientId=connect-1, groupId=backup-mm2] Starting connectors and tasks using config offset -1
2023-07-30 13:48:20,541 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1243] - [Worker clientId=connect-1, groupId=backup-mm2] Finished starting connectors and tasks
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:48:20,544 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@1fac1d5c{/,null,AVAILABLE}
2023-07-30 13:48:20,545 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:48:20,545 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:48:20,545 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:44883/'}
2023-07-30 13:48:20,545 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:48:20,555 - INFO  [kafka-admin-client-thread | adminclient-20:AppInfoParser@83] - App info kafka.admin.client for adminclient-20 unregistered
2023-07-30 13:48:20,557 - INFO  [kafka-admin-client-thread | adminclient-20:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,557 - INFO  [kafka-admin-client-thread | adminclient-20:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,557 - INFO  [kafka-admin-client-thread | adminclient-20:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,557 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:20,560 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,562 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,563 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,563 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,563 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,563 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,564 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,564 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,564 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,564 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,564 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,564 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,564 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,564 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,565 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,565 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,565 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300564
2023-07-30 13:48:20,565 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:20,568 - INFO  [kafka-producer-network-thread | producer-7:Metadata@279] - [Producer clientId=producer-7] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,570 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,571 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,571 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300570
2023-07-30 13:48:20,574 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,580 - INFO  [DistributedHerder-connect-2-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Subscribed to partition(s): mm2-configs.backup.internal-0
2023-07-30 13:48:20,580 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-configs.backup.internal-0
2023-07-30 13:48:20,590 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Resetting offset for partition mm2-configs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,593 - INFO  [KafkaBasedLog Work Thread - mm2-configs.backup.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-1, groupId=backup-mm2] Session key updated
2023-07-30 13:48:20,596 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:20,597 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:20,599 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:48:20,599 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@290] - [Worker clientId=connect-2, groupId=backup-mm2] Herder started
2023-07-30 13:48:20,605 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Worker clientId=connect-2, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,605 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-2, groupId=backup-mm2] Discovered group coordinator localhost:41091 (id: 2147483647 rack: null)
2023-07-30 13:48:20,607 - INFO  [DistributedHerder-connect-2-1:WorkerCoordinator@225] - [Worker clientId=connect-2, groupId=backup-mm2] Rebalance started
2023-07-30 13:48:20,608 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@534] - [Worker clientId=connect-2, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:48:20,612 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@468] - [Worker clientId=connect-2, groupId=backup-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:48:20,612 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@534] - [Worker clientId=connect-2, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:48:20,614 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group backup-mm2 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member connect-2-19c4f7ce-48ae-4053-bfab-c2555b604042 with group instance id None)
2023-07-30 13:48:20,688 - INFO  [main:Reflections@239] - Reflections took 141 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:48:20,691 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:20,693 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:20,694 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:20,699 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:48:20,700 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:20,700 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:20,700 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,700 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,700 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,701 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:20,701 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:20,701 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,702 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,702 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:20,702 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.backup.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = backup-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.backup.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.backup.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:48:20,703 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:48:20,703 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,703 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,704 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,705 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,706 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,706 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,706 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300706
2023-07-30 13:48:20,714 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,716 - INFO  [kafka-admin-client-thread | adminclient-21:AppInfoParser@83] - App info kafka.admin.client for adminclient-21 unregistered
2023-07-30 13:48:20,717 - INFO  [kafka-admin-client-thread | adminclient-21:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,718 - INFO  [kafka-admin-client-thread | adminclient-21:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,718 - INFO  [kafka-admin-client-thread | adminclient-21:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,718 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:48:20,718 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:48:20,719 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:48:20,730 - INFO  [main:AbstractConnector@331] - Started http_localhost0@28da7d11{HTTP/1.1, (http/1.1)}{localhost:40365}
2023-07-30 13:48:20,731 - INFO  [main:Server@400] - Started @6484ms
2023-07-30 13:48:20,731 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:40365/
2023-07-30 13:48:20,731 - INFO  [main:RestServer@219] - REST server listening at http://localhost:40365/, advertising URL http://localhost:40365/
2023-07-30 13:48:20,731 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:40365/
2023-07-30 13:48:20,731 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:40365/
2023-07-30 13:48:20,731 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:40365/
2023-07-30 13:48:20,731 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,732 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,733 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,733 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,733 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,733 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,733 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,733 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,733 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,733 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,734 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,734 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,734 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,734 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,734 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,734 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,734 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300734
2023-07-30 13:48:20,741 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,742 - INFO  [kafka-admin-client-thread | adminclient-22:AppInfoParser@83] - App info kafka.admin.client for adminclient-22 unregistered
2023-07-30 13:48:20,743 - INFO  [kafka-admin-client-thread | adminclient-22:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,743 - INFO  [kafka-admin-client-thread | adminclient-22:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,743 - INFO  [kafka-admin-client-thread | adminclient-22:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,743 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:48:20,743 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,744 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,745 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,745 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,745 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,745 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,745 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,745 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,745 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,745 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,746 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,746 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,746 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,746 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,746 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,746 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,746 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300746
2023-07-30 13:48:20,754 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,755 - INFO  [kafka-admin-client-thread | adminclient-23:AppInfoParser@83] - App info kafka.admin.client for adminclient-23 unregistered
2023-07-30 13:48:20,756 - INFO  [kafka-admin-client-thread | adminclient-23:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,756 - INFO  [kafka-admin-client-thread | adminclient-23:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,756 - INFO  [kafka-admin-client-thread | adminclient-23:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,756 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,756 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,756 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300756
2023-07-30 13:48:20,757 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:20,757 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:20,758 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,758 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,759 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,759 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,759 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,760 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,760 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,760 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,760 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,760 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,760 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,760 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,760 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,760 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,761 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,761 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,761 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300760
2023-07-30 13:48:20,768 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,768 - INFO  [kafka-admin-client-thread | adminclient-24:AppInfoParser@83] - App info kafka.admin.client for adminclient-24 unregistered
2023-07-30 13:48:20,769 - INFO  [kafka-admin-client-thread | adminclient-24:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,769 - INFO  [kafka-admin-client-thread | adminclient-24:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,769 - INFO  [kafka-admin-client-thread | adminclient-24:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,770 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,770 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,771 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,772 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,772 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,772 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,772 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,772 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,772 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,772 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,772 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,773 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,773 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,773 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,773 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,773 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,773 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300773
2023-07-30 13:48:20,785 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,786 - INFO  [kafka-admin-client-thread | adminclient-25:AppInfoParser@83] - App info kafka.admin.client for adminclient-25 unregistered
2023-07-30 13:48:20,786 - INFO  [kafka-admin-client-thread | adminclient-25:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,787 - INFO  [kafka-admin-client-thread | adminclient-25:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,787 - INFO  [kafka-admin-client-thread | adminclient-25:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,787 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,787 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,788 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,789 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,790 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,790 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,790 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300790
2023-07-30 13:48:20,799 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,799 - INFO  [kafka-admin-client-thread | adminclient-26:AppInfoParser@83] - App info kafka.admin.client for adminclient-26 unregistered
2023-07-30 13:48:20,800 - INFO  [kafka-admin-client-thread | adminclient-26:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,801 - INFO  [kafka-admin-client-thread | adminclient-26:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,801 - INFO  [kafka-admin-client-thread | adminclient-26:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,802 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:20,803 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,804 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,804 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,804 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,804 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,804 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,805 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,805 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,805 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,805 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,805 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,805 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,805 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,805 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,805 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,805 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300805
2023-07-30 13:48:20,814 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,814 - INFO  [kafka-admin-client-thread | adminclient-27:AppInfoParser@83] - App info kafka.admin.client for adminclient-27 unregistered
2023-07-30 13:48:20,815 - INFO  [kafka-admin-client-thread | adminclient-27:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,815 - INFO  [kafka-admin-client-thread | adminclient-27:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,815 - INFO  [kafka-admin-client-thread | adminclient-27:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,817 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,817 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,817 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300817
2023-07-30 13:48:20,818 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 273ms
2023-07-30 13:48:20,818 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:48:20,819 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:48:20,819 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@286] - [Worker clientId=connect-3, groupId=backup-mm2] Herder starting
2023-07-30 13:48:20,819 - INFO  [DistributedHerder-connect-3-1:Worker@195] - Worker starting
2023-07-30 13:48:20,819 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:48:20,819 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.backup.internal
2023-07-30 13:48:20,819 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:48:20,819 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,821 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,821 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,821 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,821 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,821 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,821 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,822 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,822 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,822 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,822 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,822 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,822 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,822 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,822 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,822 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,822 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,822 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:48:20,822 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300822
2023-07-30 13:48:20,823 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:48:20,823 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 600000ms
2023-07-30 13:48:20,841 - INFO  [kafka-admin-client-thread | adminclient-28:AppInfoParser@83] - App info kafka.admin.client for adminclient-28 unregistered
2023-07-30 13:48:20,843 - INFO  [kafka-admin-client-thread | adminclient-28:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,843 - INFO  [kafka-admin-client-thread | adminclient-28:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,843 - INFO  [kafka-admin-client-thread | adminclient-28:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,844 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:20,846 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,847 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,847 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,847 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,847 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,847 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,847 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,847 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,847 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,848 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,848 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,848 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,848 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,848 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,848 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,848 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,848 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300848
2023-07-30 13:48:20,849 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:20,851 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,851 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,851 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,851 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,851 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,851 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,851 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,852 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,852 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,852 - INFO  [kafka-producer-network-thread | producer-8:Metadata@279] - [Producer clientId=producer-8] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,852 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,852 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,852 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,852 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,852 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,852 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,852 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300852
2023-07-30 13:48:20,857 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,866 - INFO  [DistributedHerder-connect-3-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Subscribed to partition(s): mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-21
2023-07-30 13:48:20,866 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-0
2023-07-30 13:48:20,866 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-5
2023-07-30 13:48:20,866 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-10
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-20
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-15
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-9
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-11
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-16
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-4
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-17
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-3
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-24
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-23
2023-07-30 13:48:20,867 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-13
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-18
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-22
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-2
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-8
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-12
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-19
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-14
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-1
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-6
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-7
2023-07-30 13:48:20,868 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-21
2023-07-30 13:48:20,878 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,879 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,879 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,879 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,879 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,879 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,879 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,879 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,880 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,880 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,880 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,880 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,880 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,880 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,880 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,880 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,881 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,881 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,881 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,881 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,881 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,881 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,881 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,882 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,882 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,883 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:20,883 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:20,887 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:48:20,887 - INFO  [DistributedHerder-connect-3-1:Worker@202] - Worker started
2023-07-30 13:48:20,887 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.backup.internal
2023-07-30 13:48:20,888 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,892 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,893 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,893 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,893 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300893
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:48:20,914 - INFO  [kafka-admin-client-thread | adminclient-29:AppInfoParser@83] - App info kafka.admin.client for adminclient-29 unregistered
2023-07-30 13:48:20,915 - INFO  [kafka-admin-client-thread | adminclient-29:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,915 - INFO  [kafka-admin-client-thread | adminclient-29:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,915 - INFO  [kafka-admin-client-thread | adminclient-29:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,916 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:20,918 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,920 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,920 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,920 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,920 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300920
2023-07-30 13:48:20,921 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:20,923 - INFO  [kafka-producer-network-thread | producer-9:Metadata@279] - [Producer clientId=producer-9] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,923 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,923 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,923 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,923 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,924 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,924 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,924 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300924
2023-07-30 13:48:20,927 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,933 - INFO  [DistributedHerder-connect-3-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Subscribed to partition(s): mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2, mm2-status.backup.internal-3
2023-07-30 13:48:20,933 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-0
2023-07-30 13:48:20,933 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-4
2023-07-30 13:48:20,933 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-1
2023-07-30 13:48:20,933 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-2
2023-07-30 13:48:20,933 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-3
2023-07-30 13:48:20,939 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,939 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,939 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,940 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,940 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:20,940 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:20,940 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:20,942 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:48:20,942 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.backup.internal
2023-07-30 13:48:20,942 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,946 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,947 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,947 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,947 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300947
2023-07-30 13:48:20,961 - INFO  [kafka-admin-client-thread | adminclient-30:AppInfoParser@83] - App info kafka.admin.client for adminclient-30 unregistered
2023-07-30 13:48:20,962 - INFO  [kafka-admin-client-thread | adminclient-30:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:20,962 - INFO  [kafka-admin-client-thread | adminclient-30:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:20,962 - INFO  [kafka-admin-client-thread | adminclient-30:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:20,963 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:20,965 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,967 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,967 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,967 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300967
2023-07-30 13:48:20,968 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

Jul 30, 2023 1:48:20 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:48:20,969 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@6f94fb9d{/,null,AVAILABLE}
2023-07-30 13:48:20,969 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:48:20,969 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:48:20,970 - INFO  [kafka-producer-network-thread | producer-10:Metadata@279] - [Producer clientId=producer-10] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,970 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:40365/'}
2023-07-30 13:48:20,970 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,970 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:20,970 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,970 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:20,970 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,970 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,971 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:20,971 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:20,971 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,971 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:20,971 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:20,971 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,971 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:20,971 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:20,971 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:20,971 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739300971
2023-07-30 13:48:20,977 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:20,984 - INFO  [DistributedHerder-connect-3-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Subscribed to partition(s): mm2-configs.backup.internal-0
2023-07-30 13:48:20,986 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-configs.backup.internal-0
2023-07-30 13:48:20,993 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Resetting offset for partition mm2-configs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:21,006 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:21,007 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:21,008 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:48:21,008 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@290] - [Worker clientId=connect-3, groupId=backup-mm2] Herder started
2023-07-30 13:48:21,019 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Worker clientId=connect-3, groupId=backup-mm2] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:21,020 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-3, groupId=backup-mm2] Discovered group coordinator localhost:41091 (id: 2147483647 rack: null)
2023-07-30 13:48:21,020 - INFO  [DistributedHerder-connect-3-1:WorkerCoordinator@225] - [Worker clientId=connect-3, groupId=backup-mm2] Rebalance started
2023-07-30 13:48:21,021 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@534] - [Worker clientId=connect-3, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:48:21,024 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@468] - [Worker clientId=connect-3, groupId=backup-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:48:21,024 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@534] - [Worker clientId=connect-3, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:48:21,459 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:41979/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"WWOdeGZnSQSpvKBOAZLAUg"}
2023-07-30 13:48:21,488 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44883/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"WWOdeGZnSQSpvKBOAZLAUg"}
2023-07-30 13:48:21,507 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:40365/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"WWOdeGZnSQSpvKBOAZLAUg"}
2023-07-30 13:48:21,514 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2023-07-30 13:48:21,514 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2023-07-30 13:48:21,515 - INFO  [main:ZooKeeperServer@938] - minSessionTimeout set to 1600
2023-07-30 13:48:21,515 - INFO  [main:ZooKeeperServer@947] - maxSessionTimeout set to 16000
2023-07-30 13:48:21,515 - INFO  [main:ZooKeeperServer@166] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-6485244100902254092/version-2 snapdir /tmp/kafka-820634528737372158/version-2
2023-07-30 13:48:21,515 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2023-07-30 13:48:21,516 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2023-07-30 13:48:21,518 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-820634528737372158/version-2/snapshot.0
2023-07-30 13:48:21,519 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-820634528737372158/version-2/snapshot.0
2023-07-30 13:48:21,522 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5769116175745214285/junit1884222054139287341
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:36953
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:48:21,524 - INFO  [main:Logging@66] - starting
2023-07-30 13:48:21,525 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:36953
2023-07-30 13:48:21,525 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:36953.
2023-07-30 13:48:21,525 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:36953 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3ae126d1
2023-07-30 13:48:21,526 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2023-07-30 13:48:21,526 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2023-07-30 13:48:21,531 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2023-07-30 13:48:21,531 - INFO  [main-SendThread(127.0.0.1:36953):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:36953. Will not attempt to authenticate using SASL (unknown error)
2023-07-30 13:48:21,532 - INFO  [main-SendThread(127.0.0.1:36953):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:40346, server: localhost/127.0.0.1:36953
2023-07-30 13:48:21,535 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2023-07-30 13:48:21,538 - INFO  [main-SendThread(127.0.0.1:36953):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:36953, sessionid = 0x1087f3066fd0000, negotiated timeout = 16000
2023-07-30 13:48:21,539 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2023-07-30 13:48:21,590 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2023-07-30 13:48:21,599 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2023-07-30 13:48:21,599 - INFO  [feature-zk-node-event-process-thread:FinalizedFeatureCache$@42] - Cleared cache
2023-07-30 13:48:21,607 - INFO  [main:Logging@66] - Cluster ID = b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:21,607 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit5769116175745214285/junit1884222054139287341/meta.properties
2023-07-30 13:48:21,611 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5769116175745214285/junit1884222054139287341
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:36953
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:48:21,615 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5769116175745214285/junit1884222054139287341
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:36953
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:48:21,655 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2023-07-30 13:48:21,656 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2023-07-30 13:48:21,657 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2023-07-30 13:48:21,659 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2023-07-30 13:48:21,661 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit5769116175745214285/junit1884222054139287341)
2023-07-30 13:48:21,662 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit5769116175745214285/junit1884222054139287341 since no clean shutdown file was found
2023-07-30 13:48:21,663 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2023-07-30 13:48:21,665 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2023-07-30 13:48:21,678 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2023-07-30 13:48:21,792 - INFO  [main:Logging@66] - Created ConnectionAcceptRate sensor, quotaLimit=2147483647
2023-07-30 13:48:21,793 - INFO  [main:Logging@66] - Created ConnectionAcceptRate-PLAINTEXT sensor, quotaLimit=2147483647
2023-07-30 13:48:21,794 - INFO  [main:Logging@66] - Updated PLAINTEXT max connection creation rate to 2147483647
2023-07-30 13:48:21,795 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:34453.
2023-07-30 13:48:21,800 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:48:21,806 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2023-07-30 13:48:21,808 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2023-07-30 13:48:21,814 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2023-07-30 13:48:21,818 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2023-07-30 13:48:21,825 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2023-07-30 13:48:21,826 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Starting
2023-07-30 13:48:21,833 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2023-07-30 13:48:21,835 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 24,24,1690739301834,1690739301834,1,0,0,74449239714627584,204,0,24

2023-07-30 13:48:21,836 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:34453, czxid (broker epoch): 24
2023-07-30 13:48:21,843 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2023-07-30 13:48:21,847 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2023-07-30 13:48:21,848 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2023-07-30 13:48:21,850 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2023-07-30 13:48:21,851 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2023-07-30 13:48:21,851 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2023-07-30 13:48:21,853 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2023-07-30 13:48:21,855 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2023-07-30 13:48:21,862 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2023-07-30 13:48:21,863 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2023-07-30 13:48:21,867 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2023-07-30 13:48:21,907 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2023-07-30 13:48:21,912 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Starting socket server acceptors and processors
2023-07-30 13:48:21,918 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:48:21,919 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started socket server acceptors and processors
2023-07-30 13:48:21,920 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:21,920 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:21,921 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739295243
2023-07-30 13:48:21,921 - WARN  [main:AppInfoParser@68] - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.server:type=app-info,id=0
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:389)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:160)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:151)
	at kafka.utils.TestUtils.createServer(TestUtils.scala)
	at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.start(EmbeddedKafkaCluster.java:156)
	at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.start(EmbeddedKafkaCluster.java:134)
	at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.before(EmbeddedKafkaCluster.java:111)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:136)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2023-07-30 13:48:21,922 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2023-07-30 13:48:21,923 - INFO  [main:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:21,926 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Recorded new controller, from now on will use broker 0
2023-07-30 13:48:21,927 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:21,928 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:21,929 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739301926
2023-07-30 13:48:21,929 - INFO  [main:EmbeddedConnectCluster@235] - Starting Connect cluster 'backup-connect-cluster' with 3 workers
2023-07-30 13:48:21,929 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:48:21,936 - INFO  [kafka-producer-network-thread | producer-11:Metadata@279] - [Producer clientId=producer-11] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,046 - INFO  [main:Reflections@239] - Reflections took 114 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:48:22,049 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:22,053 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:22,055 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:22,061 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:48:22,061 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:22,062 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:22,062 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,062 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,062 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,063 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:22,064 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:22,064 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,064 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,064 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,065 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.primary.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = primary-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.primary.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.primary.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:48:22,066 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:48:22,066 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,067 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,069 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,069 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,069 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,069 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,069 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,069 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,070 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,070 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,070 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,070 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,070 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,070 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,071 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,071 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,071 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302070
2023-07-30 13:48:22,078 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,079 - INFO  [kafka-admin-client-thread | adminclient-31:AppInfoParser@83] - App info kafka.admin.client for adminclient-31 unregistered
2023-07-30 13:48:22,080 - INFO  [kafka-admin-client-thread | adminclient-31:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,080 - INFO  [kafka-admin-client-thread | adminclient-31:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,080 - INFO  [kafka-admin-client-thread | adminclient-31:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,081 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:48:22,081 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:48:22,082 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:48:22,112 - INFO  [main:AbstractConnector@331] - Started http_localhost0@630b6190{HTTP/1.1, (http/1.1)}{localhost:44785}
2023-07-30 13:48:22,112 - INFO  [main:Server@400] - Started @7866ms
2023-07-30 13:48:22,113 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44785/
2023-07-30 13:48:22,113 - INFO  [main:RestServer@219] - REST server listening at http://localhost:44785/, advertising URL http://localhost:44785/
2023-07-30 13:48:22,113 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44785/
2023-07-30 13:48:22,113 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:44785/
2023-07-30 13:48:22,114 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:44785/
2023-07-30 13:48:22,114 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,114 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,115 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,115 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,115 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,116 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,116 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,116 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,116 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,116 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,116 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,116 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,116 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,116 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,117 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,117 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,117 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302116
2023-07-30 13:48:22,128 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,129 - INFO  [kafka-admin-client-thread | adminclient-32:AppInfoParser@83] - App info kafka.admin.client for adminclient-32 unregistered
2023-07-30 13:48:22,130 - INFO  [kafka-admin-client-thread | adminclient-32:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,131 - INFO  [kafka-admin-client-thread | adminclient-32:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,131 - INFO  [kafka-admin-client-thread | adminclient-32:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,131 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:48:22,131 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,132 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,133 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,133 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,133 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,133 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,134 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,134 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,134 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,134 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,134 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,134 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,134 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,134 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,135 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,135 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,135 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302135
2023-07-30 13:48:22,152 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,152 - INFO  [kafka-admin-client-thread | adminclient-33:AppInfoParser@83] - App info kafka.admin.client for adminclient-33 unregistered
2023-07-30 13:48:22,154 - INFO  [kafka-admin-client-thread | adminclient-33:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,154 - INFO  [kafka-admin-client-thread | adminclient-33:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,154 - INFO  [kafka-admin-client-thread | adminclient-33:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,155 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,156 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,156 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302155
2023-07-30 13:48:22,157 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:22,157 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:22,157 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,158 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,159 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,160 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,160 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,160 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,160 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,160 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,160 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,160 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,161 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,161 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,161 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,161 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,161 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,161 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,162 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302161
2023-07-30 13:48:22,170 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,171 - INFO  [kafka-admin-client-thread | adminclient-34:AppInfoParser@83] - App info kafka.admin.client for adminclient-34 unregistered
2023-07-30 13:48:22,172 - INFO  [kafka-admin-client-thread | adminclient-34:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,172 - INFO  [kafka-admin-client-thread | adminclient-34:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,172 - INFO  [kafka-admin-client-thread | adminclient-34:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,172 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,173 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,175 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,175 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,175 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,176 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,176 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,176 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,176 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,176 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,176 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,176 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,176 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,177 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,177 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,177 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,177 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302177
2023-07-30 13:48:22,194 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,195 - INFO  [kafka-admin-client-thread | adminclient-35:AppInfoParser@83] - App info kafka.admin.client for adminclient-35 unregistered
2023-07-30 13:48:22,196 - INFO  [kafka-admin-client-thread | adminclient-35:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,196 - INFO  [kafka-admin-client-thread | adminclient-35:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,196 - INFO  [kafka-admin-client-thread | adminclient-35:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,197 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,197 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,199 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,199 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,199 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,199 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,200 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,200 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,200 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,200 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,200 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,200 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,200 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,201 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,201 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,201 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,201 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302201
2023-07-30 13:48:22,211 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,211 - INFO  [kafka-admin-client-thread | adminclient-36:AppInfoParser@83] - App info kafka.admin.client for adminclient-36 unregistered
2023-07-30 13:48:22,213 - INFO  [kafka-admin-client-thread | adminclient-36:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,213 - INFO  [kafka-admin-client-thread | adminclient-36:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,213 - INFO  [kafka-admin-client-thread | adminclient-36:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,214 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,215 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,217 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,217 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,217 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,218 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,218 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,218 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,218 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,218 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,219 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,219 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,219 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,219 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,219 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,219 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,219 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302219
2023-07-30 13:48:22,236 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,236 - INFO  [kafka-admin-client-thread | adminclient-37:AppInfoParser@83] - App info kafka.admin.client for adminclient-37 unregistered
2023-07-30 13:48:22,237 - INFO  [kafka-admin-client-thread | adminclient-37:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,237 - INFO  [kafka-admin-client-thread | adminclient-37:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,237 - INFO  [kafka-admin-client-thread | adminclient-37:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,239 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,239 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,239 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302239
2023-07-30 13:48:22,240 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 310ms
2023-07-30 13:48:22,240 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:48:22,240 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:48:22,240 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@286] - [Worker clientId=connect-4, groupId=primary-mm2] Herder starting
2023-07-30 13:48:22,240 - INFO  [DistributedHerder-connect-4-1:Worker@195] - Worker starting
2023-07-30 13:48:22,240 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:48:22,240 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.primary.internal
2023-07-30 13:48:22,241 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:48:22,241 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,242 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,242 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,243 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,243 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,243 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302243
2023-07-30 13:48:22,248 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:48:22,249 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:48:22,249 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 600000ms
2023-07-30 13:48:22,255 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic mm2-offsets.primary.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0))
2023-07-30 13:48:22,304 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-21, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-7)
2023-07-30 13:48:22,307 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-17, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,311 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-17 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,311 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-17 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-17
2023-07-30 13:48:22,311 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-17 broker=0] Log loaded for partition mm2-offsets.primary.internal-17 with initial high watermark 0
2023-07-30 13:48:22,317 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-2, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,318 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-2 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,318 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-2
2023-07-30 13:48:22,318 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-2 broker=0] Log loaded for partition mm2-offsets.primary.internal-2 with initial high watermark 0
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:48:22,325 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-21, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,326 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-21 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,326 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-21 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-21
2023-07-30 13:48:22,326 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-21 broker=0] Log loaded for partition mm2-offsets.primary.internal-21 with initial high watermark 0
2023-07-30 13:48:22,333 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-6, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,334 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-6 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,334 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-6 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-6
2023-07-30 13:48:22,334 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-6 broker=0] Log loaded for partition mm2-offsets.primary.internal-6 with initial high watermark 0
2023-07-30 13:48:22,342 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-10, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,344 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-10 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,344 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-10 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-10
2023-07-30 13:48:22,344 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-10 broker=0] Log loaded for partition mm2-offsets.primary.internal-10 with initial high watermark 0
2023-07-30 13:48:22,351 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-14, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,353 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-14 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,353 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-14 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-14
2023-07-30 13:48:22,353 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-14 broker=0] Log loaded for partition mm2-offsets.primary.internal-14 with initial high watermark 0
2023-07-30 13:48:22,358 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-16, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,359 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-16 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,360 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-16 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-16
2023-07-30 13:48:22,360 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-16 broker=0] Log loaded for partition mm2-offsets.primary.internal-16 with initial high watermark 0
2023-07-30 13:48:22,367 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-1, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,368 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-1 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,368 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-1
2023-07-30 13:48:22,368 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-1 broker=0] Log loaded for partition mm2-offsets.primary.internal-1 with initial high watermark 0
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:48:22,376 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-20, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,377 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@4667c4c1{/,null,AVAILABLE}
2023-07-30 13:48:22,377 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-20 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,377 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:48:22,378 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:48:22,378 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:44785/'}
2023-07-30 13:48:22,378 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-20 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-20
2023-07-30 13:48:22,378 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:48:22,379 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-20 broker=0] Log loaded for partition mm2-offsets.primary.internal-20 with initial high watermark 0
2023-07-30 13:48:22,384 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-5, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,385 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-5 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,386 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-5 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-5
2023-07-30 13:48:22,386 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-5 broker=0] Log loaded for partition mm2-offsets.primary.internal-5 with initial high watermark 0
2023-07-30 13:48:22,392 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-24, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,394 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-24 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,394 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-24 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-24
2023-07-30 13:48:22,394 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-24 broker=0] Log loaded for partition mm2-offsets.primary.internal-24 with initial high watermark 0
2023-07-30 13:48:22,401 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-9, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,403 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-9 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,403 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-9 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-9
2023-07-30 13:48:22,403 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-9 broker=0] Log loaded for partition mm2-offsets.primary.internal-9 with initial high watermark 0
2023-07-30 13:48:22,410 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-13, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,411 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-13 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,412 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-13 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-13
2023-07-30 13:48:22,412 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-13 broker=0] Log loaded for partition mm2-offsets.primary.internal-13 with initial high watermark 0
2023-07-30 13:48:22,420 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-15, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,422 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-15 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,422 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-15 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-15
2023-07-30 13:48:22,422 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-15 broker=0] Log loaded for partition mm2-offsets.primary.internal-15 with initial high watermark 0
2023-07-30 13:48:22,428 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,430 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-0 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,431 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-0
2023-07-30 13:48:22,431 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-0 broker=0] Log loaded for partition mm2-offsets.primary.internal-0 with initial high watermark 0
2023-07-30 13:48:22,437 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-19, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,439 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-19 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,439 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-19 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-19
2023-07-30 13:48:22,439 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-19 broker=0] Log loaded for partition mm2-offsets.primary.internal-19 with initial high watermark 0
2023-07-30 13:48:22,446 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-4, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,447 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-4 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,448 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-4
2023-07-30 13:48:22,448 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-4 broker=0] Log loaded for partition mm2-offsets.primary.internal-4 with initial high watermark 0
2023-07-30 13:48:22,453 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-23, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,454 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-23 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,454 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-23 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-23
2023-07-30 13:48:22,454 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-23 broker=0] Log loaded for partition mm2-offsets.primary.internal-23 with initial high watermark 0
2023-07-30 13:48:22,462 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-8, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,463 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-8 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,463 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-8 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-8
2023-07-30 13:48:22,463 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-8 broker=0] Log loaded for partition mm2-offsets.primary.internal-8 with initial high watermark 0
2023-07-30 13:48:22,470 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-12, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,472 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-12 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,472 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-12 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-12
2023-07-30 13:48:22,472 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-12 broker=0] Log loaded for partition mm2-offsets.primary.internal-12 with initial high watermark 0
2023-07-30 13:48:22,481 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-18, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,482 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-18 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,482 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-18 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-18
2023-07-30 13:48:22,483 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-18 broker=0] Log loaded for partition mm2-offsets.primary.internal-18 with initial high watermark 0
2023-07-30 13:48:22,487 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-3, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,487 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-3 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,488 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-3
2023-07-30 13:48:22,488 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-3 broker=0] Log loaded for partition mm2-offsets.primary.internal-3 with initial high watermark 0
2023-07-30 13:48:22,497 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-22, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,498 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-22 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,498 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-22 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-22
2023-07-30 13:48:22,498 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-22 broker=0] Log loaded for partition mm2-offsets.primary.internal-22 with initial high watermark 0
2023-07-30 13:48:22,504 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-7, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,505 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-7 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,506 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-7 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-7
2023-07-30 13:48:22,506 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-7 broker=0] Log loaded for partition mm2-offsets.primary.internal-7 with initial high watermark 0
2023-07-30 13:48:22,506 - INFO  [main:Reflections@239] - Reflections took 126 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:48:22,509 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:22,513 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:22,514 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-11, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,515 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-11 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,515 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-11 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-11
2023-07-30 13:48:22,515 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-11 broker=0] Log loaded for partition mm2-offsets.primary.internal-11 with initial high watermark 0
2023-07-30 13:48:22,516 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:22,522 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:48:22,524 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:22,524 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:22,524 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,524 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,524 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,525 - INFO  [DistributedHerder-connect-4-1:TopicAdmin@284] - Created topic (name=mm2-offsets.primary.internal, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:34453
2023-07-30 13:48:22,526 - INFO  [kafka-admin-client-thread | adminclient-38:AppInfoParser@83] - App info kafka.admin.client for adminclient-38 unregistered
2023-07-30 13:48:22,526 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:22,527 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:22,527 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,527 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,527 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,527 - INFO  [kafka-admin-client-thread | adminclient-38:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,528 - INFO  [kafka-admin-client-thread | adminclient-38:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,528 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.primary.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = primary-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.primary.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.primary.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:48:22,528 - INFO  [kafka-admin-client-thread | adminclient-38:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,528 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:48:22,528 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,528 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:22,529 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,530 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,531 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,531 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,531 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,531 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,531 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,533 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,533 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,533 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,534 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,533 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,534 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,534 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,534 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,534 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,534 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,534 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,535 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,535 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,535 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,535 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,535 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,535 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,535 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,535 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,535 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,535 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302535
2023-07-30 13:48:22,536 - INFO  [kafka-producer-network-thread | producer-12:Metadata@279] - [Producer clientId=producer-12] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,536 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,536 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,536 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:22,536 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,537 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,537 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302536
2023-07-30 13:48:22,540 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,540 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,540 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,540 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,540 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,541 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,541 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,541 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,541 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,541 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,541 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,541 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,541 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,541 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,541 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,541 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302541
2023-07-30 13:48:22,545 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,546 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,547 - INFO  [kafka-admin-client-thread | adminclient-39:AppInfoParser@83] - App info kafka.admin.client for adminclient-39 unregistered
2023-07-30 13:48:22,548 - INFO  [kafka-admin-client-thread | adminclient-39:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,548 - INFO  [kafka-admin-client-thread | adminclient-39:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,548 - INFO  [kafka-admin-client-thread | adminclient-39:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,548 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:48:22,549 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:48:22,549 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:48:22,553 - INFO  [DistributedHerder-connect-4-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Subscribed to partition(s): mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-7, mm2-offsets.primary.internal-21
2023-07-30 13:48:22,553 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-0
2023-07-30 13:48:22,553 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-5
2023-07-30 13:48:22,553 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-10
2023-07-30 13:48:22,553 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-20
2023-07-30 13:48:22,553 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-15
2023-07-30 13:48:22,554 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-9
2023-07-30 13:48:22,554 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-11
2023-07-30 13:48:22,554 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-16
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-4
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-17
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-3
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-24
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-23
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-13
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-18
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-22
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-2
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-8
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-12
2023-07-30 13:48:22,555 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-19
2023-07-30 13:48:22,556 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-14
2023-07-30 13:48:22,556 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-1
2023-07-30 13:48:22,556 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-6
2023-07-30 13:48:22,556 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-7
2023-07-30 13:48:22,556 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-21
2023-07-30 13:48:22,563 - INFO  [main:AbstractConnector@331] - Started http_localhost0@25d0cb3a{HTTP/1.1, (http/1.1)}{localhost:35973}
2023-07-30 13:48:22,564 - INFO  [main:Server@400] - Started @8317ms
2023-07-30 13:48:22,566 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:35973/
2023-07-30 13:48:22,567 - INFO  [main:RestServer@219] - REST server listening at http://localhost:35973/, advertising URL http://localhost:35973/
2023-07-30 13:48:22,567 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:35973/
2023-07-30 13:48:22,568 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,568 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:35973/
2023-07-30 13:48:22,568 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,569 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:35973/
2023-07-30 13:48:22,569 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,569 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,569 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,570 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,570 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,570 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,570 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,570 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,571 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,571 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,571 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,571 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,571 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,571 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,571 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,572 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,572 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,572 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,572 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,572 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,573 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,573 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,573 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,573 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,573 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,573 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,571 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,574 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,574 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,574 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302574
2023-07-30 13:48:22,574 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,574 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,575 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,576 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,576 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,576 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,576 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,576 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,576 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,577 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,578 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:22,578 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:22,580 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:48:22,580 - INFO  [DistributedHerder-connect-4-1:Worker@202] - Worker started
2023-07-30 13:48:22,580 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.primary.internal
2023-07-30 13:48:22,581 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,587 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,587 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,587 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,587 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,587 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,587 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,587 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,588 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,588 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,588 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,588 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,588 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,588 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,588 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,588 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,588 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,588 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302588
2023-07-30 13:48:22,589 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,589 - INFO  [kafka-admin-client-thread | adminclient-40:AppInfoParser@83] - App info kafka.admin.client for adminclient-40 unregistered
2023-07-30 13:48:22,594 - INFO  [kafka-admin-client-thread | adminclient-40:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,594 - INFO  [kafka-admin-client-thread | adminclient-40:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,594 - INFO  [kafka-admin-client-thread | adminclient-40:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,594 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:48:22,595 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,595 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,596 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,596 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,597 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,598 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,598 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,598 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302598
2023-07-30 13:48:22,603 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic mm2-status.primary.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2023-07-30 13:48:22,604 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,608 - INFO  [kafka-admin-client-thread | adminclient-42:AppInfoParser@83] - App info kafka.admin.client for adminclient-42 unregistered
2023-07-30 13:48:22,610 - INFO  [kafka-admin-client-thread | adminclient-42:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,610 - INFO  [kafka-admin-client-thread | adminclient-42:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,610 - INFO  [kafka-admin-client-thread | adminclient-42:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,611 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,611 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,611 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302611
2023-07-30 13:48:22,613 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:22,615 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:22,616 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,616 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,618 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,618 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,618 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,618 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,618 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,619 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,619 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,619 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,619 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,619 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,619 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,619 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,620 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,620 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,620 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302620
2023-07-30 13:48:22,626 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,626 - INFO  [kafka-admin-client-thread | adminclient-43:AppInfoParser@83] - App info kafka.admin.client for adminclient-43 unregistered
2023-07-30 13:48:22,627 - INFO  [kafka-admin-client-thread | adminclient-43:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,627 - INFO  [kafka-admin-client-thread | adminclient-43:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,627 - INFO  [kafka-admin-client-thread | adminclient-43:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,627 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,628 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,629 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,629 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,629 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,629 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,629 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,630 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-status.primary.internal-4, mm2-status.primary.internal-1, mm2-status.primary.internal-3, mm2-status.primary.internal-0, mm2-status.primary.internal-2)
2023-07-30 13:48:22,630 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,631 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,631 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,631 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,631 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,631 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,631 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,631 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,631 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,632 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302631
2023-07-30 13:48:22,634 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=mm2-status.primary.internal-1, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,635 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition mm2-status.primary.internal-1 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,635 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-1
2023-07-30 13:48:22,636 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-1 broker=0] Log loaded for partition mm2-status.primary.internal-1 with initial high watermark 0
2023-07-30 13:48:22,638 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,638 - INFO  [kafka-admin-client-thread | adminclient-44:AppInfoParser@83] - App info kafka.admin.client for adminclient-44 unregistered
2023-07-30 13:48:22,640 - INFO  [kafka-admin-client-thread | adminclient-44:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,640 - INFO  [kafka-admin-client-thread | adminclient-44:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,640 - INFO  [kafka-admin-client-thread | adminclient-44:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,640 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,640 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=mm2-status.primary.internal-2, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,641 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,641 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition mm2-status.primary.internal-2 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,642 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-2
2023-07-30 13:48:22,642 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-2 broker=0] Log loaded for partition mm2-status.primary.internal-2 with initial high watermark 0
2023-07-30 13:48:22,642 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,642 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,642 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,643 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,643 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,643 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,643 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,643 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,643 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,643 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,644 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,644 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,644 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,644 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,644 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302644
2023-07-30 13:48:22,648 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=mm2-status.primary.internal-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,650 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition mm2-status.primary.internal-0 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,650 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,650 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-0
2023-07-30 13:48:22,651 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-0 broker=0] Log loaded for partition mm2-status.primary.internal-0 with initial high watermark 0
2023-07-30 13:48:22,651 - INFO  [kafka-admin-client-thread | adminclient-45:AppInfoParser@83] - App info kafka.admin.client for adminclient-45 unregistered
2023-07-30 13:48:22,652 - INFO  [kafka-admin-client-thread | adminclient-45:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,652 - INFO  [kafka-admin-client-thread | adminclient-45:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,652 - INFO  [kafka-admin-client-thread | adminclient-45:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,652 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,653 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,655 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,655 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,655 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,655 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,656 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,656 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,656 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,656 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,656 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,656 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,656 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,657 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,657 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,657 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,657 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302657
2023-07-30 13:48:22,657 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=mm2-status.primary.internal-3, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,659 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition mm2-status.primary.internal-3 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,659 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-3
2023-07-30 13:48:22,659 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-3 broker=0] Log loaded for partition mm2-status.primary.internal-3 with initial high watermark 0
2023-07-30 13:48:22,664 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,664 - INFO  [kafka-admin-client-thread | adminclient-46:AppInfoParser@83] - App info kafka.admin.client for adminclient-46 unregistered
2023-07-30 13:48:22,666 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=mm2-status.primary.internal-4, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,666 - INFO  [kafka-admin-client-thread | adminclient-46:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,666 - INFO  [kafka-admin-client-thread | adminclient-46:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,666 - INFO  [kafka-admin-client-thread | adminclient-46:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,668 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,668 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,668 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302668
2023-07-30 13:48:22,669 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 290ms
2023-07-30 13:48:22,669 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:48:22,669 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:48:22,668 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition mm2-status.primary.internal-4 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,670 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-4
2023-07-30 13:48:22,670 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition mm2-status.primary.internal-4 broker=0] Log loaded for partition mm2-status.primary.internal-4 with initial high watermark 0
2023-07-30 13:48:22,670 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:48:22,670 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@286] - [Worker clientId=connect-5, groupId=primary-mm2] Herder starting
2023-07-30 13:48:22,670 - INFO  [DistributedHerder-connect-5-1:Worker@195] - Worker starting
2023-07-30 13:48:22,670 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:48:22,670 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.primary.internal
2023-07-30 13:48:22,671 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,672 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,673 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,673 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,674 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302673
2023-07-30 13:48:22,677 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:48:22,677 - INFO  [DistributedHerder-connect-4-1:TopicAdmin@284] - Created topic (name=mm2-status.primary.internal, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:34453
2023-07-30 13:48:22,678 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:48:22,678 - INFO  [kafka-admin-client-thread | adminclient-41:AppInfoParser@83] - App info kafka.admin.client for adminclient-41 unregistered
2023-07-30 13:48:22,678 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 660000ms
2023-07-30 13:48:22,679 - INFO  [kafka-admin-client-thread | adminclient-41:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,679 - INFO  [kafka-admin-client-thread | adminclient-41:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,679 - INFO  [kafka-admin-client-thread | adminclient-41:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,680 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:22,682 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,683 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,683 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,683 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,684 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,684 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,684 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302684
2023-07-30 13:48:22,685 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:22,686 - INFO  [kafka-producer-network-thread | producer-13:Metadata@279] - [Producer clientId=producer-13] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,688 - INFO  [kafka-admin-client-thread | adminclient-47:AppInfoParser@83] - App info kafka.admin.client for adminclient-47 unregistered
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,688 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,688 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,689 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302688
2023-07-30 13:48:22,689 - INFO  [kafka-admin-client-thread | adminclient-47:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,689 - INFO  [kafka-admin-client-thread | adminclient-47:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,689 - INFO  [kafka-admin-client-thread | adminclient-47:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,690 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:22,692 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,693 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,693 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,693 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,693 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,693 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,694 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,694 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,694 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,694 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,694 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,694 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,694 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,694 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,693 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,694 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,694 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,694 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302694
2023-07-30 13:48:22,695 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:22,696 - INFO  [kafka-producer-network-thread | producer-14:Metadata@279] - [Producer clientId=producer-14] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,698 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,698 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,698 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,699 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,699 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,699 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302699
2023-07-30 13:48:22,701 - INFO  [DistributedHerder-connect-4-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Subscribed to partition(s): mm2-status.primary.internal-0, mm2-status.primary.internal-1, mm2-status.primary.internal-4, mm2-status.primary.internal-2, mm2-status.primary.internal-3
2023-07-30 13:48:22,702 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-0
2023-07-30 13:48:22,702 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-1
2023-07-30 13:48:22,702 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-4
2023-07-30 13:48:22,702 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-2
2023-07-30 13:48:22,702 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-3
2023-07-30 13:48:22,704 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,708 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,708 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,709 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,709 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,709 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,709 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:22,709 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:22,711 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:48:22,711 - INFO  [DistributedHerder-connect-5-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Subscribed to partition(s): mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-7, mm2-offsets.primary.internal-21
2023-07-30 13:48:22,712 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.primary.internal
2023-07-30 13:48:22,713 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-0
2023-07-30 13:48:22,715 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-5
2023-07-30 13:48:22,715 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,715 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-10
2023-07-30 13:48:22,715 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-20
2023-07-30 13:48:22,715 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-15
2023-07-30 13:48:22,715 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-9
2023-07-30 13:48:22,716 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-11
2023-07-30 13:48:22,716 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-16
2023-07-30 13:48:22,716 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-4
2023-07-30 13:48:22,717 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-17
2023-07-30 13:48:22,717 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-3
2023-07-30 13:48:22,717 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-24
2023-07-30 13:48:22,717 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-23
2023-07-30 13:48:22,717 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-13
2023-07-30 13:48:22,717 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,717 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-18
2023-07-30 13:48:22,717 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,717 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-22
2023-07-30 13:48:22,717 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-2
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-8
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,718 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-12
2023-07-30 13:48:22,718 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,719 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-19
2023-07-30 13:48:22,719 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,719 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-14
2023-07-30 13:48:22,719 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302718
2023-07-30 13:48:22,719 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-1
2023-07-30 13:48:22,719 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-6
2023-07-30 13:48:22,719 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-7
2023-07-30 13:48:22,719 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-21
2023-07-30 13:48:22,726 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,726 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic mm2-configs.primary.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:48:22,726 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,727 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,727 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,727 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,727 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,727 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,727 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,728 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,729 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,729 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,729 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,729 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,729 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,729 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,729 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,730 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:22,730 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:22,732 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:48:22,732 - INFO  [DistributedHerder-connect-5-1:Worker@202] - Worker started
2023-07-30 13:48:22,732 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.primary.internal
2023-07-30 13:48:22,733 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,736 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,736 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,736 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,736 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,736 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,736 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,736 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,739 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,740 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,740 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,740 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,740 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,740 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,740 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,740 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,740 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,740 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302740
2023-07-30 13:48:22,749 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.primary.internal-0)
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:48:22,754 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=mm2-configs.primary.internal-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,755 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition mm2-configs.primary.internal-0 in /tmp/junit5769116175745214285/junit1884222054139287341/mm2-configs.primary.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,756 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-configs.primary.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-configs.primary.internal-0
2023-07-30 13:48:22,756 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition mm2-configs.primary.internal-0 broker=0] Log loaded for partition mm2-configs.primary.internal-0 with initial high watermark 0
2023-07-30 13:48:22,760 - INFO  [DistributedHerder-connect-4-1:TopicAdmin@284] - Created topic (name=mm2-configs.primary.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:34453
2023-07-30 13:48:22,761 - INFO  [kafka-admin-client-thread | adminclient-48:AppInfoParser@83] - App info kafka.admin.client for adminclient-48 unregistered
2023-07-30 13:48:22,762 - INFO  [kafka-admin-client-thread | adminclient-48:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,762 - INFO  [kafka-admin-client-thread | adminclient-48:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,762 - INFO  [kafka-admin-client-thread | adminclient-48:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,763 - INFO  [kafka-admin-client-thread | adminclient-49:AppInfoParser@83] - App info kafka.admin.client for adminclient-49 unregistered
2023-07-30 13:48:22,763 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:22,764 - INFO  [kafka-admin-client-thread | adminclient-49:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,764 - INFO  [kafka-admin-client-thread | adminclient-49:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,764 - INFO  [kafka-admin-client-thread | adminclient-49:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,765 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:22,765 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,767 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,768 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,768 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,768 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302768
2023-07-30 13:48:22,768 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:22,769 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,773 - INFO  [kafka-producer-network-thread | producer-15:Metadata@279] - [Producer clientId=producer-15] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,778 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,778 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,778 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,778 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,778 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,779 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302779
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,779 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,780 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,780 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,780 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,780 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,780 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,780 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,780 - INFO  [kafka-producer-network-thread | producer-16:Metadata@279] - [Producer clientId=producer-16] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,780 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:22,780 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,780 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,780 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,780 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,780 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302780
2023-07-30 13:48:22,783 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,783 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,783 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,783 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,783 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,783 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,783 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,783 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,783 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,784 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,784 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,784 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,784 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,784 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,784 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,784 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,784 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302784
2023-07-30 13:48:22,787 - INFO  [DistributedHerder-connect-4-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Subscribed to partition(s): mm2-configs.primary.internal-0
2023-07-30 13:48:22,787 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,787 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-configs.primary.internal-0
2023-07-30 13:48:22,794 - INFO  [DistributedHerder-connect-5-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Subscribed to partition(s): mm2-status.primary.internal-0, mm2-status.primary.internal-1, mm2-status.primary.internal-4, mm2-status.primary.internal-2, mm2-status.primary.internal-3
2023-07-30 13:48:22,794 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-configs.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,794 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-0
2023-07-30 13:48:22,794 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-1
2023-07-30 13:48:22,795 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-4
2023-07-30 13:48:22,795 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:22,795 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:22,795 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-2
2023-07-30 13:48:22,797 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:48:22,797 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-3
2023-07-30 13:48:22,798 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@290] - [Worker clientId=connect-4, groupId=primary-mm2] Herder started
2023-07-30 13:48:22,814 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Worker clientId=connect-4, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,815 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,816 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,816 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,816 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,816 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,817 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:22,817 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.primary.internal
Jul 30, 2023 1:48:22 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:48:22,818 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:48:22,821 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.primary.internal
2023-07-30 13:48:22,823 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,823 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@2fd64b11{/,null,AVAILABLE}
2023-07-30 13:48:22,824 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:48:22,824 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:48:22,824 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,824 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:35973/'}
2023-07-30 13:48:22,824 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,824 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,824 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,824 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,825 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,825 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,825 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,825 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,825 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,825 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,825 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,825 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,825 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,824 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:48:22,825 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,825 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,825 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302825
2023-07-30 13:48:22,826 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0))
2023-07-30 13:48:22,831 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2023-07-30 13:48:22,840 - INFO  [kafka-admin-client-thread | adminclient-50:AppInfoParser@83] - App info kafka.admin.client for adminclient-50 unregistered
2023-07-30 13:48:22,841 - INFO  [kafka-admin-client-thread | adminclient-50:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,841 - INFO  [kafka-admin-client-thread | adminclient-50:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,841 - INFO  [kafka-admin-client-thread | adminclient-50:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,841 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:22,844 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,846 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,846 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,846 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302846
2023-07-30 13:48:22,847 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:22,849 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,850 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,850 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,850 - INFO  [kafka-producer-network-thread | producer-17:Metadata@279] - [Producer clientId=producer-17] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,850 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,851 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,851 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,851 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302851
2023-07-30 13:48:22,854 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,859 - INFO  [DistributedHerder-connect-5-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Subscribed to partition(s): mm2-configs.primary.internal-0
2023-07-30 13:48:22,859 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-configs.primary.internal-0
2023-07-30 13:48:22,865 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Resetting offset for partition mm2-configs.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:22,865 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:22,865 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:22,865 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:48:22,867 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@290] - [Worker clientId=connect-5, groupId=primary-mm2] Herder started
2023-07-30 13:48:22,882 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Worker clientId=connect-5, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,899 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40)
2023-07-30 13:48:22,903 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,904 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,905 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2023-07-30 13:48:22,905 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2023-07-30 13:48:22,909 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-18, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,910 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-18 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,910 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18
2023-07-30 13:48:22,910 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0
2023-07-30 13:48:22,918 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-41, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,920 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-41 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,921 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41
2023-07-30 13:48:22,921 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0
2023-07-30 13:48:22,926 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-10, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,927 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-10 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,928 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10
2023-07-30 13:48:22,928 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0
2023-07-30 13:48:22,934 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-33, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,935 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-33 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,935 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33
2023-07-30 13:48:22,935 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0
2023-07-30 13:48:22,943 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-48, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,944 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-48 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,944 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48
2023-07-30 13:48:22,944 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0
2023-07-30 13:48:22,948 - INFO  [main:Reflections@239] - Reflections took 121 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:48:22,951 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:22,951 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-19, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,953 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-19 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,953 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19
2023-07-30 13:48:22,953 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0
2023-07-30 13:48:22,953 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:22,955 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:48:22,959 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-34, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,959 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:48:22,960 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:22,960 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:22,960 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,960 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,960 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,960 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-34 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,960 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34
2023-07-30 13:48:22,961 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0
2023-07-30 13:48:22,961 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:22,961 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:22,961 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,961 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,962 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:22,962 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.primary.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = primary-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.primary.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.primary.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:48:22,963 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:48:22,963 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,963 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,964 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,964 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,965 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,966 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,966 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,966 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302966
2023-07-30 13:48:22,968 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,969 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,969 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2023-07-30 13:48:22,969 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2023-07-30 13:48:22,970 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,970 - INFO  [kafka-admin-client-thread | adminclient-51:AppInfoParser@83] - App info kafka.admin.client for adminclient-51 unregistered
2023-07-30 13:48:22,971 - INFO  [kafka-admin-client-thread | adminclient-51:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,971 - INFO  [kafka-admin-client-thread | adminclient-51:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,971 - INFO  [kafka-admin-client-thread | adminclient-51:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,972 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:48:22,972 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:48:22,972 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:48:22,975 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-11, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,976 - INFO  [main:AbstractConnector@331] - Started http_localhost0@1984212d{HTTP/1.1, (http/1.1)}{localhost:37125}
2023-07-30 13:48:22,976 - INFO  [main:Server@400] - Started @8730ms
2023-07-30 13:48:22,976 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-11 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,977 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37125/
2023-07-30 13:48:22,977 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11
2023-07-30 13:48:22,977 - INFO  [main:RestServer@219] - REST server listening at http://localhost:37125/, advertising URL http://localhost:37125/
2023-07-30 13:48:22,977 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0
2023-07-30 13:48:22,977 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37125/
2023-07-30 13:48:22,977 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:37125/
2023-07-30 13:48:22,977 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37125/
2023-07-30 13:48:22,978 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,978 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,980 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,980 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,980 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,980 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,980 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,981 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,981 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,981 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,981 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,981 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,981 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,981 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,982 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,982 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,982 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302982
2023-07-30 13:48:22,984 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-26, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,985 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-26 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,985 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26
2023-07-30 13:48:22,985 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0
2023-07-30 13:48:22,986 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:22,987 - INFO  [kafka-admin-client-thread | adminclient-52:AppInfoParser@83] - App info kafka.admin.client for adminclient-52 unregistered
2023-07-30 13:48:22,987 - INFO  [kafka-admin-client-thread | adminclient-52:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:22,988 - INFO  [kafka-admin-client-thread | adminclient-52:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:22,988 - INFO  [kafka-admin-client-thread | adminclient-52:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:22,988 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:48:22,988 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:22,989 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:22,990 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,990 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:22,991 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,991 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:22,991 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,991 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,991 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:22,991 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,991 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:22,992 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:22,992 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,992 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:22,992 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-49, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:22,992 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:22,992 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:22,993 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739302992
2023-07-30 13:48:22,993 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-49 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:22,993 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49
2023-07-30 13:48:22,993 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0
2023-07-30 13:48:22,999 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,000 - INFO  [kafka-admin-client-thread | adminclient-53:AppInfoParser@83] - App info kafka.admin.client for adminclient-53 unregistered
2023-07-30 13:48:23,001 - INFO  [kafka-admin-client-thread | adminclient-53:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,001 - INFO  [kafka-admin-client-thread | adminclient-53:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,001 - INFO  [kafka-admin-client-thread | adminclient-53:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,001 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,001 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,002 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303001
2023-07-30 13:48:23,002 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-39, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,002 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:23,003 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:48:23,003 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:23,003 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-39 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,003 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,003 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39
2023-07-30 13:48:23,003 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0
2023-07-30 13:48:23,004 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,004 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,004 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,004 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,004 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,004 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,005 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,005 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,005 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,005 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,005 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,005 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,005 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,005 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,005 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303005
2023-07-30 13:48:23,009 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,009 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-9, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,010 - INFO  [kafka-admin-client-thread | adminclient-54:AppInfoParser@83] - App info kafka.admin.client for adminclient-54 unregistered
2023-07-30 13:48:23,010 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-9 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,010 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9
2023-07-30 13:48:23,011 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0
2023-07-30 13:48:23,011 - INFO  [kafka-admin-client-thread | adminclient-54:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,011 - INFO  [kafka-admin-client-thread | adminclient-54:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,011 - INFO  [kafka-admin-client-thread | adminclient-54:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,011 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:23,011 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,012 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,012 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,012 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,012 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,013 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,013 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,013 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,013 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,013 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,013 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,013 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,013 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,013 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,013 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,014 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303013
2023-07-30 13:48:23,018 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-24, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,018 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,019 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-24 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,019 - INFO  [kafka-admin-client-thread | adminclient-55:AppInfoParser@83] - App info kafka.admin.client for adminclient-55 unregistered
2023-07-30 13:48:23,019 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24
2023-07-30 13:48:23,019 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0
2023-07-30 13:48:23,020 - INFO  [kafka-admin-client-thread | adminclient-55:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,020 - INFO  [kafka-admin-client-thread | adminclient-55:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,020 - INFO  [kafka-admin-client-thread | adminclient-55:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,020 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:23,021 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,022 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,022 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,022 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,022 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,022 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,022 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,022 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,022 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,023 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,023 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,023 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,023 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,023 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,023 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,023 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303023
2023-07-30 13:48:23,027 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-31, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,028 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-31 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,028 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,028 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31
2023-07-30 13:48:23,028 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0
2023-07-30 13:48:23,028 - INFO  [kafka-admin-client-thread | adminclient-56:AppInfoParser@83] - App info kafka.admin.client for adminclient-56 unregistered
2023-07-30 13:48:23,029 - INFO  [kafka-admin-client-thread | adminclient-56:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,029 - INFO  [kafka-admin-client-thread | adminclient-56:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,029 - INFO  [kafka-admin-client-thread | adminclient-56:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,030 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:48:23,030 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,031 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,031 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,031 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,031 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,031 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,032 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,032 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,032 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,032 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,032 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,032 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,032 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,033 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,033 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,033 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303033
2023-07-30 13:48:23,034 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-46, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,035 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-46 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,035 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46
2023-07-30 13:48:23,035 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0
2023-07-30 13:48:23,037 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,037 - INFO  [kafka-admin-client-thread | adminclient-57:AppInfoParser@83] - App info kafka.admin.client for adminclient-57 unregistered
2023-07-30 13:48:23,038 - INFO  [kafka-admin-client-thread | adminclient-57:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,038 - INFO  [kafka-admin-client-thread | adminclient-57:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,038 - INFO  [kafka-admin-client-thread | adminclient-57:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,039 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,039 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,040 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303039
2023-07-30 13:48:23,040 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 216ms
2023-07-30 13:48:23,041 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:48:23,041 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:48:23,041 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@286] - [Worker clientId=connect-6, groupId=primary-mm2] Herder starting
2023-07-30 13:48:23,041 - INFO  [DistributedHerder-connect-6-1:Worker@195] - Worker starting
2023-07-30 13:48:23,041 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:48:23,041 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.primary.internal
2023-07-30 13:48:23,041 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:48:23,042 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,043 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,044 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,044 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,044 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,044 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,044 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,044 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303044
2023-07-30 13:48:23,044 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,044 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2023-07-30 13:48:23,044 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:48:23,044 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2023-07-30 13:48:23,044 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:48:23,045 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 600000ms
2023-07-30 13:48:23,052 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-16, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,053 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-16 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,053 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16
2023-07-30 13:48:23,053 - INFO  [kafka-admin-client-thread | adminclient-58:AppInfoParser@83] - App info kafka.admin.client for adminclient-58 unregistered
2023-07-30 13:48:23,053 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0
2023-07-30 13:48:23,055 - INFO  [kafka-admin-client-thread | adminclient-58:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,055 - INFO  [kafka-admin-client-thread | adminclient-58:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,055 - INFO  [kafka-admin-client-thread | adminclient-58:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,056 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:23,058 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,061 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,061 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,061 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303061
2023-07-30 13:48:23,061 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,061 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:23,062 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,062 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2023-07-30 13:48:23,062 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2023-07-30 13:48:23,062 - INFO  [kafka-producer-network-thread | producer-18:Metadata@279] - [Producer clientId=producer-18] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,064 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,065 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,065 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,065 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303064
2023-07-30 13:48:23,067 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,068 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-25, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,069 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-25 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,069 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25
2023-07-30 13:48:23,069 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0
2023-07-30 13:48:23,073 - INFO  [DistributedHerder-connect-6-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Subscribed to partition(s): mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-7, mm2-offsets.primary.internal-21
2023-07-30 13:48:23,073 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-0
2023-07-30 13:48:23,073 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-5
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-10
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-20
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-15
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-9
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-11
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-16
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-4
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-17
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-3
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-24
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-23
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-13
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-18
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-22
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-2
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-8
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-12
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-19
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-14
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-1
2023-07-30 13:48:23,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-6
2023-07-30 13:48:23,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-7
2023-07-30 13:48:23,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-21
2023-07-30 13:48:23,076 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-40, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,077 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-40 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,077 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40
2023-07-30 13:48:23,077 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0
2023-07-30 13:48:23,081 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,082 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,082 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,082 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,083 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,083 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,083 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,083 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,083 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,083 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,083 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,083 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,084 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,084 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,084 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,084 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,084 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,084 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,084 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,084 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,084 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,085 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,085 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,085 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-47, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,085 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,085 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,086 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:23,086 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:23,086 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:48:23,088 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-47 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,088 - INFO  [DistributedHerder-connect-6-1:Worker@202] - Worker started
2023-07-30 13:48:23,088 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47
2023-07-30 13:48:23,088 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.primary.internal
2023-07-30 13:48:23,088 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0
2023-07-30 13:48:23,089 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,095 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,096 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,096 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303095
2023-07-30 13:48:23,097 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-17, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,098 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-17 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,098 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17
2023-07-30 13:48:23,098 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0
2023-07-30 13:48:23,103 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-32, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,105 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-32 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,106 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32
2023-07-30 13:48:23,107 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0
2023-07-30 13:48:23,109 - INFO  [kafka-admin-client-thread | adminclient-59:AppInfoParser@83] - App info kafka.admin.client for adminclient-59 unregistered
2023-07-30 13:48:23,111 - INFO  [kafka-admin-client-thread | adminclient-59:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,111 - INFO  [kafka-admin-client-thread | adminclient-59:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,111 - INFO  [kafka-admin-client-thread | adminclient-59:Metrics@678] - Metrics reporters closed
Jul 30, 2023 1:48:23 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Jul 30, 2023 1:48:23 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
2023-07-30 13:48:23,111 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

Jul 30, 2023 1:48:23 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
Jul 30, 2023 1:48:23 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
2023-07-30 13:48:23,111 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-37, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,112 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-37 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,113 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37
2023-07-30 13:48:23,113 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0
2023-07-30 13:48:23,113 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,113 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,114 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,115 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,115 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,115 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303115
2023-07-30 13:48:23,116 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:23,117 - INFO  [kafka-producer-network-thread | producer-19:Metadata@279] - [Producer clientId=producer-19] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,118 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,118 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,118 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,118 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,118 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,118 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,118 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,118 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:23,118 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,119 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,119 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,119 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,119 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,119 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,119 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,122 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303119
2023-07-30 13:48:23,122 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-7, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,123 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-7 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,124 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7
2023-07-30 13:48:23,124 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0
2023-07-30 13:48:23,124 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,128 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-22, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,128 - INFO  [DistributedHerder-connect-6-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Subscribed to partition(s): mm2-status.primary.internal-0, mm2-status.primary.internal-1, mm2-status.primary.internal-4, mm2-status.primary.internal-2, mm2-status.primary.internal-3
2023-07-30 13:48:23,128 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-0
2023-07-30 13:48:23,128 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-1
2023-07-30 13:48:23,129 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-4
2023-07-30 13:48:23,129 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-2
2023-07-30 13:48:23,129 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-3
2023-07-30 13:48:23,129 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-22 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,129 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22
2023-07-30 13:48:23,129 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0
2023-07-30 13:48:23,134 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,134 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,134 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,134 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,134 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,135 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:23,135 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:23,136 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:48:23,137 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.primary.internal
2023-07-30 13:48:23,139 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,147 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-29, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,147 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,148 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,149 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,149 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,149 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-29 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,149 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,149 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,149 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303149
2023-07-30 13:48:23,149 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29
2023-07-30 13:48:23,149 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0
2023-07-30 13:48:23,154 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-44, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,155 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-44 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,155 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44
2023-07-30 13:48:23,155 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0
2023-07-30 13:48:23,160 - INFO  [kafka-admin-client-thread | adminclient-60:AppInfoParser@83] - App info kafka.admin.client for adminclient-60 unregistered
2023-07-30 13:48:23,161 - INFO  [kafka-admin-client-thread | adminclient-60:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,161 - INFO  [kafka-admin-client-thread | adminclient-60:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,161 - INFO  [kafka-admin-client-thread | adminclient-60:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,161 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-14, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,162 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:23,162 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-14 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,162 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14
2023-07-30 13:48:23,163 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0
Jul 30, 2023 1:48:23 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:48:23,164 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,164 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,165 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,165 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,165 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,166 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,166 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,166 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303166
2023-07-30 13:48:23,166 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@36f7d7b{/,null,AVAILABLE}
2023-07-30 13:48:23,167 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:48:23,167 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:48:23,167 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:23,167 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:37125/'}
2023-07-30 13:48:23,168 - INFO  [kafka-producer-network-thread | producer-20:Metadata@279] - [Producer clientId=producer-20] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,170 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-23, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,170 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,176 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:23,176 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,176 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:48:23,176 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,176 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,176 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:48:23,176 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:23,177 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,177 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:48:23,177 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:48:23,177 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,177 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-23 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,177 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:48:23,177 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23
2023-07-30 13:48:23,177 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,177 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0
2023-07-30 13:48:23,177 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,177 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303177
2023-07-30 13:48:23,181 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:41979/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"WWOdeGZnSQSpvKBOAZLAUg"}
2023-07-30 13:48:23,181 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,183 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-38, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,191 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-38 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,191 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38
2023-07-30 13:48:23,191 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0
2023-07-30 13:48:23,194 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44883/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"WWOdeGZnSQSpvKBOAZLAUg"}
2023-07-30 13:48:23,199 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-8, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,200 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-8 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,201 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8
2023-07-30 13:48:23,201 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0
2023-07-30 13:48:23,201 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:40365/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"WWOdeGZnSQSpvKBOAZLAUg"}
2023-07-30 13:48:23,202 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,203 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,203 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,204 - INFO  [DistributedHerder-connect-6-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Subscribed to partition(s): mm2-configs.primary.internal-0
2023-07-30 13:48:23,203 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-45, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,204 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-configs.primary.internal-0
2023-07-30 13:48:23,204 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303203
2023-07-30 13:48:23,204 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-45 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,205 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45
2023-07-30 13:48:23,205 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0
2023-07-30 13:48:23,208 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Resetting offset for partition mm2-configs.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,208 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:23,208 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:23,210 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:48:23,211 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@290] - [Worker clientId=connect-6, groupId=primary-mm2] Herder started
2023-07-30 13:48:23,214 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:48:23,215 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-15, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,216 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Worker clientId=connect-6, groupId=primary-mm2] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,216 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-15 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,216 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15
2023-07-30 13:48:23,216 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0
2023-07-30 13:48:23,222 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-30, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,223 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-30 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,223 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30
2023-07-30 13:48:23,223 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0
2023-07-30 13:48:23,229 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,230 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,230 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2023-07-30 13:48:23,230 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2023-07-30 13:48:23,233 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(test-topic-1-0, test-topic-1-3, test-topic-1-9, test-topic-1-5, test-topic-1-1, test-topic-1-6, test-topic-1-2, test-topic-1-8, test-topic-1-7, test-topic-1-4)
2023-07-30 13:48:23,235 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-0, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,235 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-0 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,236 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition test-topic-1-0
2023-07-30 13:48:23,236 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-0 broker=0] Log loaded for partition test-topic-1-0 with initial high watermark 0
2023-07-30 13:48:23,238 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-35, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,238 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-35 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,238 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35
2023-07-30 13:48:23,239 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0
2023-07-30 13:48:23,278 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-9, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,278 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-5, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,279 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-9 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,279 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-5 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,279 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-9 broker=0] No checkpointed highwatermark is found for partition test-topic-1-9
2023-07-30 13:48:23,279 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5
2023-07-30 13:48:23,279 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-9 broker=0] Log loaded for partition test-topic-1-9 with initial high watermark 0
2023-07-30 13:48:23,280 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0
2023-07-30 13:48:23,283 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-20, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,283 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-7, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,284 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-7 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,284 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-20 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,284 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-7 broker=0] No checkpointed highwatermark is found for partition test-topic-1-7
2023-07-30 13:48:23,284 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20
2023-07-30 13:48:23,284 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-7 broker=0] Log loaded for partition test-topic-1-7 with initial high watermark 0
2023-07-30 13:48:23,284 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0
2023-07-30 13:48:23,291 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-8, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,291 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-27, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,292 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-8 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,292 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-27 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,292 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-8 broker=0] No checkpointed highwatermark is found for partition test-topic-1-8
2023-07-30 13:48:23,292 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27
2023-07-30 13:48:23,292 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-8 broker=0] Log loaded for partition test-topic-1-8 with initial high watermark 0
2023-07-30 13:48:23,293 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0
2023-07-30 13:48:23,300 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-5, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,300 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-42, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,301 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-5 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,301 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-5 broker=0] No checkpointed highwatermark is found for partition test-topic-1-5
2023-07-30 13:48:23,301 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-42 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,301 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-5 broker=0] Log loaded for partition test-topic-1-5 with initial high watermark 0
2023-07-30 13:48:23,301 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42
2023-07-30 13:48:23,301 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0
2023-07-30 13:48:23,309 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-6, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,309 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-12, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,310 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-6 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,310 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-12 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,310 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-6 broker=0] No checkpointed highwatermark is found for partition test-topic-1-6
2023-07-30 13:48:23,310 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12
2023-07-30 13:48:23,310 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-6 broker=0] Log loaded for partition test-topic-1-6 with initial high watermark 0
2023-07-30 13:48:23,310 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0
2023-07-30 13:48:23,317 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-3, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,317 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-21, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,318 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-21 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,318 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-3 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,319 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21
2023-07-30 13:48:23,319 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-3 broker=0] No checkpointed highwatermark is found for partition test-topic-1-3
2023-07-30 13:48:23,319 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0
2023-07-30 13:48:23,319 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-3 broker=0] Log loaded for partition test-topic-1-3 with initial high watermark 0
2023-07-30 13:48:23,326 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-36, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,326 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-4, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,327 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-36 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,327 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-4 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,327 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36
2023-07-30 13:48:23,328 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-4 broker=0] No checkpointed highwatermark is found for partition test-topic-1-4
2023-07-30 13:48:23,328 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0
2023-07-30 13:48:23,328 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-4 broker=0] Log loaded for partition test-topic-1-4 with initial high watermark 0
2023-07-30 13:48:23,334 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-1, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,334 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-6, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,335 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-1 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,335 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-6 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,336 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-1 broker=0] No checkpointed highwatermark is found for partition test-topic-1-1
2023-07-30 13:48:23,336 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6
2023-07-30 13:48:23,336 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-1 broker=0] Log loaded for partition test-topic-1-1 with initial high watermark 0
2023-07-30 13:48:23,336 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0
2023-07-30 13:48:23,343 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-43, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,343 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-2, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,343 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-43 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,344 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43
2023-07-30 13:48:23,344 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0
2023-07-30 13:48:23,344 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-2 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-1-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,344 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-2 broker=0] No checkpointed highwatermark is found for partition test-topic-1-2
2023-07-30 13:48:23,344 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-2 broker=0] Log loaded for partition test-topic-1-2 with initial high watermark 0
2023-07-30 13:48:23,351 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-13, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,355 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-13 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,355 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13
2023-07-30 13:48:23,356 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0
2023-07-30 13:48:23,356 - INFO  [kafka-admin-client-thread | adminclient-61:AppInfoParser@83] - App info kafka.admin.client for adminclient-61 unregistered
2023-07-30 13:48:23,357 - INFO  [kafka-admin-client-thread | adminclient-61:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,360 - INFO  [kafka-admin-client-thread | adminclient-61:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,361 - INFO  [kafka-admin-client-thread | adminclient-61:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,361 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,363 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,363 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,363 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303363
2023-07-30 13:48:23,365 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-28, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,366 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-28 in /tmp/junit5769116175745214285/junit1884222054139287341/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,366 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28
2023-07-30 13:48:23,366 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0
2023-07-30 13:48:23,370 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Creating topic backup.test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:48:23,371 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2023-07-30 13:48:23,373 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18
2023-07-30 13:48:23,373 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,373 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41
2023-07-30 13:48:23,373 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10
2023-07-30 13:48:23,373 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,374 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,373 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33
2023-07-30 13:48:23,374 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48
2023-07-30 13:48:23,374 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,374 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19
2023-07-30 13:48:23,374 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,374 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34
2023-07-30 13:48:23,374 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,374 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2023-07-30 13:48:23,375 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,375 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11
2023-07-30 13:48:23,375 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,375 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26
2023-07-30 13:48:23,375 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,375 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49
2023-07-30 13:48:23,375 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,375 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,375 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9
2023-07-30 13:48:23,376 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31
2023-07-30 13:48:23,376 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46
2023-07-30 13:48:23,376 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47
2023-07-30 13:48:23,376 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17
2023-07-30 13:48:23,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32
2023-07-30 13:48:23,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37
2023-07-30 13:48:23,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7
2023-07-30 13:48:23,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22
2023-07-30 13:48:23,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29
2023-07-30 13:48:23,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44
2023-07-30 13:48:23,377 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14
2023-07-30 13:48:23,377 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23
2023-07-30 13:48:23,378 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 2 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,378 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38
2023-07-30 13:48:23,378 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,378 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8
2023-07-30 13:48:23,378 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45
2023-07-30 13:48:23,378 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15
2023-07-30 13:48:23,378 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30
2023-07-30 13:48:23,378 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,378 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2023-07-30 13:48:23,378 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,378 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35
2023-07-30 13:48:23,379 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 3 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,379 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5
2023-07-30 13:48:23,379 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,379 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20
2023-07-30 13:48:23,379 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27
2023-07-30 13:48:23,379 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,379 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42
2023-07-30 13:48:23,379 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12
2023-07-30 13:48:23,379 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,380 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,380 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,380 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,380 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,380 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,379 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21
2023-07-30 13:48:23,380 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36
2023-07-30 13:48:23,380 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,381 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,380 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6
2023-07-30 13:48:23,381 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,381 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43
2023-07-30 13:48:23,381 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,381 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13
2023-07-30 13:48:23,381 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28
2023-07-30 13:48:23,381 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,382 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,382 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,382 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(backup.test-topic-1-0)
2023-07-30 13:48:23,382 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,382 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,382 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,382 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,383 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,383 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,383 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,383 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,383 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,383 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,383 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,384 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,390 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,390 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:48:23,391 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=backup.test-topic-1-0, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,392 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition backup.test-topic-1-0 in /tmp/junit1823696247009190315/junit2950113589444295251/backup.test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,392 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition backup.test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition backup.test-topic-1-0
2023-07-30 13:48:23,392 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition backup.test-topic-1-0 broker=0] Log loaded for partition backup.test-topic-1-0 with initial high watermark 0
2023-07-30 13:48:23,396 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-5, groupId=primary-mm2] Discovered group coordinator localhost:34453 (id: 2147483647 rack: null)
2023-07-30 13:48:23,398 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:23,399 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:23,399 - INFO  [kafka-admin-client-thread | adminclient-62:AppInfoParser@83] - App info kafka.admin.client for adminclient-62 unregistered
2023-07-30 13:48:23,400 - INFO  [kafka-admin-client-thread | adminclient-62:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,401 - INFO  [kafka-admin-client-thread | adminclient-62:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,401 - INFO  [kafka-admin-client-thread | adminclient-62:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,401 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,402 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@468] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:48:23,402 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:23,403 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,405 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,405 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303403
2023-07-30 13:48:23,405 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 0 (__consumer_offsets-17) (reason: Adding new member connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7 with group instance id None)
2023-07-30 13:48:23,408 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 1 (__consumer_offsets-17)
2023-07-30 13:48:23,411 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=1, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:23,418 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 1
2023-07-30 13:48:23,425 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=1, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:23,426 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-6, groupId=primary-mm2] Discovered group coordinator localhost:34453 (id: 2147483647 rack: null)
2023-07-30 13:48:23,426 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:23,426 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset -1
2023-07-30 13:48:23,426 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:23,426 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:23,426 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:23,428 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic heartbeats with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:48:23,430 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@468] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:48:23,431 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:23,438 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 1 (__consumer_offsets-17) (reason: Adding new member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f with group instance id None)
2023-07-30 13:48:23,443 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-4, groupId=primary-mm2] Discovered group coordinator localhost:34453 (id: 2147483647 rack: null)
2023-07-30 13:48:23,448 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:23,455 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:23,457 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-1, groupId=backup-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:48:23,457 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-6, groupId=primary-mm2] Session key updated
2023-07-30 13:48:23,457 - INFO  [DistributedHerder-connect-1-1:WorkerCoordinator@225] - [Worker clientId=connect-1, groupId=backup-mm2] Rebalance started
2023-07-30 13:48:23,457 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@534] - [Worker clientId=connect-1, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:48:23,459 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-5, groupId=primary-mm2] Session key updated
2023-07-30 13:48:23,465 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Stabilized group backup-mm2 generation 2 (__consumer_offsets-5)
2023-07-30 13:48:23,466 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-4, groupId=primary-mm2] Session key updated
2023-07-30 13:48:23,469 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-1-a67aef66-336a-4e9e-8e54-170e720354ff', protocol='sessioned'}
2023-07-30 13:48:23,469 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@468] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:48:23,469 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-2-19c4f7ce-48ae-4053-bfab-c2555b604042', protocol='sessioned'}
2023-07-30 13:48:23,471 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:23,471 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-3, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-3-1326fd8d-96ab-4d62-b54d-4729124ff162', protocol='sessioned'}
2023-07-30 13:48:23,472 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group backup-mm2 for generation 2
2023-07-30 13:48:23,474 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(heartbeats-0)
2023-07-30 13:48:23,478 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-1-a67aef66-336a-4e9e-8e54-170e720354ff', protocol='sessioned'}
2023-07-30 13:48:23,485 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=heartbeats-0, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,486 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition heartbeats-0 in /tmp/junit1823696247009190315/junit2950113589444295251/heartbeats-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,486 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition heartbeats-0 broker=0] No checkpointed highwatermark is found for partition heartbeats-0
2023-07-30 13:48:23,487 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition heartbeats-0 broker=0] Log loaded for partition heartbeats-0 with initial high watermark 0
2023-07-30 13:48:23,490 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-2-19c4f7ce-48ae-4053-bfab-c2555b604042', protocol='sessioned'}
2023-07-30 13:48:23,490 - INFO  [DistributedHerder-connect-1-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-1, groupId=backup-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-a67aef66-336a-4e9e-8e54-170e720354ff', leaderUrl='http://localhost:41979/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:23,491 - INFO  [DistributedHerder-connect-2-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-2, groupId=backup-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-a67aef66-336a-4e9e-8e54-170e720354ff', leaderUrl='http://localhost:41979/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:23,494 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-3, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-3-1326fd8d-96ab-4d62-b54d-4729124ff162', protocol='sessioned'}
2023-07-30 13:48:23,494 - WARN  [DistributedHerder-connect-2-1:DistributedHerder@1094] - [Worker clientId=connect-2, groupId=backup-mm2] Catching up to assignment's config offset.
2023-07-30 13:48:23,494 - WARN  [DistributedHerder-connect-1-1:DistributedHerder@1094] - [Worker clientId=connect-1, groupId=backup-mm2] Catching up to assignment's config offset.
2023-07-30 13:48:23,494 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1157] - [Worker clientId=connect-2, groupId=backup-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:48:23,494 - INFO  [DistributedHerder-connect-3-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-3, groupId=backup-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-a67aef66-336a-4e9e-8e54-170e720354ff', leaderUrl='http://localhost:41979/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:23,494 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1157] - [Worker clientId=connect-1, groupId=backup-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:48:23,496 - WARN  [DistributedHerder-connect-3-1:DistributedHerder@1094] - [Worker clientId=connect-3, groupId=backup-mm2] Catching up to assignment's config offset.
2023-07-30 13:48:23,496 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1157] - [Worker clientId=connect-3, groupId=backup-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:48:23,498 - INFO  [kafka-admin-client-thread | adminclient-63:AppInfoParser@83] - App info kafka.admin.client for adminclient-63 unregistered
2023-07-30 13:48:23,499 - INFO  [kafka-admin-client-thread | adminclient-63:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,499 - INFO  [kafka-admin-client-thread | adminclient-63:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,500 - INFO  [kafka-admin-client-thread | adminclient-63:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,500 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,503 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,503 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,503 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303503
2023-07-30 13:48:23,510 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:48:23,519 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1161] - [Worker clientId=connect-3, groupId=backup-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:48:23,519 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1215] - [Worker clientId=connect-3, groupId=backup-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:48:23,519 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1243] - [Worker clientId=connect-3, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:48:23,529 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(test-topic-1-0, test-topic-1-3, test-topic-1-9, test-topic-1-5, test-topic-1-1, test-topic-1-6, test-topic-1-2, test-topic-1-8, test-topic-1-7, test-topic-1-4)
2023-07-30 13:48:23,532 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,533 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-0 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,534 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition test-topic-1-0
2023-07-30 13:48:23,534 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-0 broker=0] Log loaded for partition test-topic-1-0 with initial high watermark 0
2023-07-30 13:48:23,538 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-9, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,539 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-9 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,540 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-9 broker=0] No checkpointed highwatermark is found for partition test-topic-1-9
2023-07-30 13:48:23,540 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-9 broker=0] Log loaded for partition test-topic-1-9 with initial high watermark 0
2023-07-30 13:48:23,546 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-7, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,547 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-7 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,547 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-7 broker=0] No checkpointed highwatermark is found for partition test-topic-1-7
2023-07-30 13:48:23,547 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-7 broker=0] Log loaded for partition test-topic-1-7 with initial high watermark 0
2023-07-30 13:48:23,554 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-8, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,555 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-8 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,556 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-8 broker=0] No checkpointed highwatermark is found for partition test-topic-1-8
2023-07-30 13:48:23,556 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-8 broker=0] Log loaded for partition test-topic-1-8 with initial high watermark 0
2023-07-30 13:48:23,563 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-5, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,564 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-5 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,564 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-5 broker=0] No checkpointed highwatermark is found for partition test-topic-1-5
2023-07-30 13:48:23,564 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-5 broker=0] Log loaded for partition test-topic-1-5 with initial high watermark 0
2023-07-30 13:48:23,576 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-6, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,577 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-6 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,577 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-6 broker=0] No checkpointed highwatermark is found for partition test-topic-1-6
2023-07-30 13:48:23,577 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-6 broker=0] Log loaded for partition test-topic-1-6 with initial high watermark 0
2023-07-30 13:48:23,581 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-3, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,582 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-3 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,582 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-3 broker=0] No checkpointed highwatermark is found for partition test-topic-1-3
2023-07-30 13:48:23,582 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-3 broker=0] Log loaded for partition test-topic-1-3 with initial high watermark 0
2023-07-30 13:48:23,589 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-4, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,590 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-4 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,590 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-4 broker=0] No checkpointed highwatermark is found for partition test-topic-1-4
2023-07-30 13:48:23,590 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-4 broker=0] Log loaded for partition test-topic-1-4 with initial high watermark 0
2023-07-30 13:48:23,597 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-1, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,598 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-1 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,598 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-1 broker=0] No checkpointed highwatermark is found for partition test-topic-1-1
2023-07-30 13:48:23,598 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-1 broker=0] Log loaded for partition test-topic-1-1 with initial high watermark 0
2023-07-30 13:48:23,608 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=test-topic-1-2, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,609 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition test-topic-1-2 in /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,609 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-2 broker=0] No checkpointed highwatermark is found for partition test-topic-1-2
2023-07-30 13:48:23,609 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition test-topic-1-2 broker=0] Log loaded for partition test-topic-1-2 with initial high watermark 0
2023-07-30 13:48:23,618 - INFO  [kafka-admin-client-thread | adminclient-64:AppInfoParser@83] - App info kafka.admin.client for adminclient-64 unregistered
2023-07-30 13:48:23,619 - INFO  [kafka-admin-client-thread | adminclient-64:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,619 - INFO  [kafka-admin-client-thread | adminclient-64:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,619 - INFO  [kafka-admin-client-thread | adminclient-64:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,620 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,621 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,621 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,621 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303621
2023-07-30 13:48:23,625 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1161] - [Worker clientId=connect-2, groupId=backup-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:48:23,625 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1215] - [Worker clientId=connect-2, groupId=backup-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:48:23,625 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1243] - [Worker clientId=connect-2, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:48:23,626 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1161] - [Worker clientId=connect-1, groupId=backup-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:48:23,626 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1215] - [Worker clientId=connect-1, groupId=backup-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:48:23,626 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1243] - [Worker clientId=connect-1, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:48:23,628 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic primary.test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:48:23,637 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(primary.test-topic-1-0)
2023-07-30 13:48:23,640 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-1-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,641 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-1-0 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,641 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-0
2023-07-30 13:48:23,641 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-1-0 broker=0] Log loaded for partition primary.test-topic-1-0 with initial high watermark 0
2023-07-30 13:48:23,647 - INFO  [kafka-admin-client-thread | adminclient-65:AppInfoParser@83] - App info kafka.admin.client for adminclient-65 unregistered
2023-07-30 13:48:23,649 - INFO  [kafka-admin-client-thread | adminclient-65:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,649 - INFO  [kafka-admin-client-thread | adminclient-65:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,649 - INFO  [kafka-admin-client-thread | adminclient-65:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,650 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:23,651 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,651 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,651 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303651
2023-07-30 13:48:23,657 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic heartbeats with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:48:23,665 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(heartbeats-0)
2023-07-30 13:48:23,667 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=heartbeats-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:23,668 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition heartbeats-0 in /tmp/junit5769116175745214285/junit1884222054139287341/heartbeats-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:23,668 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition heartbeats-0 broker=0] No checkpointed highwatermark is found for partition heartbeats-0
2023-07-30 13:48:23,669 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition heartbeats-0 broker=0] Log loaded for partition heartbeats-0 with initial high watermark 0
2023-07-30 13:48:23,673 - INFO  [kafka-admin-client-thread | adminclient-66:AppInfoParser@83] - App info kafka.admin.client for adminclient-66 unregistered
2023-07-30 13:48:23,674 - INFO  [kafka-admin-client-thread | adminclient-66:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,674 - INFO  [kafka-admin-client-thread | adminclient-66:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,674 - INFO  [kafka-admin-client-thread | adminclient-66:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,755 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-consumer-group-dummy-19
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-dummy
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:23,758 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,758 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,759 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303758
2023-07-30 13:48:23,759 - INFO  [main:KafkaConsumer@961] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Subscribed to topic(s): test-topic-1
2023-07-30 13:48:23,763 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:23,764 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Discovered group coordinator localhost:41091 (id: 2147483647 rack: null)
2023-07-30 13:48:23,765 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:48:23,773 - INFO  [main:AbstractCoordinator@468] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:48:23,773 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:48:23,775 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member consumer-consumer-group-dummy-19-ed8f5d11-f386-4d5e-8ef5-426cc5d7619a with group instance id None)
2023-07-30 13:48:23,779 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group consumer-group-dummy generation 1 (__consumer_offsets-35)
2023-07-30 13:48:23,780 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Successfully joined group with generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-19-ed8f5d11-f386-4d5e-8ef5-426cc5d7619a', protocol='range'}
2023-07-30 13:48:23,783 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Finished assignment for group at generation 1: {consumer-consumer-group-dummy-19-ed8f5d11-f386-4d5e-8ef5-426cc5d7619a=Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])}
2023-07-30 13:48:23,790 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group consumer-group-dummy for generation 1
2023-07-30 13:48:23,794 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@750] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Successfully synced group in generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-19-ed8f5d11-f386-4d5e-8ef5-426cc5d7619a', protocol='range'}
2023-07-30 13:48:23,795 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Notifying assignor about the new Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])
2023-07-30 13:48:23,795 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Adding newly assigned partitions: test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:48:23,803 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-3
2023-07-30 13:48:23,803 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-2
2023-07-30 13:48:23,804 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-5
2023-07-30 13:48:23,804 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-4
2023-07-30 13:48:23,804 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-7
2023-07-30 13:48:23,804 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-6
2023-07-30 13:48:23,804 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-9
2023-07-30 13:48:23,804 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-8
2023-07-30 13:48:23,804 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-1
2023-07-30 13:48:23,805 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-0
2023-07-30 13:48:23,807 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,808 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,808 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,808 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,808 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,808 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,809 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,809 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,809 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,809 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,839 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Revoke previously assigned partitions test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:48:23,839 - INFO  [main:AbstractCoordinator@1016] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Member consumer-consumer-group-dummy-19-ed8f5d11-f386-4d5e-8ef5-426cc5d7619a sending LeaveGroup request to coordinator localhost:41091 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:48:23,844 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-consumer-group-dummy-19-ed8f5d11-f386-4d5e-8ef5-426cc5d7619a] in group consumer-group-dummy has left, removing it from the group
2023-07-30 13:48:23,844 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member consumer-consumer-group-dummy-19-ed8f5d11-f386-4d5e-8ef5-426cc5d7619a on LeaveGroup)
2023-07-30 13:48:23,845 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Group consumer-group-dummy with generation 2 is now empty (__consumer_offsets-35)
2023-07-30 13:48:23,850 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,850 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,850 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,851 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-consumer-group-dummy-19 unregistered
2023-07-30 13:48:23,852 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-consumer-group-dummy-20
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-dummy
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:23,854 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:23,854 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:23,854 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739303854
2023-07-30 13:48:23,855 - INFO  [main:KafkaConsumer@961] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Subscribed to topic(s): test-topic-1
2023-07-30 13:48:23,857 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:23,857 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Discovered group coordinator localhost:34453 (id: 2147483647 rack: null)
2023-07-30 13:48:23,858 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:48:23,862 - INFO  [main:AbstractCoordinator@468] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:48:23,862 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:48:23,863 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member consumer-consumer-group-dummy-20-16886e8d-6e10-406b-bbd3-76b33b69f3db with group instance id None)
2023-07-30 13:48:23,865 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group consumer-group-dummy generation 1 (__consumer_offsets-35)
2023-07-30 13:48:23,866 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Successfully joined group with generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-20-16886e8d-6e10-406b-bbd3-76b33b69f3db', protocol='range'}
2023-07-30 13:48:23,867 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Finished assignment for group at generation 1: {consumer-consumer-group-dummy-20-16886e8d-6e10-406b-bbd3-76b33b69f3db=Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])}
2023-07-30 13:48:23,872 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group consumer-group-dummy for generation 1
2023-07-30 13:48:23,873 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@750] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Successfully synced group in generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-20-16886e8d-6e10-406b-bbd3-76b33b69f3db', protocol='range'}
2023-07-30 13:48:23,874 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Notifying assignor about the new Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])
2023-07-30 13:48:23,874 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Adding newly assigned partitions: test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:48:23,875 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-3
2023-07-30 13:48:23,876 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-2
2023-07-30 13:48:23,876 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-5
2023-07-30 13:48:23,876 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-4
2023-07-30 13:48:23,876 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-7
2023-07-30 13:48:23,876 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-6
2023-07-30 13:48:23,876 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-9
2023-07-30 13:48:23,876 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-8
2023-07-30 13:48:23,876 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-1
2023-07-30 13:48:23,877 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-0
2023-07-30 13:48:23,879 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,879 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,880 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,880 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,880 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,880 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,880 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,880 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,880 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,881 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:23,894 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Revoke previously assigned partitions test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:48:23,895 - INFO  [main:AbstractCoordinator@1016] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Member consumer-consumer-group-dummy-20-16886e8d-6e10-406b-bbd3-76b33b69f3db sending LeaveGroup request to coordinator localhost:34453 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:48:23,896 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-consumer-group-dummy-20-16886e8d-6e10-406b-bbd3-76b33b69f3db] in group consumer-group-dummy has left, removing it from the group
2023-07-30 13:48:23,896 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member consumer-consumer-group-dummy-20-16886e8d-6e10-406b-bbd3-76b33b69f3db on LeaveGroup)
2023-07-30 13:48:23,897 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Group consumer-group-dummy with generation 2 is now empty (__consumer_offsets-35)
2023-07-30 13:48:23,898 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:23,898 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:23,899 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:23,900 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-consumer-group-dummy-20 unregistered
2023-07-30 13:48:23,902 - INFO  [main:MirrorConnectorsIntegrationTest@158] - primary REST service: http://localhost:41979/connectors
2023-07-30 13:48:23,902 - INFO  [main:MirrorConnectorsIntegrationTest@159] - backup REST service: http://localhost:44785/connectors
2023-07-30 13:48:23,903 - INFO  [main:MirrorConnectorsIntegrationTest@161] - primary brokers: localhost:41091
2023-07-30 13:48:23,903 - INFO  [main:MirrorConnectorsIntegrationTest@162] - backup brokers: localhost:34453
2023-07-30 13:48:23,903 - INFO  [main:AbstractConfig@361] - MirrorMakerConfig values: 
	clusters = [primary, backup]
	config.providers = []
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:24,033 - INFO  [main:Reflections@239] - Reflections took 128 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:48:24,036 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:48:24,039 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:48:24,041 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:48:24,048 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:48:24,048 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:24,048 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:24,048 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:24,048 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:24,049 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:24,050 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:48:24,050 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:48:24,050 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:48:24,050 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:48:24,050 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:48:24,052 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:24,053 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:24,053 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:24,053 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739304053
2023-07-30 13:48:24,059 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic test-topic-with-empty-partition with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:48:24,076 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(test-topic-with-empty-partition-7, test-topic-with-empty-partition-1, test-topic-with-empty-partition-6, test-topic-with-empty-partition-3, test-topic-with-empty-partition-5, test-topic-with-empty-partition-2, test-topic-with-empty-partition-0, test-topic-with-empty-partition-9, test-topic-with-empty-partition-4, test-topic-with-empty-partition-8)
2023-07-30 13:48:24,079 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-0, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,079 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-0 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,080 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-0 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-0
2023-07-30 13:48:24,080 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-0 broker=0] Log loaded for partition test-topic-with-empty-partition-0 with initial high watermark 0
2023-07-30 13:48:24,083 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-3, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,084 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-3 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,084 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-3 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-3
2023-07-30 13:48:24,085 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-3 broker=0] Log loaded for partition test-topic-with-empty-partition-3 with initial high watermark 0
2023-07-30 13:48:24,092 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-4, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,093 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-4 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,093 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-4 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-4
2023-07-30 13:48:24,093 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-4 broker=0] Log loaded for partition test-topic-with-empty-partition-4 with initial high watermark 0
2023-07-30 13:48:24,101 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-1, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,102 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-1 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,102 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-1 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-1
2023-07-30 13:48:24,102 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-1 broker=0] Log loaded for partition test-topic-with-empty-partition-1 with initial high watermark 0
2023-07-30 13:48:24,109 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-2, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,110 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-2 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,111 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-2 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-2
2023-07-30 13:48:24,111 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-2 broker=0] Log loaded for partition test-topic-with-empty-partition-2 with initial high watermark 0
2023-07-30 13:48:24,118 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-7, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,121 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-7 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,122 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-7 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-7
2023-07-30 13:48:24,122 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-7 broker=0] Log loaded for partition test-topic-with-empty-partition-7 with initial high watermark 0
2023-07-30 13:48:24,129 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-8, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,130 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-8 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,130 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-8 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-8
2023-07-30 13:48:24,130 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-8 broker=0] Log loaded for partition test-topic-with-empty-partition-8 with initial high watermark 0
2023-07-30 13:48:24,136 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-5, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,138 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-5 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,138 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-5 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-5
2023-07-30 13:48:24,138 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-5 broker=0] Log loaded for partition test-topic-with-empty-partition-5 with initial high watermark 0
2023-07-30 13:48:24,145 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-6, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,146 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-6 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,147 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-6 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-6
2023-07-30 13:48:24,147 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-6 broker=0] Log loaded for partition test-topic-with-empty-partition-6 with initial high watermark 0
2023-07-30 13:48:24,153 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=test-topic-with-empty-partition-9, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:24,154 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition test-topic-with-empty-partition-9 in /tmp/junit1823696247009190315/junit2950113589444295251/test-topic-with-empty-partition-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:24,154 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-9 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-9
2023-07-30 13:48:24,154 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition test-topic-with-empty-partition-9 broker=0] Log loaded for partition test-topic-with-empty-partition-9 with initial high watermark 0
2023-07-30 13:48:24,163 - INFO  [kafka-admin-client-thread | adminclient-67:AppInfoParser@83] - App info kafka.admin.client for adminclient-67 unregistered
2023-07-30 13:48:24,164 - INFO  [kafka-admin-client-thread | adminclient-67:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:24,165 - INFO  [kafka-admin-client-thread | adminclient-67:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:24,165 - INFO  [kafka-admin-client-thread | adminclient-67:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:24,218 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-consumer-group-testReplicationWithEmptyPartition-21
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-testReplicationWithEmptyPartition
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:24,221 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:24,221 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:24,221 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739304221
2023-07-30 13:48:24,222 - INFO  [main:KafkaConsumer@961] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Subscribed to topic(s): test-topic-with-empty-partition
2023-07-30 13:48:24,224 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:24,225 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Discovered group coordinator localhost:41091 (id: 2147483647 rack: null)
2023-07-30 13:48:24,226 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] (Re-)joining group
2023-07-30 13:48:24,231 - INFO  [main:AbstractCoordinator@468] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:48:24,231 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] (Re-)joining group
2023-07-30 13:48:24,234 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-testReplicationWithEmptyPartition in state PreparingRebalance with old generation 0 (__consumer_offsets-34) (reason: Adding new member consumer-consumer-group-testReplicationWithEmptyPartition-21-d4e7992f-e484-4aac-aaed-05704bbe87c8 with group instance id None)
2023-07-30 13:48:24,236 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group consumer-group-testReplicationWithEmptyPartition generation 1 (__consumer_offsets-34)
2023-07-30 13:48:24,238 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Successfully joined group with generation Generation{generationId=1, memberId='consumer-consumer-group-testReplicationWithEmptyPartition-21-d4e7992f-e484-4aac-aaed-05704bbe87c8', protocol='range'}
2023-07-30 13:48:24,239 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Finished assignment for group at generation 1: {consumer-consumer-group-testReplicationWithEmptyPartition-21-d4e7992f-e484-4aac-aaed-05704bbe87c8=Assignment(partitions=[test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9])}
2023-07-30 13:48:24,244 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group consumer-group-testReplicationWithEmptyPartition for generation 1
2023-07-30 13:48:24,247 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@750] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Successfully synced group in generation Generation{generationId=1, memberId='consumer-consumer-group-testReplicationWithEmptyPartition-21-d4e7992f-e484-4aac-aaed-05704bbe87c8', protocol='range'}
2023-07-30 13:48:24,247 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Notifying assignor about the new Assignment(partitions=[test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9])
2023-07-30 13:48:24,248 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Adding newly assigned partitions: test-topic-with-empty-partition-3, test-topic-with-empty-partition-2, test-topic-with-empty-partition-5, test-topic-with-empty-partition-4, test-topic-with-empty-partition-1, test-topic-with-empty-partition-0, test-topic-with-empty-partition-7, test-topic-with-empty-partition-6, test-topic-with-empty-partition-9, test-topic-with-empty-partition-8
2023-07-30 13:48:24,249 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-3
2023-07-30 13:48:24,249 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-2
2023-07-30 13:48:24,249 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-5
2023-07-30 13:48:24,249 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-4
2023-07-30 13:48:24,249 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-1
2023-07-30 13:48:24,249 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-0
2023-07-30 13:48:24,250 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-7
2023-07-30 13:48:24,250 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-6
2023-07-30 13:48:24,250 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-9
2023-07-30 13:48:24,250 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-8
2023-07-30 13:48:24,253 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,254 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,254 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,255 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,255 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,255 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,373 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,374 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,374 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,374 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:24,386 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Revoke previously assigned partitions test-topic-with-empty-partition-3, test-topic-with-empty-partition-2, test-topic-with-empty-partition-5, test-topic-with-empty-partition-4, test-topic-with-empty-partition-1, test-topic-with-empty-partition-0, test-topic-with-empty-partition-7, test-topic-with-empty-partition-6, test-topic-with-empty-partition-9, test-topic-with-empty-partition-8
2023-07-30 13:48:24,386 - INFO  [main:AbstractCoordinator@1016] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Member consumer-consumer-group-testReplicationWithEmptyPartition-21-d4e7992f-e484-4aac-aaed-05704bbe87c8 sending LeaveGroup request to coordinator localhost:41091 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:48:24,388 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-consumer-group-testReplicationWithEmptyPartition-21-d4e7992f-e484-4aac-aaed-05704bbe87c8] in group consumer-group-testReplicationWithEmptyPartition has left, removing it from the group
2023-07-30 13:48:24,388 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-testReplicationWithEmptyPartition in state PreparingRebalance with old generation 1 (__consumer_offsets-34) (reason: removing member consumer-consumer-group-testReplicationWithEmptyPartition-21-d4e7992f-e484-4aac-aaed-05704bbe87c8 on LeaveGroup)
2023-07-30 13:48:24,388 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Group consumer-group-testReplicationWithEmptyPartition with generation 2 is now empty (__consumer_offsets-34)
2023-07-30 13:48:24,389 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:24,390 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:24,390 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:24,392 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-consumer-group-testReplicationWithEmptyPartition-21 unregistered
2023-07-30 13:48:26,412 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-5, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:48:26,412 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,412 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,414 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 2 (__consumer_offsets-17)
2023-07-30 13:48:26,414 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:26,415 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:26,415 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:26,419 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 2
2023-07-30 13:48:26,421 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:26,421 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:26,421 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,421 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,421 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:26,422 - WARN  [DistributedHerder-connect-6-1:DistributedHerder@1094] - [Worker clientId=connect-6, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:48:26,422 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,422 - WARN  [DistributedHerder-connect-5-1:DistributedHerder@1094] - [Worker clientId=connect-5, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:48:26,422 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1157] - [Worker clientId=connect-6, groupId=primary-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:48:26,422 - WARN  [DistributedHerder-connect-4-1:DistributedHerder@1094] - [Worker clientId=connect-4, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:48:26,422 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1157] - [Worker clientId=connect-5, groupId=primary-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:48:26,423 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1157] - [Worker clientId=connect-4, groupId=primary-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:48:26,468 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1161] - [Worker clientId=connect-6, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:48:26,468 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:48:26,468 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,469 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1161] - [Worker clientId=connect-5, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:48:26,469 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:48:26,469 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,478 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1161] - [Worker clientId=connect-4, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:48:26,478 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:48:26,478 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,486 - INFO  [pool-25-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:48:26,571 - INFO  [pool-28-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:48:26,575 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorSourceConnector config updated
2023-07-30 13:48:26,575 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorSourceConnector config updated
2023-07-30 13:48:26,575 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorSourceConnector config updated
2023-07-30 13:48:26,575 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,575 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,575 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,576 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,576 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 2 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f during Stable)
2023-07-30 13:48:26,577 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,577 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,578 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 3 (__consumer_offsets-17)
2023-07-30 13:48:26,579 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=3, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:26,580 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=3, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:26,579 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=3, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:26,582 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 3
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=3, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=3, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=2, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=3, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 2
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=2, connectorIds=[MirrorSourceConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=2, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 2
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 2
2023-07-30 13:48:26,586 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,587 - INFO  [StartAndStopExecutor-connect-4-1:DistributedHerder@1298] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connector MirrorSourceConnector
2023-07-30 13:48:26,589 - INFO  [StartAndStopExecutor-connect-4-1:Worker@274] - Creating connector MirrorSourceConnector of type org.apache.kafka.connect.mirror.MirrorSourceConnector
2023-07-30 13:48:26,590 - INFO  [StartAndStopExecutor-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,590 - INFO  [StartAndStopExecutor-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,595 - INFO  [StartAndStopExecutor-connect-4-1:Worker@284] - Instantiated connector MirrorSourceConnector with version 1 of type class org.apache.kafka.connect.mirror.MirrorSourceConnector
2023-07-30 13:48:26,596 - INFO  [StartAndStopExecutor-connect-4-1:Worker@310] - Finished creating connector MirrorSourceConnector
2023-07-30 13:48:26,596 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,597 - INFO  [connector-thread-MirrorSourceConnector:AbstractConfig@361] - MirrorConnectorConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorSourceConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:48:26,599 - INFO  [connector-thread-MirrorSourceConnector:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:26,600 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:26,600 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:26,600 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739306600
2023-07-30 13:48:26,600 - INFO  [connector-thread-MirrorSourceConnector:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:26,601 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:26,601 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:26,601 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739306601
2023-07-30 13:48:26,603 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:26,604 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:26,604 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:26,604 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739306604
2023-07-30 13:48:26,607 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Creating topic mm2-offset-syncs.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:48:26,613 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offset-syncs.backup.internal-0)
2023-07-30 13:48:26,617 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=mm2-offset-syncs.backup.internal-0, dir=/tmp/junit1823696247009190315/junit2950113589444295251] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,618 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition mm2-offset-syncs.backup.internal-0 in /tmp/junit1823696247009190315/junit2950113589444295251/mm2-offset-syncs.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,618 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition mm2-offset-syncs.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-offset-syncs.backup.internal-0
2023-07-30 13:48:26,618 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition mm2-offset-syncs.backup.internal-0 broker=0] Log loaded for partition mm2-offset-syncs.backup.internal-0 with initial high watermark 0
2023-07-30 13:48:26,622 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:TopicAdmin@284] - Created topic (name=mm2-offset-syncs.backup.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:41091
2023-07-30 13:48:26,622 - INFO  [kafka-admin-client-thread | adminclient-70:AppInfoParser@83] - App info kafka.admin.client for adminclient-70 unregistered
2023-07-30 13:48:26,623 - INFO  [main:EmbeddedConnectCluster@689] - PUT response for URL=http://localhost:44785/connectors/MirrorSourceConnector/config is {"name":"MirrorSourceConnector","config":{"connector.class":"org.apache.kafka.connect.mirror.MirrorSourceConnector","offset-syncs.topic.replication.factor":"1","replication.factor":"1","source.cluster.producer.bootstrap.servers":"localhost:41091","sync.topic.acls.enabled":"false","topics":"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*","emit.checkpoints.interval.seconds":"1","heartbeats.topic.replication.factor":"1","source.cluster.alias":"primary","groups":"consumer-group-.*","source.cluster.bootstrap.servers":"localhost:41091","target.cluster.producer.bootstrap.servers":"localhost:34453","enabled":"true","target.cluster.admin.bootstrap.servers":"localhost:34453","target.cluster.alias":"backup","target.cluster.consumer.bootstrap.servers":"localhost:34453","name":"MirrorSourceConnector","target.cluster.bootstrap.servers":"localhost:34453","emit.heartbeats.interval.seconds":"1","checkpoints.topic.replication.factor":"1","source.cluster.admin.bootstrap.servers":"localhost:41091","refresh.groups.interval.seconds":"1","refresh.topics.interval.seconds":"1","source.cluster.consumer.bootstrap.servers":"localhost:41091"},"tasks":[],"type":"source"}
2023-07-30 13:48:26,623 - INFO  [kafka-admin-client-thread | adminclient-70:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:26,623 - INFO  [kafka-admin-client-thread | adminclient-70:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:26,623 - INFO  [kafka-admin-client-thread | adminclient-70:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:26,623 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:Scheduler@95] - creating upstream offset-syncs topic took 20 ms
2023-07-30 13:48:26,631 - INFO  [pool-25-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:48:26,641 - INFO  [Scheduler for MirrorSourceConnector-loading initial set of topic-partitions:Scheduler@95] - loading initial set of topic-partitions took 17 ms
2023-07-30 13:48:26,641 - INFO  [pool-28-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:48:26,645 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorCheckpointConnector config updated
2023-07-30 13:48:26,645 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorCheckpointConnector config updated
2023-07-30 13:48:26,645 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,645 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,645 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,645 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Creating topic primary.test-topic-with-empty-partition with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:48:26,645 - INFO  [Scheduler for MirrorSourceConnector-creating downstream topic-partitions:Scheduler@95] - creating downstream topic-partitions took 4 ms
2023-07-30 13:48:26,645 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,646 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorCheckpointConnector config updated
2023-07-30 13:48:26,646 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 3 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f during Stable)
2023-07-30 13:48:26,646 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,646 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,649 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 4 (__consumer_offsets-17)
2023-07-30 13:48:26,649 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=4, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:26,649 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=4, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:26,649 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=4, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:26,653 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Creating topic primary.heartbeats with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:48:26,653 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 4
2023-07-30 13:48:26,656 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=4, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:26,656 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=4, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:26,656 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=3, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,656 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=3, connectorIds=[MirrorSourceConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,656 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 3
2023-07-30 13:48:26,656 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=4, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:26,656 - INFO  [main:EmbeddedConnectCluster@689] - PUT response for URL=http://localhost:44785/connectors/MirrorCheckpointConnector/config is {"name":"MirrorCheckpointConnector","config":{"connector.class":"org.apache.kafka.connect.mirror.MirrorCheckpointConnector","offset-syncs.topic.replication.factor":"1","replication.factor":"1","source.cluster.producer.bootstrap.servers":"localhost:41091","sync.topic.acls.enabled":"false","topics":"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*","emit.checkpoints.interval.seconds":"1","heartbeats.topic.replication.factor":"1","source.cluster.alias":"primary","groups":"consumer-group-.*","source.cluster.bootstrap.servers":"localhost:41091","target.cluster.producer.bootstrap.servers":"localhost:34453","enabled":"true","target.cluster.admin.bootstrap.servers":"localhost:34453","target.cluster.alias":"backup","target.cluster.consumer.bootstrap.servers":"localhost:34453","name":"MirrorCheckpointConnector","target.cluster.bootstrap.servers":"localhost:34453","emit.heartbeats.interval.seconds":"1","checkpoints.topic.replication.factor":"1","source.cluster.admin.bootstrap.servers":"localhost:41091","refresh.groups.interval.seconds":"1","refresh.topics.interval.seconds":"1","source.cluster.consumer.bootstrap.servers":"localhost:41091"},"tasks":[],"type":"source"}
2023-07-30 13:48:26,656 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 3
2023-07-30 13:48:26,656 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,657 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,656 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=3, connectorIds=[MirrorCheckpointConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,657 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 3
2023-07-30 13:48:26,657 - INFO  [StartAndStopExecutor-connect-5-1:DistributedHerder@1298] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connector MirrorCheckpointConnector
2023-07-30 13:48:26,657 - INFO  [StartAndStopExecutor-connect-5-1:Worker@274] - Creating connector MirrorCheckpointConnector of type org.apache.kafka.connect.mirror.MirrorCheckpointConnector
2023-07-30 13:48:26,658 - INFO  [StartAndStopExecutor-connect-5-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,659 - INFO  [StartAndStopExecutor-connect-5-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,660 - INFO  [StartAndStopExecutor-connect-5-1:Worker@284] - Instantiated connector MirrorCheckpointConnector with version 1 of type class org.apache.kafka.connect.mirror.MirrorCheckpointConnector
2023-07-30 13:48:26,660 - INFO  [StartAndStopExecutor-connect-5-1:Worker@310] - Finished creating connector MirrorCheckpointConnector
2023-07-30 13:48:26,660 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,660 - INFO  [connector-thread-MirrorCheckpointConnector:AbstractConfig@361] - MirrorConnectorConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorCheckpointConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:48:26,662 - INFO  [connector-thread-MirrorCheckpointConnector:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:26,663 - INFO  [connector-thread-MirrorCheckpointConnector:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:26,663 - INFO  [connector-thread-MirrorCheckpointConnector:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:26,663 - INFO  [connector-thread-MirrorCheckpointConnector:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739306663
2023-07-30 13:48:26,663 - INFO  [pool-25-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:48:26,665 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(primary.test-topic-with-empty-partition-3, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-7, primary.test-topic-with-empty-partition-6, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-2, primary.test-topic-with-empty-partition-0, primary.test-topic-with-empty-partition-4, primary.test-topic-with-empty-partition-5, primary.test-topic-with-empty-partition-1)
2023-07-30 13:48:26,666 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:26,666 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:26,666 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:26,666 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739306666
2023-07-30 13:48:26,667 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-9, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,667 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-9 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,668 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-9 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-9
2023-07-30 13:48:26,668 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-9 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-9 with initial high watermark 0
2023-07-30 13:48:26,671 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,673 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-0 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,673 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-0 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-0
2023-07-30 13:48:26,673 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-0 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-0 with initial high watermark 0
2023-07-30 13:48:26,675 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic primary.checkpoints.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:48:26,678 - INFO  [pool-28-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:48:26,678 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-3, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,679 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-3 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,679 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-3 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-3
2023-07-30 13:48:26,679 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-3 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-3 with initial high watermark 0
2023-07-30 13:48:26,681 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorHeartbeatConnector config updated
2023-07-30 13:48:26,681 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorHeartbeatConnector config updated
2023-07-30 13:48:26,681 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,681 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,681 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,681 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,681 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorHeartbeatConnector config updated
2023-07-30 13:48:26,682 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:26,682 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:26,682 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 4 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f during Stable)
2023-07-30 13:48:26,683 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 5 (__consumer_offsets-17)
2023-07-30 13:48:26,684 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=5, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:26,684 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=5, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:26,685 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=5, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:26,689 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 5
2023-07-30 13:48:26,690 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-4, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,691 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=5, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:26,691 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=5, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:26,691 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=5, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:26,691 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=4, connectorIds=[MirrorCheckpointConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,691 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=4, connectorIds=[MirrorSourceConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,691 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-4 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,691 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=4, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:26,691 - INFO  [main:EmbeddedConnectCluster@689] - PUT response for URL=http://localhost:44785/connectors/MirrorHeartbeatConnector/config is {"name":"MirrorHeartbeatConnector","config":{"connector.class":"org.apache.kafka.connect.mirror.MirrorHeartbeatConnector","offset-syncs.topic.replication.factor":"1","replication.factor":"1","source.cluster.producer.bootstrap.servers":"localhost:41091","sync.topic.acls.enabled":"false","topics":"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*","emit.checkpoints.interval.seconds":"1","heartbeats.topic.replication.factor":"1","source.cluster.alias":"primary","groups":"consumer-group-.*","source.cluster.bootstrap.servers":"localhost:41091","target.cluster.producer.bootstrap.servers":"localhost:34453","enabled":"true","target.cluster.admin.bootstrap.servers":"localhost:34453","target.cluster.alias":"backup","target.cluster.consumer.bootstrap.servers":"localhost:34453","name":"MirrorHeartbeatConnector","target.cluster.bootstrap.servers":"localhost:34453","emit.heartbeats.interval.seconds":"1","checkpoints.topic.replication.factor":"1","source.cluster.admin.bootstrap.servers":"localhost:41091","refresh.groups.interval.seconds":"1","refresh.topics.interval.seconds":"1","source.cluster.consumer.bootstrap.servers":"localhost:41091"},"tasks":[],"type":"source"}
2023-07-30 13:48:26,691 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-4 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-4
2023-07-30 13:48:26,691 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 4
2023-07-30 13:48:26,691 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 4
2023-07-30 13:48:26,692 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,692 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-4 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-4 with initial high watermark 0
2023-07-30 13:48:26,691 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 4
2023-07-30 13:48:26,692 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,693 - INFO  [StartAndStopExecutor-connect-6-1:DistributedHerder@1298] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connector MirrorHeartbeatConnector
2023-07-30 13:48:26,693 - INFO  [StartAndStopExecutor-connect-6-1:Worker@274] - Creating connector MirrorHeartbeatConnector of type org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
2023-07-30 13:48:26,693 - INFO  [StartAndStopExecutor-connect-6-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,694 - INFO  [StartAndStopExecutor-connect-6-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,694 - INFO  [StartAndStopExecutor-connect-6-1:Worker@284] - Instantiated connector MirrorHeartbeatConnector with version 1 of type class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
2023-07-30 13:48:26,694 - INFO  [StartAndStopExecutor-connect-6-1:Worker@310] - Finished creating connector MirrorHeartbeatConnector
2023-07-30 13:48:26,694 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:26,694 - INFO  [connector-thread-MirrorHeartbeatConnector:AbstractConfig@361] - MirrorConnectorConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorHeartbeatConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:48:26,695 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-1, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,696 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:26,696 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-1 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,696 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-1 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-1
2023-07-30 13:48:26,696 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-1 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-1 with initial high watermark 0
2023-07-30 13:48:26,697 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:26,697 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:26,697 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739306697
2023-07-30 13:48:26,699 - INFO  [kafka-admin-client-thread | adminclient-73:AppInfoParser@83] - App info kafka.admin.client for adminclient-73 unregistered
2023-07-30 13:48:26,700 - INFO  [kafka-admin-client-thread | adminclient-73:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:26,700 - INFO  [kafka-admin-client-thread | adminclient-73:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:26,700 - INFO  [kafka-admin-client-thread | adminclient-73:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:26,700 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:Scheduler@95] - creating internal topics took 5 ms
2023-07-30 13:48:26,703 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-2, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,704 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-2 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,704 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-2 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-2
2023-07-30 13:48:26,704 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-2 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-2 with initial high watermark 0
2023-07-30 13:48:26,707 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,707 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,712 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-7, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,713 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-7 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,713 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-7 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-7
2023-07-30 13:48:26,713 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-7 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-7 with initial high watermark 0
2023-07-30 13:48:26,713 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
2023-07-30 13:48:26,714 - ERROR [main:EmbeddedConnectClusterAssertions@420] - Could not check connector state info.
org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not read connector state. Error response: {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.connectorStatus(EmbeddedConnectCluster.java:459)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.checkConnectorState(EmbeddedConnectClusterAssertions.java:413)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.lambda$assertConnectorAndAtLeastNumTasksAreRunning$16(EmbeddedConnectClusterAssertions.java:286)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:400)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:448)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:416)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:397)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:387)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.assertConnectorAndAtLeastNumTasksAreRunning(EmbeddedConnectClusterAssertions.java:285)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.waitUntilMirrorMakerIsRunning(MirrorConnectorsIntegrationTest.java:191)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplicationWithEmptyPartition(MirrorConnectorsIntegrationTest.java:367)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2023-07-30 13:48:26,720 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-8, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,721 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-8 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,721 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-8 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-8
2023-07-30 13:48:26,721 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-8 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-8 with initial high watermark 0
2023-07-30 13:48:26,728 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-5, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,728 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-5 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,729 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-5 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-5
2023-07-30 13:48:26,729 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-5 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-5 with initial high watermark 0
2023-07-30 13:48:26,737 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-6, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,737 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-6 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,737 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-6 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-6
2023-07-30 13:48:26,737 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-6 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-6 with initial high watermark 0
2023-07-30 13:48:26,746 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(primary.heartbeats-0)
2023-07-30 13:48:26,747 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=primary.heartbeats-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,747 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition primary.heartbeats-0 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.heartbeats-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,748 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition primary.heartbeats-0 broker=0] No checkpointed highwatermark is found for partition primary.heartbeats-0
2023-07-30 13:48:26,748 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition primary.heartbeats-0 broker=0] Log loaded for partition primary.heartbeats-0 with initial high watermark 0
2023-07-30 13:48:26,754 - INFO  [kafka-admin-client-thread | adminclient-69:MirrorSourceConnector@308] - Created remote topic primary.test-topic-with-empty-partition with 10 partitions.
2023-07-30 13:48:26,754 - INFO  [kafka-admin-client-thread | adminclient-69:MirrorSourceConnector@308] - Created remote topic primary.heartbeats with 1 partitions.
2023-07-30 13:48:26,755 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(primary.checkpoints.internal-0)
2023-07-30 13:48:26,756 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.checkpoints.internal-0, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,756 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.checkpoints.internal-0 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.checkpoints.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,757 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.checkpoints.internal-0 broker=0] No checkpointed highwatermark is found for partition primary.checkpoints.internal-0
2023-07-30 13:48:26,757 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.checkpoints.internal-0 broker=0] Log loaded for partition primary.checkpoints.internal-0 with initial high watermark 0
2023-07-30 13:48:26,762 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:TopicAdmin@284] - Created topic (name=primary.checkpoints.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:34453
2023-07-30 13:48:26,763 - INFO  [kafka-admin-client-thread | adminclient-72:AppInfoParser@83] - App info kafka.admin.client for adminclient-72 unregistered
2023-07-30 13:48:26,763 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating 9 partitions for 'primary.test-topic-1' with the following replica assignment: HashMap(1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=)).
2023-07-30 13:48:26,763 - INFO  [kafka-admin-client-thread | adminclient-72:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:26,763 - INFO  [kafka-admin-client-thread | adminclient-72:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:26,763 - INFO  [kafka-admin-client-thread | adminclient-72:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:26,763 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:Scheduler@95] - creating internal topics took 98 ms
2023-07-30 13:48:26,771 - INFO  [Scheduler for MirrorCheckpointConnector-loading initial consumer groups:Scheduler@95] - loading initial consumer groups took 7 ms
2023-07-30 13:48:26,772 - INFO  [connector-thread-MirrorCheckpointConnector:MirrorCheckpointConnector@79] - Started MirrorCheckpointConnector with 2 consumer groups.
2023-07-30 13:48:26,773 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(primary.test-topic-1-1, primary.test-topic-1-8, primary.test-topic-1-4, primary.test-topic-1-7, primary.test-topic-1-3, primary.test-topic-1-2, primary.test-topic-1-6, primary.test-topic-1-5, primary.test-topic-1-9)
2023-07-30 13:48:26,775 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=primary.test-topic-1-4, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,776 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition primary.test-topic-1-4 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,778 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-4 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-4
2023-07-30 13:48:26,778 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-4 broker=0] Log loaded for partition primary.test-topic-1-4 with initial high watermark 0
2023-07-30 13:48:26,781 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=primary.test-topic-1-3, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,781 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition primary.test-topic-1-3 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,781 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-3 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-3
2023-07-30 13:48:26,781 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-3 broker=0] Log loaded for partition primary.test-topic-1-3 with initial high watermark 0
2023-07-30 13:48:26,789 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=primary.test-topic-1-2, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,789 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition primary.test-topic-1-2 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,789 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-2 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-2
2023-07-30 13:48:26,790 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-2 broker=0] Log loaded for partition primary.test-topic-1-2 with initial high watermark 0
2023-07-30 13:48:26,797 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=primary.test-topic-1-1, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,798 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition primary.test-topic-1-1 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,798 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-1 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-1
2023-07-30 13:48:26,798 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-1 broker=0] Log loaded for partition primary.test-topic-1-1 with initial high watermark 0
2023-07-30 13:48:26,806 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=primary.test-topic-1-9, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,806 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition primary.test-topic-1-9 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,806 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-9 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-9
2023-07-30 13:48:26,806 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-9 broker=0] Log loaded for partition primary.test-topic-1-9 with initial high watermark 0
2023-07-30 13:48:26,814 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=primary.test-topic-1-8, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,815 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition primary.test-topic-1-8 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,815 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-8 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-8
2023-07-30 13:48:26,815 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-8 broker=0] Log loaded for partition primary.test-topic-1-8 with initial high watermark 0
2023-07-30 13:48:26,820 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
2023-07-30 13:48:26,821 - ERROR [main:EmbeddedConnectClusterAssertions@420] - Could not check connector state info.
org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not read connector state. Error response: {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.connectorStatus(EmbeddedConnectCluster.java:459)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.checkConnectorState(EmbeddedConnectClusterAssertions.java:413)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.lambda$assertConnectorAndAtLeastNumTasksAreRunning$16(EmbeddedConnectClusterAssertions.java:286)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:400)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:448)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:416)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:397)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:387)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.assertConnectorAndAtLeastNumTasksAreRunning(EmbeddedConnectClusterAssertions.java:285)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.waitUntilMirrorMakerIsRunning(MirrorConnectorsIntegrationTest.java:191)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplicationWithEmptyPartition(MirrorConnectorsIntegrationTest.java:367)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2023-07-30 13:48:26,822 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=primary.test-topic-1-7, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,823 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition primary.test-topic-1-7 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,823 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-7 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-7
2023-07-30 13:48:26,823 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-7 broker=0] Log loaded for partition primary.test-topic-1-7 with initial high watermark 0
2023-07-30 13:48:26,831 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=primary.test-topic-1-6, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,831 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition primary.test-topic-1-6 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,831 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-6 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-6
2023-07-30 13:48:26,831 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-6 broker=0] Log loaded for partition primary.test-topic-1-6 with initial high watermark 0
2023-07-30 13:48:26,839 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=primary.test-topic-1-5, dir=/tmp/junit5769116175745214285/junit1884222054139287341] Loading producer state till offset 0 with message format version 2
2023-07-30 13:48:26,840 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition primary.test-topic-1-5 in /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:48:26,840 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-5 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-5
2023-07-30 13:48:26,840 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition primary.test-topic-1-5 broker=0] Log loaded for partition primary.test-topic-1-5 with initial high watermark 0
2023-07-30 13:48:26,850 - INFO  [kafka-admin-client-thread | adminclient-69:MirrorSourceConnector@317] - Increased size of primary.test-topic-1 to 10 partitions.
2023-07-30 13:48:26,852 - INFO  [Scheduler for MirrorSourceConnector-refreshing known target topics:Scheduler@95] - refreshing known target topics took 206 ms
2023-07-30 13:48:26,853 - INFO  [connector-thread-MirrorSourceConnector:MirrorSourceConnector@126] - Started MirrorSourceConnector with 21 topic-partitions.
2023-07-30 13:48:26,854 - INFO  [connector-thread-MirrorSourceConnector:MirrorSourceConnector@127] - Starting MirrorSourceConnector took 257 ms.
2023-07-30 13:48:26,856 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,857 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:26,862 - INFO  [Scheduler for MirrorSourceConnector-syncing topic configs:Scheduler@95] - syncing topic configs took 9 ms
2023-07-30 13:48:26,868 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Admin Manager on Broker 0]: Updating topic primary.test-topic-with-empty-partition with new configuration kafka.server.KafkaConfig@76cb2d6c
2023-07-30 13:48:26,871 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Admin Manager on Broker 0]: Updating topic primary.test-topic-1 with new configuration kafka.server.KafkaConfig@76cb2d6c
2023-07-30 13:48:26,872 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing notification(s) to /config/changes
2023-07-30 13:48:26,873 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Admin Manager on Broker 0]: Updating topic primary.heartbeats with new configuration kafka.server.KafkaConfig@76cb2d6c
2023-07-30 13:48:26,874 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing override for entityPath: topics/primary.test-topic-with-empty-partition with config: HashMap()
2023-07-30 13:48:26,880 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing notification(s) to /config/changes
2023-07-30 13:48:26,881 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing override for entityPath: topics/primary.test-topic-1 with config: HashMap()
2023-07-30 13:48:26,882 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing override for entityPath: topics/primary.heartbeats with config: HashMap()
2023-07-30 13:48:26,927 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,033 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,138 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,187 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-6, groupId=primary-mm2] Tasks [MirrorHeartbeatConnector-0] configs updated
2023-07-30 13:48:27,187 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-4, groupId=primary-mm2] Tasks [MirrorHeartbeatConnector-0] configs updated
2023-07-30 13:48:27,187 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-5, groupId=primary-mm2] Tasks [MirrorHeartbeatConnector-0] configs updated
2023-07-30 13:48:27,187 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@646] - [Worker clientId=connect-4, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:48:27,187 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@646] - [Worker clientId=connect-6, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:48:27,187 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@646] - [Worker clientId=connect-5, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:48:27,188 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:27,187 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:27,188 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:27,188 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:27,188 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:27,188 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:27,188 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 5 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f during Stable)
2023-07-30 13:48:27,191 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 6 (__consumer_offsets-17)
2023-07-30 13:48:27,191 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=6, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:27,191 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=6, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:27,192 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=6, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:27,193 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 6
2023-07-30 13:48:27,194 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=6, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:27,194 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=6, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:27,194 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=6, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:27,194 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=6, connectorIds=[MirrorCheckpointConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:27,194 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=6, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:27,194 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=6, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:27,195 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 6
2023-07-30 13:48:27,195 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 6
2023-07-30 13:48:27,195 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:27,195 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 6
2023-07-30 13:48:27,195 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:27,195 - INFO  [StartAndStopExecutor-connect-4-2:DistributedHerder@1257] - [Worker clientId=connect-4, groupId=primary-mm2] Starting task MirrorHeartbeatConnector-0
2023-07-30 13:48:27,195 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:27,195 - INFO  [StartAndStopExecutor-connect-4-2:Worker@509] - Creating task MirrorHeartbeatConnector-0
2023-07-30 13:48:27,196 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:27,196 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:48:27,196 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:48:27,197 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorHeartbeatTask

2023-07-30 13:48:27,197 - INFO  [StartAndStopExecutor-connect-4-2:Worker@524] - Instantiated task MirrorHeartbeatConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorHeartbeatTask
2023-07-30 13:48:27,197 - INFO  [StartAndStopExecutor-connect-4-2:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorHeartbeatConnector-0 using the worker config
2023-07-30 13:48:27,197 - INFO  [StartAndStopExecutor-connect-4-2:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorHeartbeatConnector-0 using the worker config
2023-07-30 13:48:27,197 - INFO  [StartAndStopExecutor-connect-4-2:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorHeartbeatConnector-0 using the worker config
2023-07-30 13:48:27,200 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:27,200 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:27,201 - INFO  [StartAndStopExecutor-connect-4-2:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:48:27,202 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorHeartbeatConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:27,204 - WARN  [StartAndStopExecutor-connect-4-2:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:27,204 - WARN  [StartAndStopExecutor-connect-4-2:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:27,204 - INFO  [StartAndStopExecutor-connect-4-2:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:27,206 - INFO  [StartAndStopExecutor-connect-4-2:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:27,206 - INFO  [StartAndStopExecutor-connect-4-2:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739307204
2023-07-30 13:48:27,207 - INFO  [kafka-producer-network-thread | connector-producer-MirrorHeartbeatConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorHeartbeatConnector-0] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:27,213 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:27,214 - INFO  [task-thread-MirrorHeartbeatConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorHeartbeatConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = null
	task.assigned.partitions = null
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:48:27,214 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@233] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} Source task finished initialization and start
2023-07-30 13:48:27,244 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,349 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,453 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,558 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,663 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,692 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-6, groupId=primary-mm2] Tasks [MirrorCheckpointConnector-0] configs updated
2023-07-30 13:48:27,692 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-4, groupId=primary-mm2] Tasks [MirrorCheckpointConnector-0] configs updated
2023-07-30 13:48:27,692 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@646] - [Worker clientId=connect-6, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:48:27,692 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:27,692 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@646] - [Worker clientId=connect-4, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:48:27,692 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:27,692 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:27,692 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:27,692 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-5, groupId=primary-mm2] Tasks [MirrorCheckpointConnector-0] configs updated
2023-07-30 13:48:27,693 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 6 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f during Stable)
2023-07-30 13:48:27,768 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,773 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:48:27,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:48:27,858 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 5 ms
2023-07-30 13:48:27,873 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:27,978 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:28,083 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:28,188 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:28,197 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-6, groupId=primary-mm2] Tasks [MirrorSourceConnector-0] configs updated
2023-07-30 13:48:28,198 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-4, groupId=primary-mm2] Tasks [MirrorSourceConnector-0] configs updated
2023-07-30 13:48:28,198 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-5, groupId=primary-mm2] Tasks [MirrorSourceConnector-0] configs updated
2023-07-30 13:48:28,198 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@646] - [Worker clientId=connect-5, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:48:28,198 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:28,198 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:28,200 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 7 (__consumer_offsets-17)
2023-07-30 13:48:28,201 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=7, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:28,201 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=7, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:28,201 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=7, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:28,202 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 7
2023-07-30 13:48:28,203 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=7, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:28,203 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=7, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:28,203 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=10, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:28,203 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=10, connectorIds=[MirrorCheckpointConnector], taskIds=[MirrorCheckpointConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:28,203 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=7, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:28,203 - WARN  [DistributedHerder-connect-6-1:DistributedHerder@1094] - [Worker clientId=connect-6, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:48:28,204 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:48:28,204 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=10, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:28,204 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1157] - [Worker clientId=connect-6, groupId=primary-mm2] Current config state offset 8 is behind group assignment 10, reading to end of config log
2023-07-30 13:48:28,204 - WARN  [DistributedHerder-connect-4-1:DistributedHerder@1094] - [Worker clientId=connect-4, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:48:28,204 - INFO  [StartAndStopExecutor-connect-5-2:DistributedHerder@1257] - [Worker clientId=connect-5, groupId=primary-mm2] Starting task MirrorCheckpointConnector-0
2023-07-30 13:48:28,204 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1157] - [Worker clientId=connect-4, groupId=primary-mm2] Current config state offset 8 is behind group assignment 10, reading to end of config log
2023-07-30 13:48:28,204 - INFO  [StartAndStopExecutor-connect-5-2:Worker@509] - Creating task MirrorCheckpointConnector-0
2023-07-30 13:48:28,205 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:48:28,205 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:48:28,206 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorCheckpointTask

2023-07-30 13:48:28,207 - INFO  [StartAndStopExecutor-connect-5-2:Worker@524] - Instantiated task MirrorCheckpointConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorCheckpointTask
2023-07-30 13:48:28,207 - INFO  [StartAndStopExecutor-connect-5-2:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorCheckpointConnector-0 using the worker config
2023-07-30 13:48:28,207 - INFO  [StartAndStopExecutor-connect-5-2:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorCheckpointConnector-0 using the worker config
2023-07-30 13:48:28,207 - INFO  [StartAndStopExecutor-connect-5-2:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorCheckpointConnector-0 using the worker config
2023-07-30 13:48:28,207 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:28,207 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:28,207 - INFO  [StartAndStopExecutor-connect-5-2:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:48:28,207 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorCheckpointConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:28,209 - WARN  [StartAndStopExecutor-connect-5-2:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:28,209 - WARN  [StartAndStopExecutor-connect-5-2:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:28,209 - INFO  [StartAndStopExecutor-connect-5-2:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,209 - INFO  [StartAndStopExecutor-connect-5-2:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,209 - INFO  [StartAndStopExecutor-connect-5-2:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308209
2023-07-30 13:48:28,210 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:28,212 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorCheckpointConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = [consumer-group-testReplicationWithEmptyPartition, consumer-group-dummy]
	task.assigned.partitions = null
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:48:28,212 - INFO  [kafka-producer-network-thread | connector-producer-MirrorCheckpointConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorCheckpointConnector-0] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:28,213 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-22
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:28,214 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,214 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,214 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308214
2023-07-30 13:48:28,214 - INFO  [task-thread-MirrorCheckpointConnector-0:KafkaConsumer@1116] - [Consumer clientId=consumer-null-22, groupId=null] Subscribed to partition(s): mm2-offset-syncs.backup.internal-0
2023-07-30 13:48:28,214 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,215 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,215 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,215 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308215
2023-07-30 13:48:28,215 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,216 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,216 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,216 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308216
2023-07-30 13:48:28,219 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@233] - WorkerSourceTask{id=MirrorCheckpointConnector-0} Source task finished initialization and start
2023-07-30 13:48:28,221 - INFO  [task-thread-MirrorCheckpointConnector-0:Metadata@279] - [Consumer clientId=consumer-null-22, groupId=null] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:28,223 - INFO  [task-thread-MirrorCheckpointConnector-0:SubscriptionState@396] - [Consumer clientId=consumer-null-22, groupId=null] Resetting offset for partition mm2-offset-syncs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41091 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:28,293 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:28,397 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:28,503 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:28,610 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[],"type":"source"}
2023-07-30 13:48:28,698 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1161] - [Worker clientId=connect-4, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 10
2023-07-30 13:48:28,698 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1161] - [Worker clientId=connect-6, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 10
2023-07-30 13:48:28,698 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:48:28,698 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:48:28,698 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:28,699 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:28,699 - INFO  [StartAndStopExecutor-connect-6-2:DistributedHerder@1257] - [Worker clientId=connect-6, groupId=primary-mm2] Starting task MirrorSourceConnector-0
2023-07-30 13:48:28,699 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:28,699 - INFO  [StartAndStopExecutor-connect-6-2:Worker@509] - Creating task MirrorSourceConnector-0
2023-07-30 13:48:28,699 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@646] - [Worker clientId=connect-4, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:48:28,699 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:48:28,699 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:28,699 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:48:28,699 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:28,700 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorSourceTask

2023-07-30 13:48:28,700 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 7 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-961f724e-0581-4748-bfbb-288e704339ea during Stable)
2023-07-30 13:48:28,700 - INFO  [StartAndStopExecutor-connect-6-2:Worker@524] - Instantiated task MirrorSourceConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorSourceTask
2023-07-30 13:48:28,700 - INFO  [StartAndStopExecutor-connect-6-2:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:48:28,700 - INFO  [StartAndStopExecutor-connect-6-2:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:48:28,700 - INFO  [StartAndStopExecutor-connect-6-2:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:48:28,701 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:28,701 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:28,701 - INFO  [StartAndStopExecutor-connect-6-2:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:48:28,701 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorSourceConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:28,702 - WARN  [StartAndStopExecutor-connect-6-2:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:28,702 - WARN  [StartAndStopExecutor-connect-6-2:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:28,702 - INFO  [StartAndStopExecutor-connect-6-2:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,703 - INFO  [StartAndStopExecutor-connect-6-2:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,703 - INFO  [StartAndStopExecutor-connect-6-2:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308702
2023-07-30 13:48:28,705 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:28,705 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@646] - [Worker clientId=connect-6, groupId=primary-mm2] Handling task config update by restarting tasks [MirrorSourceConnector-0]
2023-07-30 13:48:28,706 - INFO  [DistributedHerder-connect-6-1:Worker@836] - Stopping task MirrorSourceConnector-0
2023-07-30 13:48:28,706 - INFO  [kafka-producer-network-thread | connector-producer-MirrorSourceConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:28,707 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorSourceConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = null
	task.assigned.partitions = [test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9, test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9, heartbeats-0]
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:48:28,709 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-23
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:28,710 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,710 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,710 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308710
2023-07-30 13:48:28,711 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:28,712 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,712 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,712 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308712
2023-07-30 13:48:28,713 - INFO  [kafka-producer-network-thread | producer-21:Metadata@279] - [Producer clientId=producer-21] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:28,716 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:44785"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"localhost:37125"}],"type":"source"}
2023-07-30 13:48:28,721 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorHeartbeatConnector/status is {"name":"MirrorHeartbeatConnector","connector":{"state":"RUNNING","worker_id":"localhost:37125"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"localhost:44785"}],"type":"source"}
2023-07-30 13:48:28,725 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors/MirrorCheckpointConnector/status is {"name":"MirrorCheckpointConnector","connector":{"state":"RUNNING","worker_id":"localhost:35973"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"localhost:35973"}],"type":"source"}
2023-07-30 13:48:28,728 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,729 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,729 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,729 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308729
2023-07-30 13:48:28,731 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,732 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,732 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:48:28,733 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:48:28,734 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:28,734 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:28,734 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:48:28,734 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:48:28,734 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:28,734 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,734 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,734 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308734
2023-07-30 13:48:28,735 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-24
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:28,735 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,736 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,736 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308735
2023-07-30 13:48:28,736 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-24, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:48:28,736 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-24, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:48:28,737 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-24, groupId=null] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:28,739 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-24, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:28,739 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:48:28,739 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:28,740 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:28,740 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:28,740 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-24 unregistered
2023-07-30 13:48:28,740 - INFO  [kafka-admin-client-thread | adminclient-77:AppInfoParser@83] - App info kafka.admin.client for adminclient-77 unregistered
2023-07-30 13:48:28,741 - INFO  [kafka-admin-client-thread | adminclient-77:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:28,741 - INFO  [kafka-admin-client-thread | adminclient-77:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:28,741 - INFO  [kafka-admin-client-thread | adminclient-77:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:28,773 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:48:28,845 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,845 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,845 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,846 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:48:28,846 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:28,846 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:48:28,846 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:48:28,846 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:48:28,846 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:28,847 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,847 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,848 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308847
2023-07-30 13:48:28,848 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-25
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:28,849 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,849 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,849 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308849
2023-07-30 13:48:28,849 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-25, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:48:28,849 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-25, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:48:28,851 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-25, groupId=null] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:28,852 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-25, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:28,852 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:48:28,852 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:28,852 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:28,853 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:28,853 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-25 unregistered
2023-07-30 13:48:28,853 - INFO  [kafka-admin-client-thread | adminclient-78:AppInfoParser@83] - App info kafka.admin.client for adminclient-78 unregistered
2023-07-30 13:48:28,854 - INFO  [kafka-admin-client-thread | adminclient-78:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:28,854 - INFO  [kafka-admin-client-thread | adminclient-78:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:28,854 - INFO  [kafka-admin-client-thread | adminclient-78:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:28,856 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:48:28,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:48:28,956 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,956 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,956 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:28,957 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:48:28,958 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:28,958 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:48:28,958 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:48:28,958 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:48:28,958 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:28,958 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:48:28,959 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:48:28,959 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:48:28,959 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:48:28,959 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:28,959 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:28,959 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:48:28,959 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:48:28,960 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:28,960 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,960 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,960 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308960
2023-07-30 13:48:28,961 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-26
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:28,962 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:28,962 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:28,963 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739308962
2023-07-30 13:48:28,963 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-26, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:48:28,963 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-26, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:48:28,965 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-26, groupId=null] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:28,966 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-26, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:28,966 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:48:28,967 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:28,967 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:28,967 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:28,967 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-26 unregistered
2023-07-30 13:48:28,968 - INFO  [kafka-admin-client-thread | adminclient-79:AppInfoParser@83] - App info kafka.admin.client for adminclient-79 unregistered
2023-07-30 13:48:28,968 - INFO  [kafka-admin-client-thread | adminclient-79:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:28,968 - INFO  [kafka-admin-client-thread | adminclient-79:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:28,968 - INFO  [kafka-admin-client-thread | adminclient-79:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:29,069 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:29,070 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:29,070 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:29,071 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:48:29,071 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:29,072 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:48:29,072 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:48:29,072 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:48:29,072 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:29,072 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:48:29,072 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:48:29,072 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:48:29,073 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:48:29,073 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:29,073 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:29,073 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:48:29,073 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:48:29,073 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:29,074 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:29,074 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:29,074 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739309074
2023-07-30 13:48:29,075 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-27
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:29,076 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:29,076 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:29,076 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739309076
2023-07-30 13:48:29,076 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-27, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:48:29,076 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-27, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:48:29,079 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-27, groupId=null] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:29,080 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-27, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:29,080 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:48:29,081 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:29,081 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:29,081 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:29,081 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-27 unregistered
2023-07-30 13:48:29,082 - INFO  [kafka-admin-client-thread | adminclient-80:AppInfoParser@83] - App info kafka.admin.client for adminclient-80 unregistered
2023-07-30 13:48:29,082 - INFO  [kafka-admin-client-thread | adminclient-80:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:29,082 - INFO  [kafka-admin-client-thread | adminclient-80:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:29,082 - INFO  [kafka-admin-client-thread | adminclient-80:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:29,184 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:29,184 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:29,184 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:29,185 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:48:29,185 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:29,185 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:48:29,185 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:48:29,185 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:48:29,185 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:29,186 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:29,186 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:29,187 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739309186
2023-07-30 13:48:29,187 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-28
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:29,188 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:29,188 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:29,188 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739309188
2023-07-30 13:48:29,189 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-28, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:48:29,189 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-28, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:48:29,191 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-28, groupId=null] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:29,193 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-28, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:29,193 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:48:29,193 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:29,193 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:29,193 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:29,194 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-28 unregistered
2023-07-30 13:48:29,194 - INFO  [kafka-admin-client-thread | adminclient-81:AppInfoParser@83] - App info kafka.admin.client for adminclient-81 unregistered
2023-07-30 13:48:29,195 - INFO  [kafka-admin-client-thread | adminclient-81:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:29,195 - INFO  [kafka-admin-client-thread | adminclient-81:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:29,195 - INFO  [kafka-admin-client-thread | adminclient-81:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:29,296 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:29,297 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:34453
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:29,297 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:29,298 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:48:29,298 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:29,298 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:48:29,298 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:48:29,298 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:48:29,298 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:48:29,298 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:48:29,299 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:48:29,299 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:48:29,299 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:48:29,299 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:29,299 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:29,299 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:48:29,299 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:48:29,299 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:48:29,299 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:29,300 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:29,300 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739309299
2023-07-30 13:48:29,300 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:34453]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-29
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:29,301 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:29,301 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:29,301 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739309301
2023-07-30 13:48:29,301 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-29, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:48:29,302 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-29, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:48:29,303 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-29, groupId=null] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:29,305 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-29, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34453 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:48:29,773 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:48:29,809 - INFO  [main:MirrorClient@177] - Consumed 10 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:48:29,809 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:29,810 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:29,810 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:29,810 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-29 unregistered
2023-07-30 13:48:29,811 - INFO  [kafka-admin-client-thread | adminclient-82:AppInfoParser@83] - App info kafka.admin.client for adminclient-82 unregistered
2023-07-30 13:48:29,811 - INFO  [kafka-admin-client-thread | adminclient-82:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:29,811 - INFO  [kafka-admin-client-thread | adminclient-82:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:29,811 - INFO  [kafka-admin-client-thread | adminclient-82:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:29,824 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:41979/connectors is []
2023-07-30 13:48:29,830 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:44785/connectors is ["MirrorCheckpointConnector","MirrorSourceConnector","MirrorHeartbeatConnector"]
2023-07-30 13:48:29,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:48:29,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:48:30,773 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:48:30,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:48:30,858 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 5 ms
2023-07-30 13:48:31,202 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-5, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:48:31,202 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:31,202 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:31,308 - INFO  [kafka-coordinator-heartbeat-thread | primary-mm2:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-6, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:48:31,773 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:48:31,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:48:31,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:48:32,773 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:48:32,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:48:32,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:48:33,707 - ERROR [DistributedHerder-connect-6-1:Worker@867] - Graceful stop of task MirrorSourceConnector-0 failed.
2023-07-30 13:48:33,709 - ERROR [task-thread-MirrorSourceConnector-0:OffsetStorageReaderImpl@113] - Failed to fetch offsets from namespace MirrorSourceConnector: 
org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-07-30 13:48:33,710 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorSourceConnector-0} Committing offsets
2023-07-30 13:48:33,710 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorSourceConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:48:33,710 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@187] - WorkerSourceTask{id=MirrorSourceConnector-0} Task threw an uncaught and unrecoverable exception
org.apache.kafka.connect.errors.ConnectException: Failed to fetch offsets.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:114)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	... 21 more
2023-07-30 13:48:33,711 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@188] - WorkerSourceTask{id=MirrorSourceConnector-0} Task is being killed and will not recover until manually restarted
2023-07-30 13:48:33,711 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:33,711 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:33,711 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:33,711 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:33,711 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:33,711 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.consumer for consumer-null-23 unregistered
2023-07-30 13:48:33,711 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=producer-21] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:33,713 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 8 (__consumer_offsets-17)
2023-07-30 13:48:33,713 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:33,713 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:33,713 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:33,713 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for producer-21 unregistered
2023-07-30 13:48:33,713 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:33,713 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:33,714 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=8, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:33,714 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=8, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:33,713 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=8, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:33,714 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:33,714 - INFO  [task-thread-MirrorSourceConnector-0:MirrorSourceTask@120] - Stopping task-thread-MirrorSourceConnector-0 took 3 ms.
2023-07-30 13:48:33,714 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:48:33,715 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:33,715 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 8
2023-07-30 13:48:33,715 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:33,715 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:33,715 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorSourceConnector-0 unregistered
2023-07-30 13:48:33,716 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=8, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=8, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=8, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=10, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=10, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=10, connectorIds=[MirrorCheckpointConnector], taskIds=[MirrorCheckpointConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:48:33,717 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:33,718 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,718 - INFO  [StartAndStopExecutor-connect-6-3:DistributedHerder@1257] - [Worker clientId=connect-6, groupId=primary-mm2] Starting task MirrorSourceConnector-0
2023-07-30 13:48:33,718 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,718 - INFO  [StartAndStopExecutor-connect-6-3:Worker@509] - Creating task MirrorSourceConnector-0
2023-07-30 13:48:33,718 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:48:33,718 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,718 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:48:33,718 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,719 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorSourceTask

2023-07-30 13:48:33,719 - INFO  [StartAndStopExecutor-connect-6-3:Worker@524] - Instantiated task MirrorSourceConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorSourceTask
2023-07-30 13:48:33,719 - INFO  [StartAndStopExecutor-connect-6-3:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:48:33,719 - INFO  [StartAndStopExecutor-connect-6-3:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:48:33,719 - INFO  [StartAndStopExecutor-connect-6-3:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:48:33,719 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,720 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,720 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,720 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,721 - INFO  [StartAndStopExecutor-connect-6-3:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:48:33,721 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34453]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorSourceConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:33,721 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,721 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,722 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,722 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,726 - WARN  [StartAndStopExecutor-connect-6-3:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:48:33,726 - WARN  [StartAndStopExecutor-connect-6-3:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:48:33,726 - INFO  [StartAndStopExecutor-connect-6-3:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:33,726 - INFO  [StartAndStopExecutor-connect-6-3:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:33,726 - INFO  [StartAndStopExecutor-connect-6-3:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739313726
2023-07-30 13:48:33,727 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:33,732 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorSourceConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = null
	task.assigned.partitions = [test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9, test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9, heartbeats-0]
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:48:33,735 - INFO  [kafka-producer-network-thread | connector-producer-MirrorSourceConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Cluster ID: b371YYDzQHmIPyOz-f-rVg
2023-07-30 13:48:33,744 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41091]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-30
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:48:33,745 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:33,745 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:33,745 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739313745
2023-07-30 13:48:33,746 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:41091]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:48:33,747 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:33,749 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:33,749 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorCheckpointConnector'
2023-07-30 13:48:33,749 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739313747
2023-07-30 13:48:33,750 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorCheckpointConnector'
2023-07-30 13:48:33,750 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorCheckpointConnector'
2023-07-30 13:48:33,750 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorCheckpointConnector config removed
2023-07-30 13:48:33,750 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorCheckpointConnector config removed
2023-07-30 13:48:33,750 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorCheckpointConnector config removed
2023-07-30 13:48:33,750 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:33,750 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:33,751 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@592] - [Worker clientId=connect-5, groupId=primary-mm2] Handling connector-only config update by stopping connector MirrorCheckpointConnector
2023-07-30 13:48:33,751 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:33,751 - INFO  [kafka-producer-network-thread | producer-22:Metadata@279] - [Producer clientId=producer-22] Cluster ID: WWOdeGZnSQSpvKBOAZLAUg
2023-07-30 13:48:33,751 - INFO  [DistributedHerder-connect-5-1:Worker@387] - Stopping connector MirrorCheckpointConnector
2023-07-30 13:48:33,751 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:33,751 - INFO  [DistributedHerder-connect-5-1:WorkerConnector@249] - Scheduled shutdown for WorkerConnector{id=MirrorCheckpointConnector}
2023-07-30 13:48:33,752 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 8 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f during Stable)
2023-07-30 13:48:33,753 - INFO  [kafka-admin-client-thread | adminclient-71:AppInfoParser@83] - App info kafka.admin.client for adminclient-71 unregistered
2023-07-30 13:48:33,754 - INFO  [kafka-admin-client-thread | adminclient-71:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:33,754 - INFO  [kafka-admin-client-thread | adminclient-71:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:33,755 - INFO  [kafka-admin-client-thread | adminclient-71:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:33,756 - INFO  [connector-thread-MirrorCheckpointConnector:WorkerConnector@269] - Completed shutdown for WorkerConnector{id=MirrorCheckpointConnector}
2023-07-30 13:48:33,759 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:33,760 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:33,762 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 9 (__consumer_offsets-17)
2023-07-30 13:48:33,762 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=9, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:33,762 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=9, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:33,763 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=9, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:33,765 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 9
2023-07-30 13:48:33,767 - INFO  [main:EmbeddedConnectCluster@689] - DELETE response for URL=http://localhost:44785/connectors/MirrorCheckpointConnector is empty
2023-07-30 13:48:33,767 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=9, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:33,767 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=9, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:33,767 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=12, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:33,768 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:48:33,768 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:33,769 - INFO  [StartAndStopExecutor-connect-5-3:Worker@387] - Stopping connector MirrorCheckpointConnector
2023-07-30 13:48:33,769 - INFO  [StartAndStopExecutor-connect-5-4:Worker@836] - Stopping task MirrorCheckpointConnector-0
2023-07-30 13:48:33,769 - WARN  [StartAndStopExecutor-connect-5-3:Worker@390] - Ignoring stop request for unowned connector MirrorCheckpointConnector
2023-07-30 13:48:33,771 - WARN  [StartAndStopExecutor-connect-5-3:Worker@415] - Ignoring await stop request for non-present connector MirrorCheckpointConnector
2023-07-30 13:48:33,771 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=9, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:33,771 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=12, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:33,771 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:48:33,771 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:33,858 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:48:33,858 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 5 ms
2023-07-30 13:48:33,859 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:33,859 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:34,252 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorCheckpointConnector-0} Committing offsets
2023-07-30 13:48:34,252 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:34,252 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:34,252 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:34,254 - INFO  [StartAndStopExecutor-connect-5-4:AppInfoParser@83] - App info kafka.consumer for consumer-null-22 unregistered
2023-07-30 13:48:34,255 - INFO  [kafka-admin-client-thread | adminclient-74:AppInfoParser@83] - App info kafka.admin.client for adminclient-74 unregistered
2023-07-30 13:48:34,256 - INFO  [kafka-admin-client-thread | adminclient-74:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:34,256 - INFO  [kafka-admin-client-thread | adminclient-74:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:34,256 - INFO  [kafka-admin-client-thread | adminclient-74:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:34,256 - INFO  [kafka-admin-client-thread | adminclient-75:AppInfoParser@83] - App info kafka.admin.client for adminclient-75 unregistered
2023-07-30 13:48:34,256 - INFO  [kafka-admin-client-thread | adminclient-75:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:34,257 - INFO  [kafka-admin-client-thread | adminclient-75:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:34,257 - INFO  [kafka-admin-client-thread | adminclient-75:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:34,257 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:34,257 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:34,257 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:34,257 - INFO  [StartAndStopExecutor-connect-5-4:MirrorCheckpointTask@121] - Stopping StartAndStopExecutor-connect-5-4 took 488 ms.
2023-07-30 13:48:34,257 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorCheckpointConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:48:34,272 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@574] - WorkerSourceTask{id=MirrorCheckpointConnector-0} Finished commitOffsets successfully in 20 ms
2023-07-30 13:48:34,272 - INFO  [task-thread-MirrorCheckpointConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorCheckpointConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:48:34,273 - INFO  [task-thread-MirrorCheckpointConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:34,273 - INFO  [task-thread-MirrorCheckpointConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:34,274 - INFO  [task-thread-MirrorCheckpointConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:34,274 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorCheckpointConnector-0 unregistered
2023-07-30 13:48:34,278 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-5, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:48:34,282 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-5, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:48:34,282 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[MirrorCheckpointConnector], revokedTaskIds=[MirrorCheckpointConnector-0], delay=0} with rebalance delay: 0
2023-07-30 13:48:34,283 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:48:34,283 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:34,283 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:34,283 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:34,283 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 9 (__consumer_offsets-17) (reason: leader connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7 re-joining group during Stable)
2023-07-30 13:48:34,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:48:34,858 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 5 ms
2023-07-30 13:48:34,858 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:34,858 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:35,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:48:35,857 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:48:35,857 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:35,858 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:48:36,763 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-4, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:48:36,764 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:36,764 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:36,764 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-6, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:48:36,764 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:36,764 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:36,766 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 10 (__consumer_offsets-17)
2023-07-30 13:48:36,766 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=10, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:36,766 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=10, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:36,766 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=10, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:36,768 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 10
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=10, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=10, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=10, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=12, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=12, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:36,769 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:48:36,770 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:36,770 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:36,772 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorSourceConnector'
2023-07-30 13:48:36,772 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorSourceConnector'
2023-07-30 13:48:36,772 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorSourceConnector config removed
2023-07-30 13:48:36,772 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorSourceConnector config removed
2023-07-30 13:48:36,772 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorSourceConnector'
2023-07-30 13:48:36,772 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:36,772 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorSourceConnector config removed
2023-07-30 13:48:36,772 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@592] - [Worker clientId=connect-4, groupId=primary-mm2] Handling connector-only config update by stopping connector MirrorSourceConnector
2023-07-30 13:48:36,772 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:36,772 - INFO  [DistributedHerder-connect-4-1:Worker@387] - Stopping connector MirrorSourceConnector
2023-07-30 13:48:36,772 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:36,772 - INFO  [DistributedHerder-connect-4-1:WorkerConnector@249] - Scheduled shutdown for WorkerConnector{id=MirrorSourceConnector}
2023-07-30 13:48:36,773 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:36,773 - INFO  [kafka-admin-client-thread | adminclient-68:AppInfoParser@83] - App info kafka.admin.client for adminclient-68 unregistered
2023-07-30 13:48:36,773 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 10 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f during Stable)
2023-07-30 13:48:36,775 - INFO  [kafka-admin-client-thread | adminclient-68:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:36,776 - INFO  [kafka-admin-client-thread | adminclient-68:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:36,776 - INFO  [kafka-admin-client-thread | adminclient-68:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:36,777 - INFO  [kafka-admin-client-thread | adminclient-69:AppInfoParser@83] - App info kafka.admin.client for adminclient-69 unregistered
2023-07-30 13:48:36,777 - INFO  [kafka-admin-client-thread | adminclient-69:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:36,777 - INFO  [kafka-admin-client-thread | adminclient-69:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:36,777 - INFO  [kafka-admin-client-thread | adminclient-69:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:36,778 - INFO  [connector-thread-MirrorSourceConnector:MirrorSourceConnector@141] - Stopping MirrorSourceConnector took 4 ms.
2023-07-30 13:48:36,778 - INFO  [connector-thread-MirrorSourceConnector:WorkerConnector@269] - Completed shutdown for WorkerConnector{id=MirrorSourceConnector}
2023-07-30 13:48:36,779 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:36,779 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:36,783 - INFO  [main:EmbeddedConnectCluster@689] - DELETE response for URL=http://localhost:44785/connectors/MirrorSourceConnector is empty
2023-07-30 13:48:36,783 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 11 (__consumer_offsets-17)
2023-07-30 13:48:36,783 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=11, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:36,784 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=11, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:36,784 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=11, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:36,785 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 11
2023-07-30 13:48:36,786 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=11, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:36,786 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=11, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:36,786 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=11, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:36,787 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=14, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:36,787 - INFO  [StartAndStopExecutor-connect-6-4:Worker@836] - Stopping task MirrorSourceConnector-0
2023-07-30 13:48:36,788 - INFO  [StartAndStopExecutor-connect-4-3:Worker@387] - Stopping connector MirrorSourceConnector
2023-07-30 13:48:36,788 - WARN  [StartAndStopExecutor-connect-4-3:Worker@390] - Ignoring stop request for unowned connector MirrorSourceConnector
2023-07-30 13:48:36,788 - WARN  [StartAndStopExecutor-connect-4-3:Worker@415] - Ignoring await stop request for non-present connector MirrorSourceConnector
2023-07-30 13:48:36,788 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-4, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:48:36,788 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:48:36,788 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-4, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:48:36,788 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:36,789 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=14, connectorIds=[], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[MirrorSourceConnector], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:36,789 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:48:36,789 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:36,789 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:36,789 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:36,790 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 11 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-961f724e-0581-4748-bfbb-288e704339ea during Stable)
2023-07-30 13:48:39,784 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-5, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:48:39,784 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:39,785 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:39,890 - INFO  [kafka-coordinator-heartbeat-thread | primary-mm2:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-6, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:48:41,788 - ERROR [StartAndStopExecutor-connect-6-4:Worker@867] - Graceful stop of task MirrorSourceConnector-0 failed.
2023-07-30 13:48:41,788 - ERROR [task-thread-MirrorSourceConnector-0:OffsetStorageReaderImpl@113] - Failed to fetch offsets from namespace MirrorSourceConnector: 
org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-07-30 13:48:41,789 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-6, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:48:41,789 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorSourceConnector-0} Committing offsets
2023-07-30 13:48:41,790 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-6, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:48:41,790 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorSourceConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:48:41,790 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=14, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[MirrorSourceConnector-0], delay=0} with rebalance delay: 0
2023-07-30 13:48:41,790 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@187] - WorkerSourceTask{id=MirrorSourceConnector-0} Task threw an uncaught and unrecoverable exception
org.apache.kafka.connect.errors.ConnectException: Failed to fetch offsets.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:114)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	... 21 more
2023-07-30 13:48:41,790 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:41,790 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:41,790 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@188] - WorkerSourceTask{id=MirrorSourceConnector-0} Task is being killed and will not recover until manually restarted
2023-07-30 13:48:41,790 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,790 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,790 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,791 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 12 (__consumer_offsets-17)
2023-07-30 13:48:41,791 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.consumer for consumer-null-30 unregistered
2023-07-30 13:48:41,791 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=producer-22] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,792 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=12, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:41,792 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=12, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:41,792 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=12, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:41,792 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,792 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,792 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,792 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for producer-22 unregistered
2023-07-30 13:48:41,792 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,792 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,792 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,792 - INFO  [task-thread-MirrorSourceConnector-0:MirrorSourceTask@120] - Stopping task-thread-MirrorSourceConnector-0 took 2 ms.
2023-07-30 13:48:41,792 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:48:41,793 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 12
2023-07-30 13:48:41,793 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,793 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,793 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,793 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorSourceConnector-0 unregistered
2023-07-30 13:48:41,794 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=12, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:41,794 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=12, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:41,794 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=12, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:41,795 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=14, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:41,794 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=14, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:41,795 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=14, connectorIds=[], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:41,795 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:48:41,795 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:48:41,795 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:48:41,795 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:41,795 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:41,795 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:41,803 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorHeartbeatConnector'
2023-07-30 13:48:41,803 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorHeartbeatConnector'
2023-07-30 13:48:41,803 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorHeartbeatConnector'
2023-07-30 13:48:41,803 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorHeartbeatConnector config removed
2023-07-30 13:48:41,803 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorHeartbeatConnector config removed
2023-07-30 13:48:41,803 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorHeartbeatConnector config removed
2023-07-30 13:48:41,803 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:41,803 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:41,803 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@592] - [Worker clientId=connect-6, groupId=primary-mm2] Handling connector-only config update by stopping connector MirrorHeartbeatConnector
2023-07-30 13:48:41,804 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:41,804 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:41,804 - INFO  [DistributedHerder-connect-6-1:Worker@387] - Stopping connector MirrorHeartbeatConnector
2023-07-30 13:48:41,804 - INFO  [DistributedHerder-connect-6-1:WorkerConnector@249] - Scheduled shutdown for WorkerConnector{id=MirrorHeartbeatConnector}
2023-07-30 13:48:41,805 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 12 (__consumer_offsets-17) (reason: leader connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7 re-joining group during Stable)
2023-07-30 13:48:41,806 - INFO  [connector-thread-MirrorHeartbeatConnector:WorkerConnector@269] - Completed shutdown for WorkerConnector{id=MirrorHeartbeatConnector}
2023-07-30 13:48:41,807 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:41,807 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:41,809 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 13 (__consumer_offsets-17)
2023-07-30 13:48:41,809 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=13, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:41,809 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=13, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:41,809 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=13, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:41,810 - INFO  [main:EmbeddedConnectCluster@689] - DELETE response for URL=http://localhost:44785/connectors/MirrorHeartbeatConnector is empty
2023-07-30 13:48:41,810 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 13
2023-07-30 13:48:41,811 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41091]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:41,811 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=13, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:41,811 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=13, memberId='connect-4-961f724e-0581-4748-bfbb-288e704339ea', protocol='sessioned'}
2023-07-30 13:48:41,811 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=13, memberId='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', protocol='sessioned'}
2023-07-30 13:48:41,811 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:41,812 - INFO  [StartAndStopExecutor-connect-4-4:Worker@836] - Stopping task MirrorHeartbeatConnector-0
2023-07-30 13:48:41,812 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:41,812 - INFO  [StartAndStopExecutor-connect-6-5:Worker@387] - Stopping connector MirrorHeartbeatConnector
2023-07-30 13:48:41,812 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} Committing offsets
2023-07-30 13:48:41,812 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:41,812 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739321811
2023-07-30 13:48:41,812 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:48:41,812 - WARN  [StartAndStopExecutor-connect-6-5:Worker@390] - Ignoring stop request for unowned connector MirrorHeartbeatConnector
2023-07-30 13:48:41,812 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:48:41,812 - WARN  [StartAndStopExecutor-connect-6-5:Worker@415] - Ignoring await stop request for non-present connector MirrorHeartbeatConnector
2023-07-30 13:48:41,812 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:41,812 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-6, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:48:41,814 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-6, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:48:41,814 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[MirrorHeartbeatConnector], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:41,814 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:48:41,814 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:41,814 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:41,814 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:41,816 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 13 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f during Stable)
2023-07-30 13:48:41,817 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@574] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} Finished commitOffsets successfully in 5 ms
2023-07-30 13:48:41,817 - INFO  [task-thread-MirrorHeartbeatConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorHeartbeatConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:48:41,818 - INFO  [task-thread-MirrorHeartbeatConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,819 - INFO  [task-thread-MirrorHeartbeatConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,819 - INFO  [task-thread-MirrorHeartbeatConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,819 - INFO  [task-thread-MirrorHeartbeatConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorHeartbeatConnector-0 unregistered
2023-07-30 13:48:41,821 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34453]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:48:41,821 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-4, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:48:41,822 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-4, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:48:41,822 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7', leaderUrl='http://localhost:35973/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[MirrorHeartbeatConnector-0], delay=0} with rebalance delay: 0
2023-07-30 13:48:41,822 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:48:41,822 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:41,822 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:48:41,822 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:48:41,822 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:48:41,823 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739321822
2023-07-30 13:48:41,822 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:41,826 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:41979/'}
2023-07-30 13:48:41,826 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:48:41,826 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:48:41,829 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@3113a37{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:48:41,829 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:48:41,830 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:48:41,830 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-1, groupId=backup-mm2] Herder stopping
2023-07-30 13:48:41,830 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@655] - [Worker clientId=connect-1, groupId=backup-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:48:41,831 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@1016] - [Worker clientId=connect-1, groupId=backup-mm2] Member connect-1-a67aef66-336a-4e9e-8e54-170e720354ff sending LeaveGroup request to coordinator localhost:41091 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:48:41,831 - WARN  [DistributedHerder-connect-1-1:AbstractCoordinator@997] - [Worker clientId=connect-1, groupId=backup-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:48:41,831 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,831 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,831 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-1-a67aef66-336a-4e9e-8e54-170e720354ff] in group backup-mm2 has left, removing it from the group
2023-07-30 13:48:41,831 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,831 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group backup-mm2 in state PreparingRebalance with old generation 2 (__consumer_offsets-5) (reason: removing member connect-1-a67aef66-336a-4e9e-8e54-170e720354ff on LeaveGroup)
2023-07-30 13:48:41,832 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.connect for connect-1 unregistered
2023-07-30 13:48:41,832 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:41,832 - INFO  [DistributedHerder-connect-1-1:KafkaProducer@1193] - [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,833 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,833 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,833 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,833 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.producer for producer-3 unregistered
2023-07-30 13:48:41,834 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,834 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,834 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,835 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-2 unregistered
2023-07-30 13:48:41,835 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:41,835 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:48:41,835 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:41,835 - INFO  [DistributedHerder-connect-1-1:KafkaProducer@1193] - [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,836 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,836 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,836 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,836 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.producer for producer-4 unregistered
2023-07-30 13:48:41,837 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,837 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,837 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,838 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-3 unregistered
2023-07-30 13:48:41,838 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:41,838 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:48:41,838 - INFO  [DistributedHerder-connect-1-1:Worker@209] - Worker stopping
2023-07-30 13:48:41,838 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:48:41,838 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:41,840 - INFO  [DistributedHerder-connect-1-1:KafkaProducer@1193] - [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,840 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,840 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,841 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,841 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.producer for producer-2 unregistered
2023-07-30 13:48:41,842 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,842 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,842 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,842 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-1 unregistered
2023-07-30 13:48:41,843 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:41,843 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:48:41,843 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,843 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,843 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,843 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.connect for localhost:41979 unregistered
2023-07-30 13:48:41,843 - INFO  [DistributedHerder-connect-1-1:Worker@230] - Worker stopped
2023-07-30 13:48:41,843 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Removed 0 offsets associated with deleted partitions: mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4.
2023-07-30 13:48:41,844 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Removed 0 offsets associated with deleted partitions: mm2-configs.backup.internal-0.
2023-07-30 13:48:41,844 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@299] - [Worker clientId=connect-1, groupId=backup-mm2] Herder stopped
2023-07-30 13:48:41,845 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-1, groupId=backup-mm2] Herder stopped
2023-07-30 13:48:41,845 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:48:41,845 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:44883/'}
2023-07-30 13:48:41,846 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:48:41,846 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:48:41,847 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@181e72d3{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:48:41,848 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:48:41,848 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:48:41,849 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-2, groupId=backup-mm2] Herder stopping
2023-07-30 13:48:41,849 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@655] - [Worker clientId=connect-2, groupId=backup-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:48:41,849 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@1016] - [Worker clientId=connect-2, groupId=backup-mm2] Member connect-2-19c4f7ce-48ae-4053-bfab-c2555b604042 sending LeaveGroup request to coordinator localhost:41091 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:48:41,849 - WARN  [DistributedHerder-connect-2-1:AbstractCoordinator@997] - [Worker clientId=connect-2, groupId=backup-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:48:41,849 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,849 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,850 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,850 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-2-19c4f7ce-48ae-4053-bfab-c2555b604042] in group backup-mm2 has left, removing it from the group
2023-07-30 13:48:41,850 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.connect for connect-2 unregistered
2023-07-30 13:48:41,850 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:41,850 - INFO  [DistributedHerder-connect-2-1:KafkaProducer@1193] - [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,851 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,851 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,851 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,851 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.producer for producer-6 unregistered
2023-07-30 13:48:41,852 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,852 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,852 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,853 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-5 unregistered
2023-07-30 13:48:41,853 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:41,853 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:48:41,853 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:41,853 - INFO  [DistributedHerder-connect-2-1:KafkaProducer@1193] - [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,854 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,854 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,855 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,855 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.producer for producer-7 unregistered
2023-07-30 13:48:41,856 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,856 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,856 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,856 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-6 unregistered
2023-07-30 13:48:41,857 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:41,857 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:48:41,857 - INFO  [DistributedHerder-connect-2-1:Worker@209] - Worker stopping
2023-07-30 13:48:41,857 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:48:41,857 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:41,858 - INFO  [DistributedHerder-connect-2-1:KafkaProducer@1193] - [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,858 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,859 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,859 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,859 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.producer for producer-5 unregistered
2023-07-30 13:48:41,859 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,859 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,860 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,860 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-4 unregistered
2023-07-30 13:48:41,860 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:41,860 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:48:41,860 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,860 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,860 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,860 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.connect for localhost:44883 unregistered
2023-07-30 13:48:41,860 - INFO  [DistributedHerder-connect-2-1:Worker@230] - Worker stopped
2023-07-30 13:48:41,861 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@299] - [Worker clientId=connect-2, groupId=backup-mm2] Herder stopped
2023-07-30 13:48:41,862 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-2, groupId=backup-mm2] Herder stopped
2023-07-30 13:48:41,862 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:48:41,862 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:40365/'}
2023-07-30 13:48:41,862 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:48:41,862 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:48:41,866 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@28da7d11{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:48:41,866 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:48:41,866 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:48:41,866 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:48:41,866 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:48:41,867 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:48:41,868 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:48:41,868 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-3, groupId=backup-mm2] Herder stopping
2023-07-30 13:48:41,868 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@655] - [Worker clientId=connect-3, groupId=backup-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:48:41,868 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@1016] - [Worker clientId=connect-3, groupId=backup-mm2] Member connect-3-1326fd8d-96ab-4d62-b54d-4729124ff162 sending LeaveGroup request to coordinator localhost:41091 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:48:41,869 - WARN  [DistributedHerder-connect-3-1:AbstractCoordinator@997] - [Worker clientId=connect-3, groupId=backup-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:48:41,869 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,869 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,869 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-3-1326fd8d-96ab-4d62-b54d-4729124ff162] in group backup-mm2 has left, removing it from the group
2023-07-30 13:48:41,869 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,870 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Group backup-mm2 with generation 3 is now empty (__consumer_offsets-5)
2023-07-30 13:48:41,871 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.connect for connect-3 unregistered
2023-07-30 13:48:41,871 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:41,872 - INFO  [DistributedHerder-connect-3-1:KafkaProducer@1193] - [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,874 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,874 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,874 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,874 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.producer for producer-9 unregistered
2023-07-30 13:48:41,875 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,875 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,875 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,876 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-8 unregistered
2023-07-30 13:48:41,876 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:48:41,876 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:48:41,876 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:41,877 - INFO  [DistributedHerder-connect-3-1:KafkaProducer@1193] - [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,879 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,879 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,879 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,879 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.producer for producer-10 unregistered
2023-07-30 13:48:41,879 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,879 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,879 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,880 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-9 unregistered
2023-07-30 13:48:41,880 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:48:41,880 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:48:41,881 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:48:41,881 - INFO  [DistributedHerder-connect-3-1:Worker@209] - Worker stopping
2023-07-30 13:48:41,881 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:48:41,880 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:48:41,882 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:48:41,882 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:41,882 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:48:41,883 - INFO  [DistributedHerder-connect-3-1:KafkaProducer@1193] - [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,884 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,885 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,885 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,885 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.producer for producer-8 unregistered
2023-07-30 13:48:41,886 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,886 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,886 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,887 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-7 unregistered
2023-07-30 13:48:41,887 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:48:41,887 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:48:41,887 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,888 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,888 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,888 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.connect for localhost:40365 unregistered
2023-07-30 13:48:41,888 - INFO  [DistributedHerder-connect-3-1:Worker@230] - Worker stopped
2023-07-30 13:48:41,889 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@299] - [Worker clientId=connect-3, groupId=backup-mm2] Herder stopped
2023-07-30 13:48:41,890 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-3, groupId=backup-mm2] Herder stopped
2023-07-30 13:48:41,890 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:48:41,890 - INFO  [main:KafkaProducer@1193] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:41,892 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:41,893 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:41,893 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:41,894 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-1 unregistered
2023-07-30 13:48:41,894 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 66 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,894 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 66 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,894 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 66 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,895 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2023-07-30 13:48:41,896 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Log for partition mm2-configs.backup.internal-0 is renamed to /tmp/junit1823696247009190315/junit2950113589444295251/mm2-configs.backup.internal-0.cbbcd933e0154267ae8b1aa2928320b4-delete and is scheduled for deletion
2023-07-30 13:48:41,896 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-1 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-1.a859a4fa8d10455b879702e431db3e11-delete and is scheduled for deletion
2023-07-30 13:48:41,898 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-0 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-0.afd224c9300c4269bff2b4714e2ef9f9-delete and is scheduled for deletion
2023-07-30 13:48:41,899 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-3 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-3.3d943fe3b64b4efab5d37013ece93b21-delete and is scheduled for deletion
2023-07-30 13:48:41,901 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-2 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-2.e9ab878643804ce4a8b42063df16a709-delete and is scheduled for deletion
2023-07-30 13:48:41,901 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-4 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-status.primary.internal-4.5000f9b9373249ffac3432a9aff17cc8-delete and is scheduled for deletion
2023-07-30 13:48:41,907 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2023-07-30 13:48:41,919 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Group consumer-group-testReplicationWithEmptyPartition transitioned to Dead in generation 2
2023-07-30 13:48:41,921 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Group backup-mm2 transitioned to Dead in generation 3
2023-07-30 13:48:41,922 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Group consumer-group-dummy transitioned to Dead in generation 2
2023-07-30 13:48:41,922 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Removed 20 offsets associated with deleted partitions: heartbeats-0, test-topic-with-empty-partition-3, test-topic-with-empty-partition-7, test-topic-with-empty-partition-2, test-topic-with-empty-partition-6, test-topic-with-empty-partition-5, test-topic-with-empty-partition-1, test-topic-with-empty-partition-9, test-topic-with-empty-partition-4, test-topic-with-empty-partition-0, test-topic-with-empty-partition-8, mm2-status.backup.internal-2, mm2-status.backup.internal-3, mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-offset-syncs.backup.internal-0, backup.test-topic-1-0, test-topic-1-3, test-topic-1-7, test-topic-1-2, test-topic-1-6, test-topic-1-5, test-topic-1-9, test-topic-1-1, test-topic-1-4, test-topic-1-8, test-topic-1-0, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-21, mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-15.
2023-07-30 13:48:41,925 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2023-07-30 13:48:41,927 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupMetadataManager brokerId=0] Group consumer-group-dummy transitioned to Dead in generation 2
2023-07-30 13:48:41,927 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2023-07-30 13:48:41,928 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Removed 10 offsets associated with deleted partitions: mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-21, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-7, heartbeats-0, primary.test-topic-1-9, primary.test-topic-1-0, primary.test-topic-1-4, primary.test-topic-1-6, primary.test-topic-1-1, primary.test-topic-1-5, primary.test-topic-1-7, primary.test-topic-1-2, primary.test-topic-1-8, primary.test-topic-1-3, mm2-configs.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-with-empty-partition-1, primary.test-topic-with-empty-partition-6, primary.test-topic-with-empty-partition-2, primary.test-topic-with-empty-partition-7, primary.test-topic-with-empty-partition-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-with-empty-partition-0, primary.heartbeats-0, test-topic-1-3, test-topic-1-7, test-topic-1-2, test-topic-1-6, test-topic-1-5, test-topic-1-9, test-topic-1-1, test-topic-1-4, test-topic-1-8, test-topic-1-0.
2023-07-30 13:48:41,927 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, heartbeats-0, mm2-offsets.backup.internal-1, test-topic-1-3, test-topic-with-empty-partition-3, mm2-offsets.backup.internal-2, test-topic-1-7, mm2-offsets.backup.internal-6, mm2-status.backup.internal-2, mm2-offsets.backup.internal-10, test-topic-with-empty-partition-7, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-21, backup.test-topic-1-0, mm2-offsets.backup.internal-0, test-topic-1-2, test-topic-with-empty-partition-2, mm2-offsets.backup.internal-5, test-topic-1-6, mm2-offsets.backup.internal-9, mm2-status.backup.internal-3, mm2-offsets.backup.internal-13, test-topic-with-empty-partition-6, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-24, test-topic-1-5, test-topic-with-empty-partition-5, mm2-offsets.backup.internal-4, test-topic-1-9, mm2-status.backup.internal-0, test-topic-with-empty-partition-1, mm2-offsets.backup.internal-8, mm2-status.backup.internal-4, mm2-offsets.backup.internal-12, test-topic-1-1, test-topic-with-empty-partition-9, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-23, mm2-offset-syncs.backup.internal-0, mm2-offsets.backup.internal-3, test-topic-1-4, test-topic-with-empty-partition-4, mm2-offsets.backup.internal-7, test-topic-1-8, mm2-status.backup.internal-1, test-topic-with-empty-partition-0, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-15, test-topic-1-0, test-topic-with-empty-partition-8)
2023-07-30 13:48:41,929 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, heartbeats-0, mm2-offsets.backup.internal-1, test-topic-1-3, test-topic-with-empty-partition-3, mm2-offsets.backup.internal-2, test-topic-1-7, mm2-offsets.backup.internal-6, mm2-status.backup.internal-2, mm2-offsets.backup.internal-10, test-topic-with-empty-partition-7, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-21, backup.test-topic-1-0, mm2-offsets.backup.internal-0, test-topic-1-2, test-topic-with-empty-partition-2, mm2-offsets.backup.internal-5, test-topic-1-6, mm2-offsets.backup.internal-9, mm2-status.backup.internal-3, mm2-offsets.backup.internal-13, test-topic-with-empty-partition-6, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-24, test-topic-1-5, test-topic-with-empty-partition-5, mm2-offsets.backup.internal-4, test-topic-1-9, mm2-status.backup.internal-0, test-topic-with-empty-partition-1, mm2-offsets.backup.internal-8, mm2-status.backup.internal-4, mm2-offsets.backup.internal-12, test-topic-1-1, test-topic-with-empty-partition-9, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-23, mm2-offset-syncs.backup.internal-0, mm2-offsets.backup.internal-3, test-topic-1-4, test-topic-with-empty-partition-4, mm2-offsets.backup.internal-7, test-topic-1-8, mm2-status.backup.internal-1, test-topic-with-empty-partition-0, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-15, test-topic-1-0, test-topic-with-empty-partition-8)
2023-07-30 13:48:41,929 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2023-07-30 13:48:41,930 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2023-07-30 13:48:41,930 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopping socket server request processors
2023-07-30 13:48:41,932 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:48:41,932 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:48:41,935 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopped socket server request processors
2023-07-30 13:48:41,936 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2023-07-30 13:48:41,937 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:48:41,937 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:48:41,939 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2023-07-30 13:48:41,947 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 49 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,947 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 50 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,947 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 79 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,949 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2023-07-30 13:48:41,955 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 76 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,956 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 61 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,956 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 60 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,962 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-2 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-2.3bfec171ab444379858dbdf8d1083bc9-delete and is scheduled for deletion
2023-07-30 13:48:41,963 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-9 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-9.1e375a6dbb3f43df807c8e09a1f23ff2-delete and is scheduled for deletion
2023-07-30 13:48:41,964 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-5 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-5.2bc67e33d57a4d2eb3d3512e1a3ce4bd-delete and is scheduled for deletion
2023-07-30 13:48:41,965 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-9 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-9.cd3f71aa5da24b588d97d4c39c537178-delete and is scheduled for deletion
2023-07-30 13:48:41,965 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-23 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-23.12643f09759a43f28dc79f671f9c673e-delete and is scheduled for deletion
2023-07-30 13:48:41,966 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition heartbeats-0 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/heartbeats-0.ea6946ed8902495d9ec2ba009d5570b6-delete and is scheduled for deletion
2023-07-30 13:48:41,966 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-3 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-3.6cb88d33edec4e8a909a6815d1ef3279-delete and is scheduled for deletion
2023-07-30 13:48:41,967 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-18 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-18.3db4bca26a9f435bad7101becafc7d6c-delete and is scheduled for deletion
2023-07-30 13:48:41,967 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-0 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-0.ffa4cd39404343c6aab5d8d6ae6bfd05-delete and is scheduled for deletion
2023-07-30 13:48:41,968 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-7 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-7.a773e5d2e9624726aa86f260e984f1c2-delete and is scheduled for deletion
2023-07-30 13:48:41,969 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-14 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-14.9fad8d7ab3a2441f81de52af3faf8efd-delete and is scheduled for deletion
2023-07-30 13:48:41,969 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-4 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-4.50039b68db6048c1b75511b0bd1199ff-delete and is scheduled for deletion
2023-07-30 13:48:41,970 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-10 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-10.acb46e0c604c42389b67619c2b3f6fc6-delete and is scheduled for deletion
2023-07-30 13:48:41,970 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-1 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-1.a86aab897eff43d4809b0c17aa19ef66-delete and is scheduled for deletion
2023-07-30 13:48:41,971 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-6 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-6.67e9aa19141a426aaf2da28f3010ab61-delete and is scheduled for deletion
2023-07-30 13:48:41,971 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-6 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-6.7bbf2170f36f47f7b0fca4e41302a968-delete and is scheduled for deletion
2023-07-30 13:48:41,975 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-1 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-1.e52391cfa8f64f1b861ac3eb76321fda-delete and is scheduled for deletion
2023-07-30 13:48:41,977 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-2 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-2.a77c2df834d34322844028d417e3a404-delete and is scheduled for deletion
2023-07-30 13:48:41,978 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-6 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-6.f6d640b087194755b884e5104420fc94-delete and is scheduled for deletion
2023-07-30 13:48:41,978 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2023-07-30 13:48:41,978 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2023-07-30 13:48:41,979 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-22 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-22.30143a82c7c84655a8bdc6a9936bbb5f-delete and is scheduled for deletion
2023-07-30 13:48:41,980 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2023-07-30 13:48:41,980 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-2 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-2.8a0ff025fadc433181153343ee639bcc-delete and is scheduled for deletion
2023-07-30 13:48:41,981 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-17 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-17.3ff40ffc01714582b64ad82b52d8b335-delete and is scheduled for deletion
2023-07-30 13:48:41,981 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-1 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-1.5376e1e8419744cc891306c110559f0a-delete and is scheduled for deletion
2023-07-30 13:48:41,983 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-6 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-6.ca2887d1b77841378a95c5b3c9b18f19-delete and is scheduled for deletion
2023-07-30 13:48:41,983 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2023-07-30 13:48:41,984 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-13 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-13.598e6e8e4426422f8df0815319cf5086-delete and is scheduled for deletion
2023-07-30 13:48:41,984 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-5 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-5.592b9591db2749c4a3a8941e41aa90de-delete and is scheduled for deletion
2023-07-30 13:48:41,985 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-9 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-9.71e7cb1921134e38819ddcb2517f2c5e-delete and is scheduled for deletion
2023-07-30 13:48:41,985 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-5 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-5.0cb435a09d1d43599359aa787f6ed9c4-delete and is scheduled for deletion
2023-07-30 13:48:41,986 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-7 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-7.63e4cfb686b8439a9a5e6af3f0776bdf-delete and is scheduled for deletion
2023-07-30 13:48:41,987 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-0 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-0.81a83dfb1567410997d8f63ef0c38a24-delete and is scheduled for deletion
2023-07-30 13:48:41,987 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.checkpoints.internal-0 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.checkpoints.internal-0.8a3b014ce9e54679befdb669072ad17c-delete and is scheduled for deletion
2023-07-30 13:48:41,988 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-3 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-3.d62a6034b7c042bb999c6bc467d4cd95-delete and is scheduled for deletion
2023-07-30 13:48:41,989 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-configs.primary.internal-0 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-configs.primary.internal-0.621c5419b4574073858d5c55bbfe4fcc-delete and is scheduled for deletion
2023-07-30 13:48:41,990 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-7 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-7.5078b74752c94992a9fef33b3a765ab2-delete and is scheduled for deletion
2023-07-30 13:48:41,990 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-21 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-21.6f4e23a2e70f402aa1c122064c10823a-delete and is scheduled for deletion
2023-07-30 13:48:41,991 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-5 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-5.871ddbfa3e284ee6ba04a18c3a05e880-delete and is scheduled for deletion
2023-07-30 13:48:41,992 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-16 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-16.49294fdb5bc44803824444c544173b80-delete and is scheduled for deletion
2023-07-30 13:48:41,992 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-2 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-2.26ff2dd2515b4f85a1e4968cd3787e04-delete and is scheduled for deletion
2023-07-30 13:48:41,993 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-9 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-9.99cd644b66fb496387f07d0e699ebb30-delete and is scheduled for deletion
2023-07-30 13:48:41,993 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-12 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-12.b2ff8f079c2e48cab7c882ba0f8d6c64-delete and is scheduled for deletion
2023-07-30 13:48:41,994 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-8 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-8.6b67a2980b324618b5a09f83a7584f8f-delete and is scheduled for deletion
2023-07-30 13:48:41,994 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-1 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-1.83e57557200845909fa05228af28156f-delete and is scheduled for deletion
2023-07-30 13:48:41,995 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-4 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-4.c27f4362f14c4d8cb24049faf296e3be-delete and is scheduled for deletion
2023-07-30 13:48:41,995 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 168 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,996 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 185 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:41,997 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 181 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,000 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-3 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-3.b34dbbb6628242639e5730dbc716ee3e-delete and is scheduled for deletion
2023-07-30 13:48:42,001 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-8 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-8.17898c603af349dc834297f46f9c01d8-delete and is scheduled for deletion
2023-07-30 13:48:42,002 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-4 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-4.40a91adda2f64e428dc7324427c8d610-delete and is scheduled for deletion
2023-07-30 13:48:42,003 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-8 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-8.86e5d8e72bdf44a8af94173f2f568258-delete and is scheduled for deletion
2023-07-30 13:48:42,003 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-24 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-24.02269a53157245cfbaf2ce5dc91b599b-delete and is scheduled for deletion
2023-07-30 13:48:42,004 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-20 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-20.3eaa11e9fdf249208ef4907fdbacec95-delete and is scheduled for deletion
2023-07-30 13:48:42,004 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-19 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-19.7cbdfe48ad704d929f2dba137641a1ce-delete and is scheduled for deletion
2023-07-30 13:48:42,005 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-4 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-4.abcbe19f05e04e0994489b6fda66457f-delete and is scheduled for deletion
2023-07-30 13:48:42,005 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-15 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-15.deb8dfe8bd05458c9272b8271b8f1d68-delete and is scheduled for deletion
2023-07-30 13:48:42,005 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-1-3 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-1-3.b21d1520c7fa494f957fb263d945b274-delete and is scheduled for deletion
2023-07-30 13:48:42,006 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-8 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-8.9e120462b4b049608529afac8f74cfff-delete and is scheduled for deletion
2023-07-30 13:48:42,006 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-11 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-11.e8917250582d4f78899a5f9aac1fc30e-delete and is scheduled for deletion
2023-07-30 13:48:42,007 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.heartbeats-0 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.heartbeats-0.fb5d039ed4d24bb6a6ad9707ffc28d23-delete and is scheduled for deletion
2023-07-30 13:48:42,007 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition primary.test-topic-with-empty-partition-0 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/primary.test-topic-with-empty-partition-0.ddae0e3466884b04a668a70c963828a1-delete and is scheduled for deletion
2023-07-30 13:48:42,008 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition mm2-offsets.primary.internal-7 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/mm2-offsets.primary.internal-7.934ccfc6687342ba84fbdc61421a44c3-delete and is scheduled for deletion
2023-07-30 13:48:42,008 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Log for partition test-topic-1-0 is renamed to /tmp/junit5769116175745214285/junit1884222054139287341/test-topic-1-0.c2ccd4533edd4ea09aa01d3b646d5b4c-delete and is scheduled for deletion
2023-07-30 13:48:42,035 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,036 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,047 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 119 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,048 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 149 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,049 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 120 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,056 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 188 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,057 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 152 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,057 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 164 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,096 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 250 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,098 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 298 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,098 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 284 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,136 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,138 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,149 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 196 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,150 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 224 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,150 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 182 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,157 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 233 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,157 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 254 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,157 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 304 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,178 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2023-07-30 13:48:42,179 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2023-07-30 13:48:42,181 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2023-07-30 13:48:42,182 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2023-07-30 13:48:42,183 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2023-07-30 13:48:42,183 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2023-07-30 13:48:42,185 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2023-07-30 13:48:42,185 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2023-07-30 13:48:42,186 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2023-07-30 13:48:42,187 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2023-07-30 13:48:42,188 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2023-07-30 13:48:42,198 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 331 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,200 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 427 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,200 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 405 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,228 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2023-07-30 13:48:42,228 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2023-07-30 13:48:42,229 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2023-07-30 13:48:42,239 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,244 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2023-07-30 13:48:42,244 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2023-07-30 13:48:42,245 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2023-07-30 13:48:42,247 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2023-07-30 13:48:42,247 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2023-07-30 13:48:42,247 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2023-07-30 13:48:42,247 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2023-07-30 13:48:42,248 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2023-07-30 13:48:42,250 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2023-07-30 13:48:42,251 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 280 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,251 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 297 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,251 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2023-07-30 13:48:42,252 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2023-07-30 13:48:42,252 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 264 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,252 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2023-07-30 13:48:42,258 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 364 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,258 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 338 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,258 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 426 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,298 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 423 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,300 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 538 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,301 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 559 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,340 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,346 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2023-07-30 13:48:42,346 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2023-07-30 13:48:42,347 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2023-07-30 13:48:42,352 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 376 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,352 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 393 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,353 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 366 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,358 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 500 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,359 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 475 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,359 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 571 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,379 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2023-07-30 13:48:42,379 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2023-07-30 13:48:42,379 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2023-07-30 13:48:42,399 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 535 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,400 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 700 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,402 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 737 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,437 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,441 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,452 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 490 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,452 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 507 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,454 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 496 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,458 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 665 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,459 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 753 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,460 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 632 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,503 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 860 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,503 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 895 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,514 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 636 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,542 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,553 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 589 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,553 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 608 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,554 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 613 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,558 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 814 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,559 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 916 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,560 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 767 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,579 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2023-07-30 13:48:42,579 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2023-07-30 13:48:42,580 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2023-07-30 13:48:42,603 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1110 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,603 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1056 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,615 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 765 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,643 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,653 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 715 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,653 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 731 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,655 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 763 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,659 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1123 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,663 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1001 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,669 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 929 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,704 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1336 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,704 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1285 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,716 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 901 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,744 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,754 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 850 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,755 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 858 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,756 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 923 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,759 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1340 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,763 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1203 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,769 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1114 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,779 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2023-07-30 13:48:42,779 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2023-07-30 13:48:42,782 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2023-07-30 13:48:42,783 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutting down
2023-07-30 13:48:42,783 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Stopped
2023-07-30 13:48:42,783 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:48:42,784 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:48:42,785 - INFO  [main:Logging@66] - Shutting down.
2023-07-30 13:48:42,805 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1516 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,806 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1580 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,811 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-4] Writing producer snapshot at offset 1
2023-07-30 13:48:42,817 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1041 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,823 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-9] Writing producer snapshot at offset 1
2023-07-30 13:48:42,832 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-4] Writing producer snapshot at offset 1
2023-07-30 13:48:42,839 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,840 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-6] Writing producer snapshot at offset 1
2023-07-30 13:48:42,845 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,848 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-9] Writing producer snapshot at offset 1
2023-07-30 13:48:42,855 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 979 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,855 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 988 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,856 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-6] Writing producer snapshot at offset 1
2023-07-30 13:48:42,857 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1078 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,860 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1564 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,864 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1403 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,865 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-0] Writing producer snapshot at offset 1
2023-07-30 13:48:42,870 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1294 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,873 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-5] Writing producer snapshot at offset 1
2023-07-30 13:48:42,882 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-1] Writing producer snapshot at offset 1
2023-07-30 13:48:42,890 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-5] Writing producer snapshot at offset 1
2023-07-30 13:48:42,898 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-35] Writing producer snapshot at offset 23
2023-07-30 13:48:42,905 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1751 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,906 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1826 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,907 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-7] Writing producer snapshot at offset 1
2023-07-30 13:48:42,915 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-8] Writing producer snapshot at offset 1
2023-07-30 13:48:42,916 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1167 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,923 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-7] Writing producer snapshot at offset 1
2023-07-30 13:48:42,932 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-8] Writing producer snapshot at offset 1
2023-07-30 13:48:42,940 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-1] Writing producer snapshot at offset 1
2023-07-30 13:48:42,945 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:42,949 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-3] Writing producer snapshot at offset 1
2023-07-30 13:48:42,955 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1103 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,956 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1104 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,957 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-3] Writing producer snapshot at offset 1
2023-07-30 13:48:42,958 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1253 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,960 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1810 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,964 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1616 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,965 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-0] Writing producer snapshot at offset 1
2023-07-30 13:48:42,969 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1491 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:42,974 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-with-empty-partition-2] Writing producer snapshot at offset 1
2023-07-30 13:48:42,982 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=test-topic-1-2] Writing producer snapshot at offset 1
2023-07-30 13:48:42,990 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-5] Writing producer snapshot at offset 4
2023-07-30 13:48:42,993 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-34] Writing producer snapshot at offset 23
2023-07-30 13:48:43,005 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2003 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,006 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2091 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,012 - INFO  [main:Logging@66] - Shutdown complete.
2023-07-30 13:48:43,016 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1309 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,026 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2023-07-30 13:48:43,026 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2023-07-30 13:48:43,026 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2023-07-30 13:48:43,027 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2023-07-30 13:48:43,055 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1242 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,055 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1247 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,059 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1439 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,060 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2061 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,064 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1846 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,069 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1663 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,105 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2277 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,106 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2371 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,117 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1460 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,131 - INFO  [main:ZooKeeper@1422] - Session: 0x1087f304f460000 closed
2023-07-30 13:48:43,131 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x1087f304f460000
2023-07-30 13:48:43,132 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2023-07-30 13:48:43,133 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2023-07-30 13:48:43,155 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1390 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,155 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1381 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,159 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1609 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,160 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2334 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,164 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2122 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,169 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1905 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,205 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2546 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,206 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2666 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,218 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1617 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,255 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1528 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,255 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1534 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,259 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1819 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,260 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2623 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,264 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2383 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,270 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2152 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,305 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2864 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,306 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2998 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,319 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1771 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,355 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1687 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,355 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1681 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,373 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2639 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,373 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2928 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,373 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2381 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,373 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2020 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,405 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 3140 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,406 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3284 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,418 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1919 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,435 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2023-07-30 13:48:43,435 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2023-07-30 13:48:43,435 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2023-07-30 13:48:43,439 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2023-07-30 13:48:43,439 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2023-07-30 13:48:43,439 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2023-07-30 13:48:43,442 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2023-07-30 13:48:43,442 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2023-07-30 13:48:43,442 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2023-07-30 13:48:43,455 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1830 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,455 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1827 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,473 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2259 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,473 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 3276 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,473 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2940 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,473 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2657 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,505 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 3511 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,506 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3669 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,519 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2104 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,555 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2008 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,555 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2005 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,573 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 3642 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,573 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 3258 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,573 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2948 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,573 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2505 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,605 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 3879 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,606 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 4056 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,620 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2298 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,641 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:43,655 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2185 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,656 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2182 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,673 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 4006 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,673 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2751 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,673 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 3236 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,673 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 3571 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,705 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 4254 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,706 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 4444 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,720 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2488 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,755 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2358 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,755 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2366 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,773 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2999 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,773 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 3531 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,773 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 3884 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,773 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 4380 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,805 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 4631 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,806 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 4840 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,821 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2669 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,855 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2540 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,855 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2531 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,873 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 3244 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,873 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 4758 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,873 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4202 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,873 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 3824 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,905 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 5019 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,906 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 5253 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,922 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2867 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,955 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2720 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,955 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2717 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,973 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 3495 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,973 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 4128 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,973 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 5141 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:43,973 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4535 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,005 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 5361 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,006 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 5621 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,022 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3041 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,055 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2883 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,055 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2875 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,073 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 3721 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,073 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 4394 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,073 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4829 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,073 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 5481 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,105 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 5737 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,106 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 6018 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,122 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3224 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,155 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3048 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,155 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3044 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,173 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 5837 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,173 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 4675 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,173 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 3955 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,173 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 5139 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,205 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 6117 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,206 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 6421 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,222 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3411 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,255 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3225 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,256 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3217 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,273 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 4204 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,273 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 4973 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,273 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 5457 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,273 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 6214 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,305 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 6506 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,306 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 6833 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,322 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3605 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,355 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3401 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,355 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3406 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,373 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 4451 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,373 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 6609 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,373 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 5779 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,373 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 5269 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,405 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 6915 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,406 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 7261 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,423 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3802 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,442 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2023-07-30 13:48:44,442 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2023-07-30 13:48:44,443 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutting down socket server
2023-07-30 13:48:44,455 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3584 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,455 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3583 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,457 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutdown completed
2023-07-30 13:48:44,458 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,458 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,458 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,460 - INFO  [main:Logging@66] - Broker and topic stats closed
2023-07-30 13:48:44,461 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2023-07-30 13:48:44,461 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2023-07-30 13:48:44,461 - INFO  [main:EmbeddedKafkaCluster@198] - Cleaning up kafka log dirs at ArraySeq(/tmp/junit1823696247009190315/junit2950113589444295251)
2023-07-30 13:48:44,473 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 4700 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,473 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 6101 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,473 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 6962 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,473 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 5568 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,486 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2023-07-30 13:48:44,487 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:48:44,487 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:48:44,488 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:48:44,489 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2023-07-30 13:48:44,489 - INFO  [main:ZooKeeperServer@558] - shutting down
2023-07-30 13:48:44,490 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2023-07-30 13:48:44,490 - INFO  [main:PrepRequestProcessor@1007] - Shutting down
2023-07-30 13:48:44,490 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2023-07-30 13:48:44,490 - INFO  [ProcessThread(sid:0 cport:37775)::PrepRequestProcessor@155] - PrepRequestProcessor exited loop!
2023-07-30 13:48:44,490 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2023-07-30 13:48:44,490 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete
2023-07-30 13:48:44,499 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:44785/'}
2023-07-30 13:48:44,499 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:48:44,499 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:48:44,501 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@630b6190{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:48:44,502 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:48:44,502 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:48:44,503 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-4, groupId=primary-mm2] Herder stopping
2023-07-30 13:48:44,503 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@655] - [Worker clientId=connect-4, groupId=primary-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:48:44,503 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@1016] - [Worker clientId=connect-4, groupId=primary-mm2] Member connect-4-961f724e-0581-4748-bfbb-288e704339ea sending LeaveGroup request to coordinator localhost:34453 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:48:44,504 - WARN  [DistributedHerder-connect-4-1:AbstractCoordinator@997] - [Worker clientId=connect-4, groupId=primary-mm2] Close timed out with 2 pending requests to coordinator, terminating client connections
2023-07-30 13:48:44,504 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,504 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,504 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,505 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.connect for connect-4 unregistered
2023-07-30 13:48:44,505 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:44,505 - INFO  [DistributedHerder-connect-4-1:KafkaProducer@1193] - [Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,505 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 7254 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,506 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,506 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,506 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,506 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.producer for producer-13 unregistered
2023-07-30 13:48:44,506 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 7611 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:48:44,507 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,507 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,507 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,508 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-11 unregistered
2023-07-30 13:48:44,508 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:44,508 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:48:44,508 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:44,509 - INFO  [DistributedHerder-connect-4-1:KafkaProducer@1193] - [Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,510 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,510 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,510 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,510 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.producer for producer-15 unregistered
2023-07-30 13:48:44,511 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,511 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,511 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,512 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-13 unregistered
2023-07-30 13:48:44,512 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:44,512 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:48:44,512 - INFO  [DistributedHerder-connect-4-1:Worker@209] - Worker stopping
2023-07-30 13:48:44,512 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:48:44,512 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:44,513 - INFO  [DistributedHerder-connect-4-1:KafkaProducer@1193] - [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,514 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,514 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,514 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,514 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.producer for producer-12 unregistered
2023-07-30 13:48:44,515 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,515 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,515 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,516 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-10 unregistered
2023-07-30 13:48:44,516 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:44,516 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:48:44,516 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,516 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,516 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,516 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.connect for localhost:44785 unregistered
2023-07-30 13:48:44,516 - INFO  [DistributedHerder-connect-4-1:Worker@230] - Worker stopped
2023-07-30 13:48:44,517 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@299] - [Worker clientId=connect-4, groupId=primary-mm2] Herder stopped
2023-07-30 13:48:44,518 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-4, groupId=primary-mm2] Herder stopped
2023-07-30 13:48:44,518 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:48:44,518 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:35973/'}
2023-07-30 13:48:44,518 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:48:44,518 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:48:44,520 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@25d0cb3a{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:48:44,520 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:48:44,521 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:48:44,522 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-5, groupId=primary-mm2] Herder stopping
2023-07-30 13:48:44,522 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@655] - [Worker clientId=connect-5, groupId=primary-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:48:44,522 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@1016] - [Worker clientId=connect-5, groupId=primary-mm2] Member connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7 sending LeaveGroup request to coordinator localhost:34453 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:48:44,522 - WARN  [DistributedHerder-connect-5-1:AbstractCoordinator@997] - [Worker clientId=connect-5, groupId=primary-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:48:44,522 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,522 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,522 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,523 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.connect for connect-5 unregistered
2023-07-30 13:48:44,523 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:44,523 - INFO  [DistributedHerder-connect-5-1:KafkaProducer@1193] - [Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,524 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,524 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,524 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,524 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.producer for producer-16 unregistered
2023-07-30 13:48:44,524 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,524 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,524 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,525 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-14 unregistered
2023-07-30 13:48:44,525 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:44,525 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:48:44,525 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:44,525 - INFO  [DistributedHerder-connect-5-1:KafkaProducer@1193] - [Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,526 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-5-4fd230dc-331c-4676-8aad-5aae5f038de7] in group primary-mm2 has left, removing it from the group
2023-07-30 13:48:44,526 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,527 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,527 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,527 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 14 (__consumer_offsets-17)
2023-07-30 13:48:44,527 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.producer for producer-17 unregistered
2023-07-30 13:48:44,528 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,528 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,528 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,528 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=14, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:44,528 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-4-961f724e-0581-4748-bfbb-288e704339ea] in group primary-mm2 has left, removing it from the group
2023-07-30 13:48:44,528 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 14 (__consumer_offsets-17) (reason: removing member connect-4-961f724e-0581-4748-bfbb-288e704339ea on LeaveGroup)
2023-07-30 13:48:44,529 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-15 unregistered
2023-07-30 13:48:44,529 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:44,529 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:48:44,529 - INFO  [DistributedHerder-connect-5-1:Worker@209] - Worker stopping
2023-07-30 13:48:44,529 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:48:44,529 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:44,530 - INFO  [DistributedHerder-connect-5-1:KafkaProducer@1193] - [Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,530 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,530 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,530 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,531 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.producer for producer-14 unregistered
2023-07-30 13:48:44,531 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@775] - [Worker clientId=connect-6, groupId=primary-mm2] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=14, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:44,531 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,531 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@468] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
2023-07-30 13:48:44,531 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,531 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:48:44,531 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,532 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 15 (__consumer_offsets-17)
2023-07-30 13:48:44,532 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-12 unregistered
2023-07-30 13:48:44,532 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:44,532 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:48:44,532 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,532 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=15, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:44,532 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,533 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,533 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.connect for localhost:35973 unregistered
2023-07-30 13:48:44,533 - INFO  [DistributedHerder-connect-5-1:Worker@230] - Worker stopped
2023-07-30 13:48:44,534 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@299] - [Worker clientId=connect-5, groupId=primary-mm2] Herder stopped
2023-07-30 13:48:44,534 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 15
2023-07-30 13:48:44,535 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-5, groupId=primary-mm2] Herder stopped
2023-07-30 13:48:44,535 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:48:44,535 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:37125/'}
2023-07-30 13:48:44,535 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:48:44,535 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:48:44,536 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=15, memberId='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', protocol='sessioned'}
2023-07-30 13:48:44,536 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 15 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f', leaderUrl='http://localhost:37125/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:48:44,536 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:48:44,536 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:48:44,537 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@1984212d{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:48:44,537 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:48:44,538 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:48:44,538 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-6, groupId=primary-mm2] Herder stopping
2023-07-30 13:48:44,538 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@655] - [Worker clientId=connect-6, groupId=primary-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:48:44,539 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@1016] - [Worker clientId=connect-6, groupId=primary-mm2] Member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f sending LeaveGroup request to coordinator localhost:34453 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:48:44,539 - WARN  [DistributedHerder-connect-6-1:AbstractCoordinator@997] - [Worker clientId=connect-6, groupId=primary-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:48:44,539 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,539 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,539 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f] in group primary-mm2 has left, removing it from the group
2023-07-30 13:48:44,539 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,539 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 15 (__consumer_offsets-17) (reason: removing member connect-6-8c3a8b07-661c-4e69-83b9-8f55d9caf90f on LeaveGroup)
2023-07-30 13:48:44,539 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Group primary-mm2 with generation 16 is now empty (__consumer_offsets-17)
2023-07-30 13:48:44,540 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.connect for connect-6 unregistered
2023-07-30 13:48:44,540 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:44,540 - INFO  [DistributedHerder-connect-6-1:KafkaProducer@1193] - [Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,541 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,541 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,541 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,541 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.producer for producer-19 unregistered
2023-07-30 13:48:44,542 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,542 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,542 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,542 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-17 unregistered
2023-07-30 13:48:44,543 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:48:44,543 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:48:44,543 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:44,543 - INFO  [DistributedHerder-connect-6-1:KafkaProducer@1193] - [Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,543 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,543 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,543 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,543 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.producer for producer-20 unregistered
2023-07-30 13:48:44,545 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,545 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,545 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,545 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-18 unregistered
2023-07-30 13:48:44,546 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:48:44,546 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:48:44,546 - INFO  [DistributedHerder-connect-6-1:Worker@209] - Worker stopping
2023-07-30 13:48:44,546 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:48:44,546 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:44,546 - INFO  [DistributedHerder-connect-6-1:KafkaProducer@1193] - [Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,546 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,547 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,547 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,547 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.producer for producer-18 unregistered
2023-07-30 13:48:44,547 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,547 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,547 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,548 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-16 unregistered
2023-07-30 13:48:44,548 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:48:44,548 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:48:44,548 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,548 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,548 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,548 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.connect for localhost:37125 unregistered
2023-07-30 13:48:44,548 - INFO  [DistributedHerder-connect-6-1:Worker@230] - Worker stopped
2023-07-30 13:48:44,548 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@299] - [Worker clientId=connect-6, groupId=primary-mm2] Herder stopped
2023-07-30 13:48:44,549 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-6, groupId=primary-mm2] Herder stopped
2023-07-30 13:48:44,549 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:48:44,549 - INFO  [main:KafkaProducer@1193] - [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:48:44,550 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:44,550 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:44,550 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:44,550 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-11 unregistered
2023-07-30 13:48:44,550 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2023-07-30 13:48:44,551 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2023-07-30 13:48:44,555 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2023-07-30 13:48:44,556 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2023-07-30 13:48:44,556 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2023-07-30 13:48:44,556 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2023-07-30 13:48:44,556 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopping socket server request processors
2023-07-30 13:48:44,558 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopped socket server request processors
2023-07-30 13:48:44,558 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2023-07-30 13:48:44,558 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2023-07-30 13:48:44,560 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2023-07-30 13:48:44,587 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2023-07-30 13:48:44,587 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2023-07-30 13:48:44,588 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2023-07-30 13:48:44,588 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2023-07-30 13:48:44,627 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2023-07-30 13:48:44,627 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2023-07-30 13:48:44,627 - INFO  [SessionTracker:SessionTrackerImpl@163] - SessionTrackerImpl exited loop!
2023-07-30 13:48:44,627 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2023-07-30 13:48:44,627 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2023-07-30 13:48:44,627 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2023-07-30 13:48:44,628 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2023-07-30 13:48:44,628 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2023-07-30 13:48:44,628 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2023-07-30 13:48:44,628 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2023-07-30 13:48:44,628 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2023-07-30 13:48:44,628 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2023-07-30 13:48:44,657 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:44,658 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:44,758 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:44,827 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2023-07-30 13:48:44,827 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2023-07-30 13:48:44,827 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2023-07-30 13:48:44,843 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:44,858 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:44,988 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2023-07-30 13:48:44,988 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2023-07-30 13:48:44,988 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2023-07-30 13:48:44,988 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2023-07-30 13:48:44,988 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2023-07-30 13:48:44,989 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2023-07-30 13:48:44,989 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2023-07-30 13:48:44,989 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2023-07-30 13:48:44,989 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2023-07-30 13:48:44,989 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2023-07-30 13:48:44,989 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2023-07-30 13:48:44,989 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2023-07-30 13:48:45,028 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2023-07-30 13:48:45,028 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2023-07-30 13:48:45,028 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2023-07-30 13:48:45,031 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2023-07-30 13:48:45,031 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2023-07-30 13:48:45,031 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2023-07-30 13:48:45,059 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:45,159 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:45,227 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2023-07-30 13:48:45,227 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2023-07-30 13:48:45,227 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2023-07-30 13:48:45,229 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2023-07-30 13:48:45,229 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2023-07-30 13:48:45,230 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2023-07-30 13:48:45,230 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutting down
2023-07-30 13:48:45,230 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Stopped
2023-07-30 13:48:45,230 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:48:45,231 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:48:45,231 - INFO  [main:Logging@66] - Shutting down.
2023-07-30 13:48:45,239 - INFO  [pool-38-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-35] Writing producer snapshot at offset 23
2023-07-30 13:48:45,263 - INFO  [pool-38-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-17] Writing producer snapshot at offset 15
2023-07-30 13:48:45,280 - INFO  [main:Logging@66] - Shutdown complete.
2023-07-30 13:48:45,283 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2023-07-30 13:48:45,284 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2023-07-30 13:48:45,284 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2023-07-30 13:48:45,284 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2023-07-30 13:48:45,385 - INFO  [main:ZooKeeper@1422] - Session: 0x1087f3066fd0000 closed
2023-07-30 13:48:45,385 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x1087f3066fd0000
2023-07-30 13:48:45,385 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2023-07-30 13:48:45,386 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2023-07-30 13:48:45,460 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:45,560 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:45,657 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2023-07-30 13:48:45,657 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2023-07-30 13:48:45,657 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2023-07-30 13:48:45,660 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2023-07-30 13:48:45,660 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2023-07-30 13:48:45,660 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2023-07-30 13:48:45,845 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:41091) could not be established. Broker may not be available.
2023-07-30 13:48:46,162 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:46,562 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:34453) could not be established. Broker may not be available.
2023-07-30 13:48:46,660 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2023-07-30 13:48:46,660 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2023-07-30 13:48:46,660 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2023-07-30 13:48:46,661 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2023-07-30 13:48:46,661 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2023-07-30 13:48:46,661 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutting down socket server
2023-07-30 13:48:46,669 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutdown completed
2023-07-30 13:48:46,670 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:48:46,670 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:48:46,670 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:48:46,670 - INFO  [main:Logging@66] - Broker and topic stats closed
2023-07-30 13:48:46,670 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2023-07-30 13:48:46,671 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2023-07-30 13:48:46,671 - INFO  [main:EmbeddedKafkaCluster@198] - Cleaning up kafka log dirs at ArraySeq(/tmp/junit5769116175745214285/junit1884222054139287341)
2023-07-30 13:48:46,681 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2023-07-30 13:48:46,682 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:48:46,682 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:48:46,682 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:48:46,683 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2023-07-30 13:48:46,683 - INFO  [main:ZooKeeperServer@558] - shutting down
2023-07-30 13:48:46,683 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2023-07-30 13:48:46,684 - INFO  [main:PrepRequestProcessor@1007] - Shutting down
2023-07-30 13:48:46,684 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2023-07-30 13:48:46,684 - INFO  [ProcessThread(sid:0 cport:36953)::PrepRequestProcessor@155] - PrepRequestProcessor exited loop!
2023-07-30 13:48:46,684 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2023-07-30 13:48:46,684 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete

Thanks for using JUnit! Support its development at https://junit.org/sponsoring

[36m[0m
[36m[0m [36mJUnit Jupiter[0m [32m[0m
[36m[0m [36mJUnit Vintage[0m [32m[0m
[36m   [0m [36mMirrorConnectorsIntegrationTest[0m [32m[0m
[36m      [0m [31mtestReplicationWithEmptyPartition[0m [36m31871 ms[0m [31m[0m [31mOffset of last partition is not zero expected:<0> but was:<1>[0m

Failures (1):
  JUnit Vintage:MirrorConnectorsIntegrationTest:testReplicationWithEmptyPartition
    MethodSource [className = 'org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest', methodName = 'testReplicationWithEmptyPartition', methodParameterTypes = '']
    => java.lang.AssertionError: Offset of last partition is not zero expected:<0> but was:<1>
       org.junit.Assert.fail(Assert.java:89)
       org.junit.Assert.failNotEquals(Assert.java:835)
       org.junit.Assert.assertEquals(Assert.java:647)
       org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplicationWithEmptyPartition(MirrorConnectorsIntegrationTest.java:375)
       sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
       sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
       java.lang.reflect.Method.invoke(Method.java:498)
       org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
       org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
       [...]

Test run finished after 31913 ms
[         3 containers found      ]
[         0 containers skipped    ]
[         3 containers started    ]
[         0 containers aborted    ]
[         3 containers successful ]
[         0 containers failed     ]
[         1 tests found           ]
[         0 tests skipped         ]
[         1 tests started         ]
[         0 tests aborted         ]
[         0 tests successful      ]
[         1 tests failed          ]

