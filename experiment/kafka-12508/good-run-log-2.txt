2025-03-26 17:31:19,913 - INFO  [main:Log4jControllerRegistration$@31] - Registered kafka:type=kafka.Log4jController MBean
2025-03-26 17:31:19,993 - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2025-03-26 17:31:19,993 - INFO  [main:Environment@109] - Server environment:host.name=146-193-190-213.edr.inesc.pt
2025-03-26 17:31:19,993 - INFO  [main:Environment@109] - Server environment:java.version=1.8.0_442
2025-03-26 17:31:19,994 - INFO  [main:Environment@109] - Server environment:java.vendor=Private Build
2025-03-26 17:31:19,994 - INFO  [main:Environment@109] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2025-03-26 17:31:19,994 - INFO  [main:Environment@109] - Server environment:java.class.path=/home/sebastiaoamaro/phd/torefidevel/rw/Anduril/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2025-03-26 17:31:19,994 - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2025-03-26 17:31:19,995 - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2025-03-26 17:31:19,995 - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2025-03-26 17:31:19,995 - INFO  [main:Environment@109] - Server environment:os.name=Linux
2025-03-26 17:31:19,995 - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2025-03-26 17:31:19,995 - INFO  [main:Environment@109] - Server environment:os.version=6.11.0-061100-generic
2025-03-26 17:31:19,995 - INFO  [main:Environment@109] - Server environment:user.name=root
2025-03-26 17:31:19,996 - INFO  [main:Environment@109] - Server environment:user.home=/root
2025-03-26 17:31:19,996 - INFO  [main:Environment@109] - Server environment:user.dir=/home/sebastiaoamaro/phd/torefidevel/rw/Anduril/experiment/kafka-12508
2025-03-26 17:31:19,997 - INFO  [main:Environment@109] - Server environment:os.memory.free=357MB
2025-03-26 17:31:19,997 - INFO  [main:Environment@109] - Server environment:os.memory.max=7060MB
2025-03-26 17:31:19,997 - INFO  [main:Environment@109] - Server environment:os.memory.total=477MB
2025-03-26 17:31:20,003 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2025-03-26 17:31:20,039 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2025-03-26 17:31:20,044 - INFO  [main:ZooKeeperServer@953] - minSessionTimeout set to 1600
2025-03-26 17:31:20,044 - INFO  [main:ZooKeeperServer@962] - maxSessionTimeout set to 16000
2025-03-26 17:31:20,045 - INFO  [main:ZooKeeperServer@181] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-3187426591505449025/version-2 snapdir /tmp/kafka-8912057916640670385/version-2
2025-03-26 17:31:20,054 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 24 worker threads, and 64 kB direct buffers.
2025-03-26 17:31:20,060 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2025-03-26 17:31:20,068 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-8912057916640670385/version-2/snapshot.0
2025-03-26 17:31:20,071 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-8912057916640670385/version-2/snapshot.0
2025-03-26 17:31:20,086 - INFO  [ProcessThread(sid:0 cport:46823)::PrepRequestProcessor@132] - PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-03-26 17:31:20,455 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6417420901555019609/junit6084492695690390029
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:46823
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2025-03-26 17:31:20,473 - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-03-26 17:31:20,537 - INFO  [main:Logging@66] - starting
2025-03-26 17:31:20,538 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:46823
2025-03-26 17:31:20,552 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:46823.
2025-03-26 17:31:20,556 - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:host.name=146-193-190-213.edr.inesc.pt
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_442
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:java.vendor=Private Build
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:java.class.path=/home/sebastiaoamaro/phd/torefidevel/rw/Anduril/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:os.name=Linux
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:os.version=6.11.0-061100-generic
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:user.name=root
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:user.home=/root
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:user.dir=/home/sebastiaoamaro/phd/torefidevel/rw/Anduril/experiment/kafka-12508
2025-03-26 17:31:20,557 - INFO  [main:Environment@109] - Client environment:os.memory.free=271MB
2025-03-26 17:31:20,558 - INFO  [main:Environment@109] - Client environment:os.memory.max=7060MB
2025-03-26 17:31:20,558 - INFO  [main:Environment@109] - Client environment:os.memory.total=336MB
2025-03-26 17:31:20,560 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:46823 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@74a6f9c1
2025-03-26 17:31:20,563 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2025-03-26 17:31:20,568 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2025-03-26 17:31:20,570 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2025-03-26 17:31:20,574 - INFO  [main-SendThread(127.0.0.1:46823):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:46823. Will not attempt to authenticate using SASL (unknown error)
2025-03-26 17:31:20,575 - INFO  [main-SendThread(127.0.0.1:46823):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:38572, server: localhost/127.0.0.1:46823
2025-03-26 17:31:20,586 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2025-03-26 17:31:20,594 - INFO  [main-SendThread(127.0.0.1:46823):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:46823, sessionid = 0x10002a5c4e50000, negotiated timeout = 10000
2025-03-26 17:31:20,599 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2025-03-26 17:31:20,691 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2025-03-26 17:31:20,703 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2025-03-26 17:31:20,704 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Cleared cache
2025-03-26 17:31:20,873 - INFO  [main:Logging@66] - Cluster ID = DGe319klTh-DRzp8B9k_rQ
2025-03-26 17:31:20,877 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit6417420901555019609/junit6084492695690390029/meta.properties
2025-03-26 17:31:20,947 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6417420901555019609/junit6084492695690390029
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:46823
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2025-03-26 17:31:20,961 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6417420901555019609/junit6084492695690390029
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:46823
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2025-03-26 17:31:21,005 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2025-03-26 17:31:21,005 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2025-03-26 17:31:21,007 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2025-03-26 17:31:21,008 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2025-03-26 17:31:21,051 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit6417420901555019609/junit6084492695690390029)
2025-03-26 17:31:21,055 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit6417420901555019609/junit6084492695690390029 since no clean shutdown file was found
2025-03-26 17:31:21,059 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2025-03-26 17:31:21,059 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2025-03-26 17:31:21,062 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2025-03-26 17:31:21,591 - INFO  [main:Logging@66] - Updated connection-accept-rate max connection creation rate to 2147483647
2025-03-26 17:31:21,595 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:32925.
2025-03-26 17:31:21,636 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-03-26 17:31:21,659 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting
2025-03-26 17:31:21,676 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2025-03-26 17:31:21,676 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2025-03-26 17:31:21,677 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2025-03-26 17:31:21,677 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2025-03-26 17:31:21,688 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2025-03-26 17:31:21,727 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2025-03-26 17:31:21,748 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 25,25,1743010281741,1743010281741,1,0,0,72060505034129408,204,0,25

2025-03-26 17:31:21,749 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:32925, czxid (broker epoch): 25
2025-03-26 17:31:21,803 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2025-03-26 17:31:21,811 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2025-03-26 17:31:21,811 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2025-03-26 17:31:21,812 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2025-03-26 17:31:21,830 - INFO  [main-EventThread:Logging@66] - Feature ZK node created at path: /feature
2025-03-26 17:31:21,837 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2025-03-26 17:31:21,842 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2025-03-26 17:31:21,871 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2025-03-26 17:31:21,872 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2025-03-26 17:31:21,875 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
2025-03-26 17:31:21,876 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2025-03-26 17:31:21,876 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2025-03-26 17:31:21,903 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2025-03-26 17:31:21,927 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2025-03-26 17:31:21,945 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
2025-03-26 17:31:21,951 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2025-03-26 17:31:21,952 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
2025-03-26 17:31:21,954 - WARN  [main:AppInfoParser@46] - Error while loading kafka-version.properties: null
2025-03-26 17:31:21,955 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2025-03-26 17:31:21,956 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2025-03-26 17:31:21,956 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1743010279595
2025-03-26 17:31:21,958 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2025-03-26 17:31:21,989 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:32925]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-03-26 17:31:22,023 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2025-03-26 17:31:22,023 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2025-03-26 17:31:22,023 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1743010282023
2025-03-26 17:31:22,073 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:32925 (id: 0 rack: null)
2025-03-26 17:31:22,100 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-03-26 17:31:22,199 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2025-03-26 17:31:22,303 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6417420901555019609/junit6084492695690390029] Loading producer state till offset 0 with message format version 2
2025-03-26 17:31:22,309 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit6417420901555019609/junit6084492695690390029/inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2025-03-26 17:31:22,311 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2025-03-26 17:31:22,313 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2025-03-26 17:31:22,359 - INFO  [kafka-admin-client-thread | adminclient-1:AppInfoParser@83] - App info kafka.admin.client for adminclient-1 unregistered
2025-03-26 17:31:22,363 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:22,364 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:22,364 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:22,368 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:32925]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-03-26 17:31:22,371 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2025-03-26 17:31:22,372 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2025-03-26 17:31:22,372 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1743010282371
2025-03-26 17:31:22,386 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Creating topic outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-03-26 17:31:22,405 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2025-03-26 17:31:22,410 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6417420901555019609/junit6084492695690390029] Loading producer state till offset 0 with message format version 2
2025-03-26 17:31:22,412 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit6417420901555019609/junit6084492695690390029/outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2025-03-26 17:31:22,412 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2025-03-26 17:31:22,413 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2025-03-26 17:31:22,420 - INFO  [kafka-admin-client-thread | adminclient-2:AppInfoParser@83] - App info kafka.admin.client for adminclient-2 unregistered
2025-03-26 17:31:22,423 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:22,423 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:22,423 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:22,505 - INFO  [main:AbstractConfig@372] - StreamsConfig values: 
	acceptable.recovery.lag = 10000
	application.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	application.server = 
	bootstrap.servers = [localhost:32925]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 0
	client.id = 
	commit.interval.ms = 300000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$IntegerSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.windowed.key.serde.inner = null
	default.windowed.value.serde.inner = null
	max.task.idle.ms = 0
	max.warmup.replicas = 2
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	probing.rebalance.interval.ms = 600000
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-3306489674260051305
	task.timeout.ms = 300000
	topology.optimization = none
	upgrade.from = null
	window.size.ms = null
	windowstore.changelog.additional.retention.ms = 86400000

2025-03-26 17:31:22,535 - WARN  [main:StateDirectory@138] - Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-3306489674260051305]
2025-03-26 17:31:22,537 - INFO  [main:StateDirectory@212] - No process id found on disk, got fresh process id 9a367866-848f-4b1a-a36d-d12961b23cab
2025-03-26 17:31:22,572 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:32925]
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-03-26 17:31:22,575 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2025-03-26 17:31:22,575 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2025-03-26 17:31:22,576 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1743010282575
2025-03-26 17:31:22,579 - WARN  [main:ClientMetrics@55] - Error while loading kafka-streams-version.properties
java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:434)
	at java.util.Properties.load0(Properties.java:353)
	at java.util.Properties.load(Properties.java:341)
	at org.apache.kafka.streams.internals.metrics.ClientMetrics.<clinit>(ClientMetrics.java:53)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:825)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:781)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:691)
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.shouldEmitSameRecordAfterFailover(EmitOnChangeIntegrationTest.java:131)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2025-03-26 17:31:22,582 - INFO  [main:KafkaStreams@825] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab] Kafka Streams version: unknown
2025-03-26 17:31:22,582 - INFO  [main:KafkaStreams@826] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab] Kafka Streams commit ID: unknown
2025-03-26 17:31:22,594 - INFO  [main:StreamThread@337] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Creating restore consumer client
2025-03-26 17:31:22,600 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:32925]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-03-26 17:31:22,629 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2025-03-26 17:31:22,630 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2025-03-26 17:31:22,630 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1743010282629
2025-03-26 17:31:22,638 - INFO  [main:ActiveTaskCreator@96] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Creating thread producer client
2025-03-26 17:31:22,644 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:32925]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-03-26 17:31:22,654 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2025-03-26 17:31:22,655 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2025-03-26 17:31:22,655 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1743010282654
2025-03-26 17:31:22,659 - INFO  [main:StreamThread@389] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Creating consumer client
2025-03-26 17:31:22,661 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:32925]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-03-26 17:31:22,664 - INFO  [kafka-producer-network-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-producer:Metadata@279] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-producer] Cluster ID: DGe319klTh-DRzp8B9k_rQ
2025-03-26 17:31:22,674 - INFO  [main:AssignorConfiguration@114] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer] Cooperative rebalancing enabled now
2025-03-26 17:31:22,687 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2025-03-26 17:31:22,687 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2025-03-26 17:31:22,687 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1743010282687
2025-03-26 17:31:22,692 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab] State transition from CREATED to REBALANCING
2025-03-26 17:31:22,693 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@550] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Starting
2025-03-26 17:31:22,693 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] State transition from CREATED to STARTING
2025-03-26 17:31:22,693 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:KafkaConsumer@968] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Subscribed to topic(s): inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2025-03-26 17:31:22,708 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Cluster ID: DGe319klTh-DRzp8B9k_rQ
2025-03-26 17:31:22,711 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-03-26 17:31:22,737 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-03-26 17:31:22,742 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit6417420901555019609/junit6084492695690390029] Loading producer state till offset 0 with message format version 2
2025-03-26 17:31:22,743 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit6417420901555019609/junit6084492695690390029/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2025-03-26 17:31:22,744 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-03-26 17:31:22,744 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-03-26 17:31:22,748 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit6417420901555019609/junit6084492695690390029] Loading producer state till offset 0 with message format version 2
2025-03-26 17:31:22,749 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit6417420901555019609/junit6084492695690390029/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2025-03-26 17:31:22,749 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-03-26 17:31:22,749 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-03-26 17:31:22,753 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit6417420901555019609/junit6084492695690390029] Loading producer state till offset 0 with message format version 2
2025-03-26 17:31:22,754 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit6417420901555019609/junit6084492695690390029/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2025-03-26 17:31:22,754 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-03-26 17:31:22,754 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-03-26 17:31:22,758 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit6417420901555019609/junit6084492695690390029] Loading producer state till offset 0 with message format version 2
2025-03-26 17:31:22,759 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit6417420901555019609/junit6084492695690390029/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2025-03-26 17:31:22,759 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-03-26 17:31:22,760 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-03-26 17:31:22,765 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit6417420901555019609/junit6084492695690390029] Loading producer state till offset 0 with message format version 2
2025-03-26 17:31:22,766 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit6417420901555019609/junit6084492695690390029/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2025-03-26 17:31:22,766 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-03-26 17:31:22,767 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-03-26 17:31:22,769 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 3
2025-03-26 17:31:22,770 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2025-03-26 17:31:22,771 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 2
2025-03-26 17:31:22,771 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2025-03-26 17:31:22,771 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 4
2025-03-26 17:31:22,771 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2025-03-26 17:31:22,771 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 1
2025-03-26 17:31:22,772 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2025-03-26 17:31:22,772 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 0
2025-03-26 17:31:22,772 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2025-03-26 17:31:22,774 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds, of which 1 milliseconds was spent in the scheduler.
2025-03-26 17:31:22,774 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2025-03-26 17:31:22,774 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2025-03-26 17:31:22,774 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2025-03-26 17:31:22,775 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 3 milliseconds, of which 2 milliseconds was spent in the scheduler.
2025-03-26 17:31:22,798 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@829] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2025-03-26 17:31:22,807 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Discovered group coordinator localhost:32925 (id: 2147483647 rack: null)
2025-03-26 17:31:22,809 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2025-03-26 17:31:22,837 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in Empty state. Created a new member id appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer-01e92f00-d0d8-4d31-800a-1c2251b9a8f4 and request the member to rejoin with this id.
2025-03-26 17:31:22,843 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:AbstractCoordinator@982] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Request joining group due to: need to re-join with the given member-id
2025-03-26 17:31:22,843 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2025-03-26 17:31:22,851 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer-01e92f00-d0d8-4d31-800a-1c2251b9a8f4 with group instance id None)
2025-03-26 17:31:22,858 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover generation 1 (__consumer_offsets-0) with 1 members
2025-03-26 17:31:22,860 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully joined group with generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer-01e92f00-d0d8-4d31-800a-1c2251b9a8f4', protocol='stream'}
2025-03-26 17:31:22,878 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog with configuration {message.timestamp.type=CreateTime, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-03-26 17:31:22,892 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0)
2025-03-26 17:31:22,897 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0, dir=/tmp/junit6417420901555019609/junit6084492695690390029] Loading producer state till offset 0 with message format version 2
2025-03-26 17:31:22,898 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 in /tmp/junit6417420901555019609/junit6084492695690390029/appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2025-03-26 17:31:22,899 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2025-03-26 17:31:22,899 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] Log loaded for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with initial high watermark 0
2025-03-26 17:31:22,916 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:HighAvailabilityTaskAssignor@95] - Decided on assignment: {9a367866-848f-4b1a-a36d-d12961b23cab=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0]) capacity: 1 assigned: 1]} with no followup probing rebalance.
2025-03-26 17:31:22,917 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamsPartitionAssignor@595] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
9a367866-848f-4b1a-a36d-d12961b23cab=[activeTasks: ([0_0]) standbyTasks: ([])].
2025-03-26 17:31:22,923 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamsPartitionAssignor@803] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer] Client 9a367866-848f-4b1a-a36d-d12961b23cab per-consumer assignment:
	prev owned active {}
	prev owned standby {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer-01e92f00-d0d8-4d31-800a-1c2251b9a8f4=[]}
	assigned active {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer-01e92f00-d0d8-4d31-800a-1c2251b9a8f4=[0_0]}
	revoking active {}
	assigned standby {}

2025-03-26 17:31:22,923 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamsPartitionAssignor@822] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required.
2025-03-26 17:31:22,924 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:ConsumerCoordinator@626] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Finished assignment for group at generation 1: {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer-01e92f00-d0d8-4d31-800a-1c2251b9a8f4=Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)}
2025-03-26 17:31:22,932 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover for generation 1. The group has 1 members, 0 of which are static.
2025-03-26 17:31:22,967 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully synced group in generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer-01e92f00-d0d8-4d31-800a-1c2251b9a8f4', protocol='stream'}
2025-03-26 17:31:22,968 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:ConsumerCoordinator@392] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Updating assignment with
	Assigned partitions:                       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Revoked partitions (owned - assigned):     []

2025-03-26 17:31:22,969 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:ConsumerCoordinator@276] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Notifying assignor about the new Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)
2025-03-26 17:31:22,969 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamsPartitionAssignor@1306] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2025-03-26 17:31:22,971 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:TaskManager@254] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2025-03-26 17:31:22,985 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:ConsumerCoordinator@288] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Adding newly assigned partitions: inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2025-03-26 17:31:22,985 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2025-03-26 17:31:22,994 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2025-03-26 17:31:23,015 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Resetting offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:32925 (id: 0 rack: null)], epoch=0}}.
2025-03-26 17:31:23,125 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:RocksDBTimestampedStore@100] - Opening store test-store in regular mode
2025-03-26 17:31:23,128 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:ProcessorStateManager@256] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] task [0_0] State store test-store did not find checkpoint offset, hence would default to the starting offset at changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2025-03-26 17:31:23,128 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamTask@235] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] task [0_0] Initialized
2025-03-26 17:31:23,136 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:KafkaConsumer@1123] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2025-03-26 17:31:23,136 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:SubscriptionState@619] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2025-03-26 17:31:23,141 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-restore-consumer, groupId=null] Cluster ID: DGe319klTh-DRzp8B9k_rQ
2025-03-26 17:31:23,145 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:32925 (id: 0 rack: null)], epoch=0}}.
2025-03-26 17:31:23,245 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StoreChangelogReader@609] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Finished restoring changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to store test-store with a total number of 0 records
2025-03-26 17:31:23,248 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2025-03-26 17:31:23,251 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamTask@255] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] task [0_0] Restored and ready to run
2025-03-26 17:31:23,251 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@853] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Restoration took 266 ms for all tasks [0_0]
2025-03-26 17:31:23,251 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2025-03-26 17:31:23,252 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab] State transition from REBALANCING to RUNNING
2025-03-26 17:31:23,255 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:32925]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-03-26 17:31:23,257 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2025-03-26 17:31:23,258 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2025-03-26 17:31:23,258 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1743010283257
2025-03-26 17:31:23,264 - INFO  [kafka-producer-network-thread | producer-1:Metadata@279] - [Producer clientId=producer-1] Cluster ID: DGe319klTh-DRzp8B9k_rQ
2025-03-26 17:31:23,270 - INFO  [main:KafkaProducer@1204] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-03-26 17:31:23,291 - INFO  [main:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:23,292 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:23,292 - INFO  [main:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:23,292 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-1 unregistered
2025-03-26 17:31:23,293 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:32925]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b79fb011-c1fe-480d-84dd-ec2dc56b6c9f
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-03-26 17:31:23,296 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2025-03-26 17:31:23,296 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2025-03-26 17:31:23,296 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1743010283296
2025-03-26 17:31:23,298 - INFO  [main:KafkaConsumer@968] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Subscribed to topic(s): outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2025-03-26 17:31:23,302 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Cluster ID: DGe319klTh-DRzp8B9k_rQ
2025-03-26 17:31:23,303 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Discovered group coordinator localhost:32925 (id: 2147483647 rack: null)
2025-03-26 17:31:23,305 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] (Re-)joining group
2025-03-26 17:31:23,308 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group b79fb011-c1fe-480d-84dd-ec2dc56b6c9f in Empty state. Created a new member id consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1-ef88d86a-3a51-49f7-85da-bbc172bd4de8 and request the member to rejoin with this id.
2025-03-26 17:31:23,310 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Request joining group due to: need to re-join with the given member-id
2025-03-26 17:31:23,310 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] (Re-)joining group
2025-03-26 17:31:23,312 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group b79fb011-c1fe-480d-84dd-ec2dc56b6c9f in state PreparingRebalance with old generation 0 (__consumer_offsets-3) (reason: Adding new member consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1-ef88d86a-3a51-49f7-85da-bbc172bd4de8 with group instance id None)
2025-03-26 17:31:23,314 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group b79fb011-c1fe-480d-84dd-ec2dc56b6c9f generation 1 (__consumer_offsets-3) with 1 members
2025-03-26 17:31:23,315 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Successfully joined group with generation Generation{generationId=1, memberId='consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1-ef88d86a-3a51-49f7-85da-bbc172bd4de8', protocol='range'}
2025-03-26 17:31:23,316 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Finished assignment for group at generation 1: {consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1-ef88d86a-3a51-49f7-85da-bbc172bd4de8=Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])}
2025-03-26 17:31:23,320 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group b79fb011-c1fe-480d-84dd-ec2dc56b6c9f for generation 1. The group has 1 members, 0 of which are static.
2025-03-26 17:31:23,325 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Successfully synced group in generation Generation{generationId=1, memberId='consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1-ef88d86a-3a51-49f7-85da-bbc172bd4de8', protocol='range'}
2025-03-26 17:31:23,325 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Notifying assignor about the new Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])
2025-03-26 17:31:23,326 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Adding newly assigned partitions: outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2025-03-26 17:31:23,329 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Found no committed offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2025-03-26 17:31:23,335 - INFO  [main:SubscriptionState@398] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Resetting offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:32925 (id: 0 rack: null)], epoch=0}}.
2025-03-26 17:31:23,452 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Revoke previously assigned partitions outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2025-03-26 17:31:23,452 - INFO  [main:AbstractCoordinator@1038] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Member consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1-ef88d86a-3a51-49f7-85da-bbc172bd4de8 sending LeaveGroup request to coordinator localhost:32925 (id: 2147483647 rack: null) due to the consumer is being closed
2025-03-26 17:31:23,453 - INFO  [main:AbstractCoordinator@961] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Resetting generation due to: consumer pro-actively leaving the group
2025-03-26 17:31:23,454 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, groupId=b79fb011-c1fe-480d-84dd-ec2dc56b6c9f] Request joining group due to: consumer pro-actively leaving the group
2025-03-26 17:31:23,457 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group b79fb011-c1fe-480d-84dd-ec2dc56b6c9f in state PreparingRebalance with old generation 1 (__consumer_offsets-3) (reason: removing member consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1-ef88d86a-3a51-49f7-85da-bbc172bd4de8 on LeaveGroup)
2025-03-26 17:31:23,459 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Group b79fb011-c1fe-480d-84dd-ec2dc56b6c9f with generation 2 is now empty (__consumer_offsets-3)
2025-03-26 17:31:23,462 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1-ef88d86a-3a51-49f7-85da-bbc172bd4de8, groupInstanceId=None, clientId=consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) has left group b79fb011-c1fe-480d-84dd-ec2dc56b6c9f through explicit `LeaveGroup` request
2025-03-26 17:31:23,465 - INFO  [main:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:23,466 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:23,466 - INFO  [main:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:23,468 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-b79fb011-c1fe-480d-84dd-ec2dc56b6c9f-1 unregistered
2025-03-26 17:31:23,468 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab] State transition from RUNNING to PENDING_SHUTDOWN
2025-03-26 17:31:23,469 - INFO  [kafka-streams-close-thread:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Informed to shut down
2025-03-26 17:31:23,470 - INFO  [kafka-streams-close-thread:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2025-03-26 17:31:23,537 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@729] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Thread state is already PENDING_SHUTDOWN, skipping the run once call after poll request
2025-03-26 17:31:23,537 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@1071] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Shutting down
2025-03-26 17:31:23,548 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamTask@1144] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] task [0_0] Suspended RUNNING
2025-03-26 17:31:23,548 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamTask@290] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] task [0_0] Suspended running
2025-03-26 17:31:23,550 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2025-03-26 17:31:23,553 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:RecordCollectorImpl@268] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] task [0_0] Closing record collector clean
2025-03-26 17:31:23,554 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamTask@508] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] task [0_0] Closed clean
2025-03-26 17:31:23,555 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:KafkaProducer@1204] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-03-26 17:31:23,557 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:23,557 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:23,557 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:23,557 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:AppInfoParser@83] - App info kafka.producer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-producer unregistered
2025-03-26 17:31:23,558 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2025-03-26 17:31:23,558 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:23,558 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:23,558 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:23,559 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-consumer unregistered
2025-03-26 17:31:23,560 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:23,560 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:23,560 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:23,560 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1-restore-consumer unregistered
2025-03-26 17:31:23,561 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2025-03-26 17:31:23,561 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1:StreamThread@1100] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-StreamThread-1] Shutdown complete
2025-03-26 17:31:23,561 - ERROR [kafka-streams-close-thread:StateDirectory@414] - Some task directories still locked while closing state, this indicates unclean shutdown: {}
2025-03-26 17:31:23,561 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-admin:AppInfoParser@83] - App info kafka.admin.client for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-admin unregistered
2025-03-26 17:31:23,562 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-admin:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:23,562 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-admin:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:23,562 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab-admin:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:23,562 - INFO  [kafka-streams-close-thread:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:23,562 - INFO  [kafka-streams-close-thread:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:23,562 - INFO  [kafka-streams-close-thread:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:23,562 - INFO  [kafka-streams-close-thread:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2025-03-26 17:31:23,562 - INFO  [main:KafkaStreams@1367] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-9a367866-848f-4b1a-a36d-d12961b23cab] Streams client stopped completely
2025-03-26 17:31:23,566 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2025-03-26 17:31:23,566 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2025-03-26 17:31:23,575 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2025-03-26 17:31:23,576 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2025-03-26 17:31:23,577 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2025-03-26 17:31:23,577 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2025-03-26 17:31:23,578 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-03-26 17:31:23,582 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-03-26 17:31:23,583 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2025-03-26 17:31:23,584 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-03-26 17:31:23,586 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2025-03-26 17:31:23,705 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2025-03-26 17:31:23,705 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-03-26 17:31:23,706 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2025-03-26 17:31:23,707 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2025-03-26 17:31:23,804 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2025-03-26 17:31:23,804 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2025-03-26 17:31:23,807 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2025-03-26 17:31:23,807 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2025-03-26 17:31:23,808 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2025-03-26 17:31:23,808 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2025-03-26 17:31:23,808 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2025-03-26 17:31:23,808 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2025-03-26 17:31:23,809 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2025-03-26 17:31:23,809 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2025-03-26 17:31:23,810 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2025-03-26 17:31:23,813 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2025-03-26 17:31:23,813 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-03-26 17:31:23,813 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2025-03-26 17:31:23,914 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2025-03-26 17:31:23,914 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-03-26 17:31:23,915 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2025-03-26 17:31:23,916 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2025-03-26 17:31:23,916 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2025-03-26 17:31:23,916 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2025-03-26 17:31:23,916 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2025-03-26 17:31:23,917 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2025-03-26 17:31:23,918 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2025-03-26 17:31:23,919 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-03-26 17:31:23,919 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-03-26 17:31:23,919 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2025-03-26 17:31:23,937 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2025-03-26 17:31:23,937 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2025-03-26 17:31:23,938 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2025-03-26 17:31:24,085 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2025-03-26 17:31:24,085 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2025-03-26 17:31:24,085 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-03-26 17:31:24,285 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2025-03-26 17:31:24,285 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-03-26 17:31:24,286 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2025-03-26 17:31:24,485 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2025-03-26 17:31:24,485 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-03-26 17:31:24,488 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2025-03-26 17:31:24,489 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down
2025-03-26 17:31:24,490 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped
2025-03-26 17:31:24,490 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed
2025-03-26 17:31:24,491 - INFO  [main:Logging@66] - Broker to controller channel manager for alterIsr shutdown
2025-03-26 17:31:24,492 - INFO  [main:Logging@66] - Shutting down.
2025-03-26 17:31:24,505 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-3] Writing producer snapshot at offset 3
2025-03-26 17:31:24,516 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 2
2025-03-26 17:31:24,519 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0] Writing producer snapshot at offset 2
2025-03-26 17:31:24,523 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 2
2025-03-26 17:31:24,526 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 2
2025-03-26 17:31:24,539 - INFO  [main:Logging@66] - Shutdown complete.
2025-03-26 17:31:24,547 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2025-03-26 17:31:24,547 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2025-03-26 17:31:24,547 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2025-03-26 17:31:24,548 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2025-03-26 17:31:24,651 - INFO  [main:ZooKeeper@1422] - Session: 0x10002a5c4e50000 closed
2025-03-26 17:31:24,651 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x10002a5c4e50000
2025-03-26 17:31:24,652 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2025-03-26 17:31:24,653 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2025-03-26 17:31:25,005 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2025-03-26 17:31:25,005 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-03-26 17:31:25,006 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2025-03-26 17:31:26,006 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2025-03-26 17:31:26,006 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2025-03-26 17:31:26,006 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2025-03-26 17:31:26,007 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2025-03-26 17:31:26,007 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2025-03-26 17:31:26,007 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-03-26 17:31:26,009 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-03-26 17:31:26,009 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-03-26 17:31:26,010 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-03-26 17:31:26,026 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-03-26 17:31:26,027 - INFO  [main:Metrics@659] - Metrics scheduler closed
2025-03-26 17:31:26,027 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-03-26 17:31:26,027 - INFO  [main:Metrics@669] - Metrics reporters closed
2025-03-26 17:31:26,030 - INFO  [main:Logging@66] - Broker and topic stats closed
2025-03-26 17:31:26,031 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2025-03-26 17:31:26,031 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2025-03-26 17:31:26,038 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2025-03-26 17:31:26,038 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2025-03-26 17:31:26,039 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2025-03-26 17:31:26,039 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2025-03-26 17:31:26,039 - INFO  [main:ZooKeeperServer@573] - shutting down
2025-03-26 17:31:26,039 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2025-03-26 17:31:26,039 - INFO  [main:PrepRequestProcessor@1008] - Shutting down
2025-03-26 17:31:26,040 - INFO  [ProcessThread(sid:0 cport:46823)::PrepRequestProcessor@156] - PrepRequestProcessor exited loop!
2025-03-26 17:31:26,040 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2025-03-26 17:31:26,040 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2025-03-26 17:31:26,040 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete

Thanks for using JUnit! Support its development at https://junit.org/sponsoring

[36m[0m
[36m[0m [36mJUnit Jupiter[0m [32m[0m
[36m[0m [36mJUnit Vintage[0m [32m[0m
[36m   [0m [36mEmitOnChangeIntegrationTest[0m [32m[0m
[36m      [0m [34mshouldEmitSameRecordAfterFailover[0m [32m[0m

Test run finished after 6613 ms
[         3 containers found      ]
[         0 containers skipped    ]
[         3 containers started    ]
[         0 containers aborted    ]
[         3 containers successful ]
[         0 containers failed     ]
[         1 tests found           ]
[         0 tests skipped         ]
[         1 tests started         ]
[         0 tests aborted         ]
[         1 tests successful      ]
[         0 tests failed          ]

